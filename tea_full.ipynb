{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b450759-0b63-43dc-aaae-0c552dc742ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#полный пайплайн TEA\n",
    "#изготавливает модель, сохраняет, разбирает, генерит с помощью модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc926f-a841-44cb-b4ef-c98edc2153c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716ec6b-dd22-4253-afd0-5c8f7f5449ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False\n",
    "#Через requirements поставить это затруднительно, последовательность установки важна, так как часть либ компилируются\n",
    "#Эта последовательность установки, как бы странно она ни выглядела, работает. Не в любых окружениях, но во многих.\n",
    "if install:\n",
    "\n",
    "    !pip install deepspeed==0.14.4\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --upgrade transformers==4.46.2 tyro==0.9.8 triton==2.3.1 trl==0.12.0\n",
    "\n",
    "    !pip install -v \"xformers==0.0.29.post1\"\n",
    "    #!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps -v \"peft==0.13.2\"\n",
    "    !pip install --no-deps packaging ninja einops flash-attn bitsandbytes==0.44.1\n",
    "    !pip install accelerate==0.34.2\n",
    "    !pip uninstall peft -y\n",
    "    !pip install -v \"peft==0.13.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56dee4c5-5085-485f-b5f4-6f3202afd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import ensembles\n",
    "from sequential_models import GPTAdapterLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b51f5dc-45af-42fe-b377-275433640215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется GPU с номером: 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Используется GPU с номером: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83241649-7c80-421c-b139-687028ee1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4ac20d-b48b-432b-8888-1553f76a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем доступность CUDA и количество GPU\n",
    "#если надо, можем переключиться на другую GPU\n",
    "if 0:\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:  # Если есть более 1 GPU\n",
    "            device = torch.device(\"cuda:1\")  # Явно указываем устройство 1\n",
    "            print(f\"Переключились на GPU 1\")\n",
    "        else:\n",
    "            print(\"Доступен только один GPU (номер 0), переключение невозможно\")\n",
    "    else:\n",
    "        print(\"CUDA недоступно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff90a212-5828-45ee-87af-82cc907b1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = False#Запускаем ли мы сейчас обучение с нуля, или с чекпоинта\n",
    "train_transformer = True#надо ли обучать трансформерный адаптер\n",
    "train_lm_head = True#надо ли обучать lm_head\n",
    "learnable_linear_model = True#учим ли мы линейную субмодель внутри lm_head\n",
    "\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling\n",
    "\n",
    "bits_per_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae93daaf-fbaa-4898-a816-8bf140ba04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size4loss = 1024 * 2#loss считаем батчами, во избежание out of memory error\n",
    "head_max_batch_size = 1024#lm_head заускаем батчами, во избежание out of memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e3204a-6817-4fde-b2be-7f327cc8fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конфигурации моделек\n",
    "mode = 'pretrain'#'finetune', 'pretrain'\n",
    "model_version = \"heavy_2\"\n",
    "transformer_adapter_params = {}\n",
    "lm_head_adapter_params = {'cardinality':cardinality}\n",
    "optimizer_params = {}\n",
    "if model_version == \"heavy_2\":\n",
    "    transformer_adapter_params['embed_dim'] = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 16\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 4\n",
    "    transformer_adapter_params['t_layers_count'] = 2\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 2\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 900\n",
    "    optimizer_params['lr'] = 1e-5\n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    optimizer_params['opt_type'] = 'adam'\n",
    "\n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [2048] * 5 + [1024] * 5\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False\n",
    "\n",
    "elif model_version == \"heavy_6\":\n",
    "    transformer_adapter_params['embed_dim'] = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 16\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 4\n",
    "    transformer_adapter_params['t_layers_count'] = 2\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 2\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 9\n",
    "    optimizer_params['lr'] = 1e-5\n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    optimizer_params['opt_type'] = 'adam'\n",
    "\n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [2048] * 5 + [1024] * 5\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c9abcd-c2b0-4e10-9a32-5e516e321c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_adapter_params['head_max_batch_size'] = head_max_batch_size\n",
    "lm_head_adapter_params['learnable_linear_model'] = learnable_linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3c61a7-a882-40ae-bf7e-65b844fd0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2tadapter = f\"tadapter_{model_version}.pth\"\n",
    "path2lmhead = f\"lmhead_{model_version}.pth\"\n",
    "tokenizer_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "211d07a1-531e-46f1-9aa1-ecd890e6704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#настройки датасета\n",
    "\n",
    "if mode == 'pretrain':\n",
    "    #претрейн\n",
    "    max_seq_len = 6900\n",
    "    max_seq_len4inference = 6850\n",
    "else:\n",
    "    #дообучение\n",
    "    max_seq_len = 2050\n",
    "    max_seq_len4inference = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a99486-1f3f-4b83-b57a-88fe00ee0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\c'\n",
      "/tmp/ipykernel_367795/3520330652.py:20: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/ssdovgan/main/lib/python3.12/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "#Загрузили исходную модельку\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")#, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_name,\n",
    "              quantization_config=bnb_config,\n",
    "              cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f308a2e1-93f4-42c1-8ff6-42eb4239c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.lm_head\n",
    "torch.save(model.lm_head.weight, \"lin_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac2e1da-5e9f-4ec9-a3c8-e3f4229ad40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e8a7526-ec4c-40b7-8320-4a469f360318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071219"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "#такая конструкция нужна на случай, что вы загружаете датасет на диск, сейчас час ночи, \n",
    "#датасет догрузится примерно через час, а завтра вам рано вставать\n",
    "#с таким подходм код будет выполняться, когда найдёт на диске датасет\n",
    "while 1:\n",
    "    try:\n",
    "        if mode == 'pretrain':\n",
    "            #dataset = InstructDatasetR(\"./data/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_flat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "        else:\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e, pd.Timestamp.now())\n",
    "        time.sleep(10)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00128664-8ce8-48d9-bf3e-ba7fb99ad2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens\n",
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)\n",
    "def count_trainable_parameters(model: nn.Module) -> tuple[int, int]:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable, frozen\n",
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d252c6c-567b-4166-a38c-be91a3d05abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#теперь сборка модели с адаптерами\n",
    "def assemble_model(model, \n",
    "                   path2lmhead, \n",
    "                   path2tadapter, \n",
    "                   start_train,\n",
    "                   to_generate,\n",
    "                   lm_head_adapter_params,\n",
    "                   transformer_adapter_params,\n",
    "                   optimizer_params\n",
    "                  ):\n",
    "    if start_train:\n",
    "        #инициализировать всё это дело\n",
    "        torch.manual_seed(1)\n",
    "        np.random.seed(1)\n",
    "        random.seed(1)\n",
    "        head = ensembles.EResNetPro(input_size=lm_head_adapter_params['embedding_size'], \n",
    "                   out_size=lm_head_adapter_params['cardinality'], \n",
    "                   net_dropout_rate=lm_head_adapter_params['net_dropout_rate'], \n",
    "                   individ_dropout_rate=lm_head_adapter_params['individ_dropout_rate'],\n",
    "                   layer_configs=lm_head_adapter_params['layer_configs_head'], \n",
    "                   use_sigmoid_end=False, \n",
    "                   use_batchnorm=True, \n",
    "                   use_activation=True, \n",
    "                   activation=nn.LeakyReLU(), \n",
    "                   sample_features=lm_head_adapter_params['sample_features'], \n",
    "                   composition_size=lm_head_adapter_params['composition_size'], \n",
    "                   lin_bottleneck_size=None,\n",
    "                   lin_model_add=nn.Linear(lm_head_adapter_params['embedding_size'], lm_head_adapter_params['cardinality']).to(device),\n",
    "                   memnet_params=lm_head_adapter_params['memnet_params'],\n",
    "                   use_memnets=lm_head_adapter_params['use_memnets'],\n",
    "                   max_batch_size=lm_head_adapter_params['head_max_batch_size'],\n",
    "                   aggregation_by_mean=False).to(device)\n",
    "        head.submodels[-1].weight = torch.nn.Parameter(torch.load(\"lin_model.pth\").to(device).to(torch.float32))\n",
    "        head.submodels[-1].weight.requires_grad = lm_head_adapter_params['learnable_linear_model']\n",
    "    \n",
    "        tadapter = GPTAdapterLayer(initial_layer=None, \n",
    "                                 dim=transformer_adapter_params['embed_dim'], \n",
    "                                 num_heads=transformer_adapter_params['num_heads_tlayer'], \n",
    "                                 ff_dim=transformer_adapter_params['ff_dim'], \n",
    "                                 t_layers_count=transformer_adapter_params['t_layers_count'], \n",
    "                                 layer_configs=transformer_adapter_params['layer_configs'],\n",
    "                                 conservativity=transformer_adapter_params['transformer_adapter_weight'],\n",
    "                                 dropout=transformer_adapter_params['dropout'],\n",
    "                                 composition_type='stack').to(device)\n",
    "    else:\n",
    "        head = torch.load(path2lmhead, weights_only=False)\n",
    "        tadapter = torch.load(path2tadapter, weights_only=False)\n",
    "    \n",
    "    #собрать\n",
    "    model.eval()\n",
    "\n",
    "    head.to(device)\n",
    "    head.by_submodels = False\n",
    "\n",
    "    optimizer = None\n",
    "    if to_generate:\n",
    "        #ща сошьём\n",
    "        head.half()\n",
    "        tadapter.half()\n",
    "        model.lm_head = head\n",
    "        tadapter.initial_layer = model.model.layers[-1]\n",
    "        model.model.layers[-1] = tadapter\n",
    "        model.model.layers[-1].transformer_float_mode = 16\n",
    "        model.model.layers[-1].lmhead_float_mode = 16\n",
    "        return\n",
    "    else:\n",
    "        head.train()\n",
    "        tadapter.train()\n",
    "        if optimizer_params['opt_type'] == 'adam':\n",
    "            optimizer = torch.optim.Adam([\n",
    "                {'params': head.parameters(), 'lr': optimizer_params['lr']},\n",
    "                {'params': tadapter.parameters(), 'lr': optimizer_params['lr'] * 0.05}\n",
    "            ], lr=optimizer_params['lr'])\n",
    "        else:\n",
    "            momentum = 0.\n",
    "            lr = optimizer_params['lr'] * 0.08\n",
    "            optimizer = torch.optim.SGD([\n",
    "                {'params': head.parameters(), 'lr': lr},\n",
    "                {'params': tadapter.parameters(), 'lr': lr * 0.05}\n",
    "            ],  momentum=momentum)\n",
    "        trainable, frozen = count_trainable_parameters(head)\n",
    "        print(f\"Head. Обучаемые: {trainable:,}\\nЗамороженные: {frozen:,}\")\n",
    "        trainable, frozen = count_trainable_parameters(tadapter)\n",
    "        print(f\"Transformer adapter. Обучаемые: {trainable:,}\\nЗамороженные: {frozen:,}\")\n",
    "        return head, tadapter, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882c53b2-f9ce-41f6-b485-f6aafb22340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#теперь сохранение адаптеров\n",
    "def save_adapters(head, tadapter, path2lmhead, path2tadapter):\n",
    "    head.training = False\n",
    "    torch.save(head, path2lmhead)\n",
    "    torch.save(tadapter, path2tadapter)\n",
    "    head.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dfef2eb-45c5-46e2-ac7a-93981027dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_memnets False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367795/2452345961.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  head.submodels[-1].weight = torch.nn.Parameter(torch.load(\"lin_model.pth\").to(device).to(torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_memnets True\n",
      "use_memnets True\n",
      "Head. Обучаемые: 1,410,552,156\n",
      "Замороженные: 0\n",
      "Transformer adapter. Обучаемые: 701,224,480\n",
      "Замороженные: 0\n"
     ]
    }
   ],
   "source": [
    "head, tadapter, optimizer = assemble_model(model, \n",
    "                path2lmhead, \n",
    "                path2tadapter, \n",
    "                start_train=start_train,\n",
    "                to_generate=False,\n",
    "                lm_head_adapter_params=lm_head_adapter_params,\n",
    "                transformer_adapter_params=transformer_adapter_params,\n",
    "                optimizer_params=optimizer_params\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9977600-efbf-435d-a573-69ed74961c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586d9dd-8124-4868-b5ea-d0c6238a85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367795/721343507.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  weights_full = torch.stack(weights_full).to('cuda').T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot backward, shape 991\n",
      "cannot backward, shape 124\n"
     ]
    }
   ],
   "source": [
    "#TRAIN!!!\n",
    "#Если вы задаётесь вопросом, почему такой странный цикл обучения, давайте посмотрим, что не сработало в \"нормальных\" циклах обучения.\n",
    "#Можно было запихнуть трансформерный адаптер и lm-resnet в исходную LLM, разморозить их и заморозить всё остальное. \n",
    "#НО тогда по непонятной причине выходила крайне плохая точность обучения. loss = 1.65 на датасете, где у классического TEA (ну, чисто резнеты) loss = 0.85\n",
    "#Две недели поисков выявили, что видимое различие в том, что в одном случае адаптер обучает ВНУТРИ сетки, а в другом СНАРУЖИ. Снаружи ПОЧЕМУ-ТО лучше\n",
    "#\n",
    "#Если проинференсить трансформерный адаптер, а потом LM-head, то запросто может получиться, что на этапе backward мы сдохнем по памяти.\n",
    "#Как фиксить? Делаем, чтобы для всего множества эмбеддингов от трансформерной части loss считался батчами\n",
    "#В таком случае возникает ошибка, мол, чё нам делать с градиентами в трансформерном адаптере - мы же их уже посчитали, а ты предлагаешь считать ещё раз!\n",
    "#Решаем так: берёт сабсемпл от эмбеддингов трансформерного адаптера и считаем loss на сабсемпле\n",
    "#Это не вполне честно, так как мы менее тщательно учитываем длинные последовательности, но эй, это работает!\n",
    "\n",
    "print('batch_size', optimizer_params['batch_size'])\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < optimizer_params['batch_size']:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "\n",
    "    i_pointer += i #should be batch_size\n",
    "    with torch.no_grad():\n",
    "        input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "        labels_full = torch.stack(labels_full).to('cuda')\n",
    "        weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "        cnt_pads = count_padding(labels_full, padding_token)\n",
    "        if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "            continue\n",
    "        if cnt_pads > 0:\n",
    "            input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "            labels_full = labels_full[:, :-cnt_pads]\n",
    "            \n",
    "        outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=True)\n",
    "        outp['states'] = outp['hidden_states'][-1]\n",
    "        outp['states'] = outp['states'][:, :input_ids_full.shape[1]]\n",
    "        state_full = outp['states']\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        weights_cur = weights_full2d.ravel()\n",
    "\n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        del outp\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "        weights_cur = weights_cur[~idx]\n",
    "    if state_full.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    state_full = tadapter(state_full)[0]\n",
    "    state_cur = state_full.reshape([state_full.shape[0] * state_full.shape[1], state_full.shape[2]])\n",
    "    state_cur = state_cur[~idx]\n",
    "    del state_full\n",
    "    perm = torch.randperm(state_cur.size(0), device=device)\n",
    "    state_cur = state_cur[perm]\n",
    "    weights_cur = weights_cur[perm]\n",
    "    labels_cur = labels_cur[perm]\n",
    "\n",
    "    loss = None\n",
    "    for x_start_pointer in range(0, state_cur.shape[0], max_batch_size4loss): \n",
    "        logits = head(state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_batch_size4loss])\n",
    "        ce_loss = loss_fct(logits, labels_cur[x_start_pointer: x_start_pointer + max_batch_size4loss].view(-1))\n",
    "        ce_loss = ce_loss * weights_cur[x_start_pointer: x_start_pointer + max_batch_size4loss].to('cuda')\n",
    "        ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "        loss = torch.mean(ce_loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        if state_cur.shape[0] - (x_start_pointer + max_batch_size4loss) > 0:\n",
    "            print('cannot backward, shape', state_cur.shape[0] - (x_start_pointer + max_batch_size4loss))\n",
    "        break\n",
    "       \n",
    "    del state_cur\n",
    "    del weights_cur\n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits, axis=-1) == labels_cur[x_start_pointer: x_start_pointer + max_batch_size4loss]).to(torch.float16))\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= optimizer_params['accum_batch'] :\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "        if (np.random.rand() < 1 and optimizer_params['accum_batch'] > 100) or (np.random.rand() < 0.1 and optimizer_params['accum_batch']  <= 100):\n",
    "            try:\n",
    "                save_adapters(head, tadapter, path2lmhead, path2tadapter)\n",
    "                if np.random.rand()<0.1:\n",
    "                    save_adapters(head, tadapter, path2lmhead + '.back', path2tadapter + '.back')\n",
    "            except Exception:\n",
    "                #Я не хочу, чтобы скрипт падал только потому, что кто-то завалил веь диск\n",
    "                print(\"not saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ad884-d47e-4d8a-82c3-60f4e2b120d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d448-2a4d-4153-adbf-5bdfc27b6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3705-a864-4c7f-bf5e-5cc592eae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ручной сейв\n",
    "save_adapters(head, tadapter, path2lmhead, path2tadapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9bab7-e955-407d-b6ce-1d25f44d0277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19e26e-4153-486c-912b-b8bab08f4460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ef640-f536-4479-92a1-1f285fdebe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестим модель\n",
    "assemble_model(model, \n",
    "                path2lmhead, \n",
    "                path2tadapter, \n",
    "                start_train=False,\n",
    "                to_generate=True,\n",
    "                lm_head_adapter_params=lm_head_adapter_params,\n",
    "                transformer_adapter_params=transformer_adapter_params,\n",
    "                optimizer_params=optimizer_params\n",
    "              )\n",
    "\n",
    "prompt = 'Твоя роль: Сталин. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Девушка-флористка. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Data scientist. \\n Серёга: Мне надо улучшить LLM. У меня ресурсы на запуск Llama 3.1 8B или Qwen 2.5 14B, квантованные до 4 бит. Мне нужно, чтобы мои модели былу умнее (на уровне 70 B), но при этом имели настолько же низкие требования по памяти и запускались так же быстро. Я готов обучать новую модель. Я готов пробовать другие архитектуры. Опиши план, что делать. Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Инженер-ракетчик. \\n Серёга: Мне надо сделать ракету, в домашних условиях, небольшую. Расскажи, как это провернуть? ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Геймер, студент-инженер. \\n Серёга: Перечисли всех монстров из Doom, каких вспомнишь! Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Гермиона Грейнджер. \\n Серёга: Как справиться с василиском? Your answer, ONLY ENGLISH  (end by \"|\", 200 tokens):'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "repetition_penalty = 1.2\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                        max_new_tokens=200,\n",
    "                                        temperature=0.01,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        #no_repeat_ngram_size=3,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        early_stopping=True,\n",
    "                                        use_cache=True,\n",
    "                                        num_beams=1,\n",
    "                                        tokenizer=tokenizer)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232b8cc-35b4-492b-aa21-2f0278efcbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
