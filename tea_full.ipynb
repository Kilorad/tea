{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b450759-0b63-43dc-aaae-0c552dc742ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#полный пайплайн TEA\n",
    "#изготавливает модель, сохраняет, разбирает, генерит с помощью модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc926f-a841-44cb-b4ef-c98edc2153c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716ec6b-dd22-4253-afd0-5c8f7f5449ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False\n",
    "#Через requirements поставить это затруднительно, последовательность установки важна, так как часть либ компилируются\n",
    "#Эта последовательность установки, как бы странно она ни выглядела, работает. Не в любых окружениях, но во многих.\n",
    "if install:\n",
    "\n",
    "    !pip install deepspeed==0.14.4\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --upgrade transformers==4.46.2 tyro==0.9.8 triton==2.3.1 trl==0.12.0\n",
    "\n",
    "    !pip install -v \"xformers==0.0.29.post1\"\n",
    "    #!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps -v \"peft==0.13.2\"\n",
    "    !pip install --no-deps packaging ninja einops flash-attn bitsandbytes==0.44.1\n",
    "    !pip install accelerate==0.34.2\n",
    "    !pip uninstall peft -y\n",
    "    !pip install -v \"peft==0.13.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56dee4c5-5085-485f-b5f4-6f3202afd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import ensembles\n",
    "from sequential_models import GPTAdapterLayer, GPTAdapterLayerWide, assemble_model, disassemble_model\n",
    "from benchmarks import LLMBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b51f5dc-45af-42fe-b377-275433640215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется GPU с номером: 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Используется GPU с номером: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83241649-7c80-421c-b139-687028ee1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff5bac9-e1fe-4843-b2f9-6904909d4bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Remaining GPU memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4ac20d-b48b-432b-8888-1553f76a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем доступность CUDA и количество GPU\n",
    "#если надо, можем переключиться на другую GPU\n",
    "if 0:\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:  # Если есть более 1 GPU\n",
    "            device = torch.device(\"cuda:1\")  # Явно указываем устройство 1\n",
    "            print(f\"Переключились на GPU 1\")\n",
    "        else:\n",
    "            print(\"Доступен только один GPU (номер 0), переключение невозможно\")\n",
    "    else:\n",
    "        print(\"CUDA недоступно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff90a212-5828-45ee-87af-82cc907b1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = False#Запускаем ли мы сейчас обучение с нуля, или с чекпоинта\n",
    "learnable_linear_model = True#учим ли мы линейную субмодель внутри lm_head\n",
    "\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling\n",
    "\n",
    "bits_per_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae93daaf-fbaa-4898-a816-8bf140ba04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size4loss = 1024 * 3#1024 * 2#loss считаем батчами, во избежание out of memory error\n",
    "head_max_batch_size = 1024#lm_head заускаем батчами, во избежание out of memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f83cc3-cf9b-401a-b1f2-c07f63ddb985",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cp_for_head = True#если True, то мы более устойчивы к OOM, если false, то быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e3204a-6817-4fde-b2be-7f327cc8fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конфигурации моделек\n",
    "mode = 'pretrain'#'finetune', 'pretrain'\n",
    "model_version = \"heavy_4\"#\"light_1\"#\"heavy_4\"\n",
    "transformer_adapter_params = {}\n",
    "lm_head_adapter_params = {'cardinality':cardinality}\n",
    "optimizer_params = {}\n",
    "depth2freeze = 0\n",
    "if model_version == \"heavy_2\":\n",
    "    transformer_adapter_params['embed_dim'] = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 16\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 4\n",
    "    transformer_adapter_params['t_layers_count'] = 2\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 2\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 50\n",
    "    optimizer_params['lr'] = 1e-6#lr для LM_head\n",
    "    optimizer_params['lr_transformer'] = optimizer_params['lr'] * 0.05#lr для трансформерного адаптера\n",
    "    optimizer_params['transformer_update_rate'] = 0.5#0.2#доля случаев, когда через трансформерный адаптер реально идут градиенты. Влияет на быстродействие\n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    transformer_adapter_params['concat'] = False\n",
    "    optimizer_params['opt_type'] = 'adam'\n",
    "\n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [2048] * 5 + [1024] * 5\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False\n",
    "\n",
    "\n",
    "\n",
    "elif model_version == \"heavy_3\":\n",
    "    transformer_adapter_params['embed_dim'] = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 32\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 2\n",
    "    transformer_adapter_params['t_layers_count'] = 1\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 4\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 13000\n",
    "    optimizer_params['lr'] = 1e-6#lr для LM_head\n",
    "    optimizer_params['transformer_update_rate'] = 0.5#0.2#доля случаев, когда через трансформерный адаптер реально идут градиенты. Влияет на быстродействие\n",
    "    optimizer_params['lr_transformer'] = optimizer_params['lr'] * 0.1 * optimizer_params['transformer_update_rate']#lr для трансформерного адаптера\n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    transformer_adapter_params['concat'] = True\n",
    "    optimizer_params['opt_type'] = 'adam'\n",
    "\n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [2048] * 6 + [1024] * 5\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False\n",
    "\n",
    "elif model_version == \"heavy_4\":\n",
    "    transformer_adapter_params['embed_dim'] = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 32\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 2\n",
    "    transformer_adapter_params['t_layers_count'] = 1\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 10\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 1000#23000#10000\n",
    "    optimizer_params['lr'] = 1e-6#lr для LM_head\n",
    "    optimizer_params['transformer_update_rate'] = 0.#0.2#доля случаев, когда через трансформерный адаптер реально идут градиенты. Влияет на быстродействие\n",
    "    optimizer_params['lr_transformer'] = optimizer_params['lr'] * 0.1 * optimizer_params['transformer_update_rate']#lr для трансформерного адаптера\n",
    "    \n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    transformer_adapter_params['concat'] = True\n",
    "    optimizer_params['opt_type'] = 'adam'#'adam'\n",
    "    \n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    # lm_head_adapter_params['net_dropout_rate'] = 0.\n",
    "    # lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    # lm_head_adapter_params['layer_configs_head'] = [1024 * 3]  + [1024 * 2] * 2 + [1024] * 4 + [512] * 3\n",
    "    # lm_head_adapter_params['sample_features'] = 1.\n",
    "    # lm_head_adapter_params['composition_size'] = 1\n",
    "\n",
    "\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0.7\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [256] * 2 + [128] * 5  + [64] * 5\n",
    "    lm_head_adapter_params['sample_features'] = 0.7\n",
    "    lm_head_adapter_params['composition_size'] = 10\n",
    "\n",
    "    \n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False\n",
    "\n",
    "\n",
    "    lm_head_adapter_params['recreate_lm_head'] = False\n",
    "    \n",
    "\n",
    "    depth2freeze = 0#32#28\n",
    "\n",
    "elif model_version == \"light_1\":\n",
    "    #размерность \"родного\" трансформера\n",
    "    transformer_adapter_params['original_transformer_size'] = 2048\n",
    "    #размерность адаптера! Может не совпадать с размерностью \"родного\" трансформера\n",
    "    transformer_adapter_params['embed_dim'] = 2048\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 32\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 2\n",
    "    transformer_adapter_params['t_layers_count'] = 2\n",
    "    transformer_adapter_params['layer_configs'] = [1024*2] * 10\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так здесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 100#10000\n",
    "    optimizer_params['lr'] = 1e-6#lr для LM_head\n",
    "    optimizer_params['transformer_update_rate'] = 1.#0.2#доля случаев, когда через трансформерный адаптер реально идут градиенты. Влияет на быстродействие\n",
    "    optimizer_params['lr_transformer'] = optimizer_params['lr'] * 0.1 * optimizer_params['transformer_update_rate']#lr для трансформерного адаптера\n",
    "    \n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-3#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    transformer_adapter_params['concat'] = True\n",
    "    transformer_adapter_params['wide'] = True\n",
    "\n",
    "    optimizer_params['opt_type'] = 'adam'#'adam'\n",
    "\n",
    "    #адаптер для LM-head \n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    #Ну типа исходный трансформер + все слои адаптера через конкат\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim'] * transformer_adapter_params['t_layers_count'] + transformer_adapter_params['original_transformer_size']\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [1024 * 3] * 4 + [1024 * 2] * 2 + [1024 * 1] * 8\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False\n",
    "\n",
    "    depth2freeze = 0\n",
    "\n",
    "\n",
    "elif model_version == \"light_2\":\n",
    "    transformer_adapter_params['embed_dim'] = 2048\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    #трансформерный адаптер\n",
    "    transformer_adapter_params['num_heads_tlayer'] = 32\n",
    "    transformer_adapter_params['ff_dim'] = transformer_adapter_params['embed_dim']  * 2\n",
    "    transformer_adapter_params['t_layers_count'] = 2\n",
    "    transformer_adapter_params['layer_configs'] = [1024*4] * 8\n",
    "    optimizer_params['batch_size'] = 1#почему-то распараллеливание ухудшает скорость, а так щдесь можно поставить и другое число\n",
    "    optimizer_params['accum_batch'] = 1000#10000\n",
    "    optimizer_params['lr'] = 1e-6#lr для LM_head\n",
    "    optimizer_params['transformer_update_rate'] = 0.#0.2#доля случаев, когда через трансформерный адаптер реально идут градиенты. Влияет на быстродействие\n",
    "    optimizer_params['lr_transformer'] = optimizer_params['lr'] * 0.1 * optimizer_params['transformer_update_rate']#lr для трансформерного адаптера\n",
    "    \n",
    "    transformer_adapter_params['transformer_adapter_weight'] = 1e-4#идея в том, что со старта адаптерный слой должен влиять совсем чуть-чуть\n",
    "    transformer_adapter_params['dropout'] = 0.1\n",
    "    transformer_adapter_params['concat'] = True\n",
    "    optimizer_params['opt_type'] = 'adam'#'adam'\n",
    "\n",
    "    #адаптер для LM-head\n",
    "    lm_head_adapter_params['embedding_size'] = transformer_adapter_params['embed_dim']\n",
    "    lm_head_adapter_params['heavy_head'] = True\n",
    "    lm_head_adapter_params['net_dropout_rate'] = 0\n",
    "    lm_head_adapter_params['individ_dropout_rate'] = 0.03\n",
    "    lm_head_adapter_params['layer_configs_head'] = [1024 * 4] * 7 + [1024 * 2] * 8\n",
    "    lm_head_adapter_params['sample_features'] = 1\n",
    "    lm_head_adapter_params['composition_size'] = 1\n",
    "    lm_head_adapter_params['memnet_params'] = {}\n",
    "    lm_head_adapter_params['use_memnets'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c9abcd-c2b0-4e10-9a32-5e516e321c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_adapter_params['head_max_batch_size'] = head_max_batch_size\n",
    "lm_head_adapter_params['learnable_linear_model'] = learnable_linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3c61a7-a882-40ae-bf7e-65b844fd0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2tadapter = f\"tadapter_{model_version}.pth\"\n",
    "path2lmhead = f\"lmhead_{model_version}.pth\"\n",
    "tokenizer_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211d07a1-531e-46f1-9aa1-ecd890e6704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#настройки датасета\n",
    "\n",
    "max_loss_len = 3500#столько токенов можно пихать в Y, не больше\n",
    "if mode == 'finetune':\n",
    "    #претрейн\n",
    "    max_seq_len4inference = 6850#в X и в Y можно пихать столько токенов, в каждый. Не больше.\n",
    "else:\n",
    "    #дообучение\n",
    "    max_seq_len4inference = 6850\n",
    "    #max_seq_len4inference = 5050\n",
    "    #max_seq_len4inference = 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a99486-1f3f-4b83-b57a-88fe00ee0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "#Загрузили исходную модельку\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)#, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_name,\n",
    "              quantization_config=bnb_config,\n",
    "              cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f308a2e1-93f4-42c1-8ff6-42eb4239c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.lm_head\n",
    "torch.save(model.lm_head.weight, \"lin_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac2e1da-5e9f-4ec9-a3c8-e3f4229ad40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length=1000000, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        \n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8a7526-ec4c-40b7-8320-4a469f360318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155497"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "#такая конструкция нужна на случай, что вы загружаете датасет на диск, сейчас час ночи, \n",
    "#датасет догрузится примерно через час, а завтра вам рано вставать\n",
    "#с таким подходм код будет выполняться, когда найдёт на диске датасет\n",
    "while 1:\n",
    "    try:\n",
    "        if mode == 'pretrain':\n",
    "            #dataset = InstructDatasetR(\"./data/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_flat.pkl\", tokenizer, max_seq_length=1000000, cut=None)\n",
    "        else:\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_length=1000000, cut=None)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e, pd.Timestamp.now())\n",
    "        time.sleep(10)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00128664-8ce8-48d9-bf3e-ba7fb99ad2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens\n",
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)\n",
    "def count_trainable_parameters(model: nn.Module) -> tuple[int, int]:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable, frozen\n",
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d252c6c-567b-4166-a38c-be91a3d05abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a5c48-2a64-406d-bcce-0f624ad4dd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882c53b2-f9ce-41f6-b485-f6aafb22340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#теперь сохранение адаптеров\n",
    "def save_adapters(head, tadapter, path2lmhead, path2tadapter):\n",
    "    head.training = False\n",
    "    torch.save(head, path2lmhead)\n",
    "    torch.save(tadapter, path2tadapter)\n",
    "    head.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dfef2eb-45c5-46e2-ac7a-93981027dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tadapter\n",
      "loading heavy LM head\n",
      "Head. Обучаемые: 2,433,850,752\n",
      "Замороженные: 0\n",
      "Transformer adapter. Обучаемые: 0\n",
      "Замороженные: 373,848,393\n"
     ]
    }
   ],
   "source": [
    "head, tadapter, optimizer = assemble_model(model, \n",
    "                path2lmhead, \n",
    "                path2tadapter, \n",
    "                start_train=start_train,\n",
    "                to_generate=False,\n",
    "                lm_head_adapter_params=lm_head_adapter_params,\n",
    "                transformer_adapter_params=transformer_adapter_params,\n",
    "                optimizer_params=optimizer_params\n",
    "              )\n",
    "if optimizer_params['transformer_update_rate'] == 0.:\n",
    "    tadapter.transformer_float_mode = 16\n",
    "    tadapter.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "070c93d9-ed7b-4f9f-bcb4-d47b3799475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet depth 52 depth2freeze 0\n"
     ]
    }
   ],
   "source": [
    "#заморозить первые N слоёв выходного резнета\n",
    "print('resnet depth', len(head.submodels[0].layers), 'depth2freeze', depth2freeze)\n",
    "for i in range(len(head.submodels) - 1):\n",
    "    for l_num in range(depth2freeze):\n",
    "        for p in head.submodels[i].layers[l_num].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "# for l_num in range(depth2freeze + 2):\n",
    "#     for p in head.submodels[0].layers[l_num].parameters():\n",
    "#         print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2990d7-4218-4aaf-b948-ae750a8cb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#head.submodels[0].layers[-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cf32a5c-8f32-4124-a2e3-cd9943d3a841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=5735, out_features=256, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Dropout(p=0.03, inplace=False)\n",
       "  (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Dropout(p=0.03, inplace=False)\n",
       "  (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (8): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (9): LeakyReLU(negative_slope=0.01)\n",
       "  (10): Dropout(p=0.03, inplace=False)\n",
       "  (11): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (13): LeakyReLU(negative_slope=0.01)\n",
       "  (14): Dropout(p=0.03, inplace=False)\n",
       "  (15): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (16): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (17): LeakyReLU(negative_slope=0.01)\n",
       "  (18): Dropout(p=0.03, inplace=False)\n",
       "  (19): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (20): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (21): LeakyReLU(negative_slope=0.01)\n",
       "  (22): Dropout(p=0.03, inplace=False)\n",
       "  (23): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (24): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (25): LeakyReLU(negative_slope=0.01)\n",
       "  (26): Dropout(p=0.03, inplace=False)\n",
       "  (27): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (28): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (29): LeakyReLU(negative_slope=0.01)\n",
       "  (30): Dropout(p=0.03, inplace=False)\n",
       "  (31): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (32): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (33): LeakyReLU(negative_slope=0.01)\n",
       "  (34): Dropout(p=0.03, inplace=False)\n",
       "  (35): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (36): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (37): LeakyReLU(negative_slope=0.01)\n",
       "  (38): Dropout(p=0.03, inplace=False)\n",
       "  (39): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (40): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (41): LeakyReLU(negative_slope=0.01)\n",
       "  (42): Dropout(p=0.03, inplace=False)\n",
       "  (43): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (44): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (45): LeakyReLU(negative_slope=0.01)\n",
       "  (46): Dropout(p=0.03, inplace=False)\n",
       "  (47): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (48): Linear(in_features=1472, out_features=128256, bias=True)\n",
       "  (49): Dropout(p=0.03, inplace=False)\n",
       "  (50): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "  (51): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.submodels[0].layers#[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9977600-efbf-435d-a573-69ed74961c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.04\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b39554-479c-41a8-8dcf-1b7456c753e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tadapter\n",
      "loading heavy LM head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0 days 00:02:18.386420\n",
      "{'fact_score': 0.174, 'translation_rouge': 0.25744, 'reasoning_score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#снять метрики\n",
    "if not start_train:\n",
    "    assemble_model(model, \n",
    "        path2lmhead, \n",
    "        path2tadapter, \n",
    "        start_train=False,\n",
    "        to_generate=True,\n",
    "        lm_head_adapter_params=lm_head_adapter_params,\n",
    "        transformer_adapter_params=transformer_adapter_params,\n",
    "        optimizer_params=optimizer_params\n",
    "      )\n",
    "    current_benchmark = LLMBenchmark()\n",
    "    results = current_benchmark.run(model, tokenizer, device)\n",
    "    disassemble_model(model)\n",
    "    print(results)\n",
    "#{'fact_score': 0.109, 'translation_rouge': 0.33223}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46c99a49-fa06-42f4-9f11-4527bb296bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 12:00:32.449286\n"
     ]
    }
   ],
   "source": [
    "print(pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586d9dd-8124-4868-b5ea-d0c6238a85d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1\n"
     ]
    }
   ],
   "source": [
    "#TRAIN!!!\n",
    "#Если вы задаётесь вопросом, почему такой странный цикл обучения, давайте посмотрим, что не сработало в \"нормальных\" циклах обучения.\n",
    "#Можно было запихнуть трансформерный адаптер и lm-resnet в исходную LLM, разморозить их и заморозить всё остальное. \n",
    "#НО тогда по непонятной причине выходила крайне плохая точность обучения. loss = 1.65 на датасете, где у классического TEA (ну, чисто резнеты) loss = 0.85\n",
    "#Две недели поисков выявили, что видимое различие в том, что в одном случае адаптер обучает ВНУТРИ сетки, а в другом СНАРУЖИ. Снаружи ПОЧЕМУ-ТО лучше\n",
    "#\n",
    "#Если проинференсить трансформерный адаптер, а потом LM-head, то запросто может получиться, что на этапе backward мы сдохнем по памяти.\n",
    "#Как фиксить? Делаем, чтобы для всего множества эмбеддингов от трансформерной части loss считался батчами\n",
    "#В таком случае возникает ошибка, мол, чё нам делать с градиентами в трансформерном адаптере - мы же их уже посчитали, а ты предлагаешь считать ещё раз!\n",
    "#Решаем так: берёт сабсемпл от эмбеддингов трансформерного адаптера и считаем loss на сабсемпле\n",
    "#Это не вполне честно, так как мы менее тщательно учитываем длинные последовательности, но эй, это работает!\n",
    "\n",
    "print('batch_size', optimizer_params['batch_size'])\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "\n",
    "log = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < optimizer_params['batch_size']:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        #print('input_ids', input_ids.shape, 'labels', labels.shape)\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "        #log += [labels_cur.shape[0], input_ids_cur.shape[0]]\n",
    "\n",
    "    i_pointer += i #should be batch_size\n",
    "    with torch.no_grad():\n",
    "        input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "        labels_full = torch.stack(labels_full).to('cuda')\n",
    "        weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "        cnt_pads = count_padding(labels_full, padding_token)\n",
    "        if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "            continue\n",
    "        if cnt_pads > 0:\n",
    "            input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "            labels_full = labels_full[:, :-cnt_pads]\n",
    "            \n",
    "        outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=True)\n",
    "        #outp = model.forward(input_ids_full, output_hidden_states=False, return_dict=True)\n",
    "\n",
    "        \n",
    "        outp['states'] = outp['hidden_states'][-1]\n",
    "        outp['states'] = outp['states'][:, :input_ids_full.shape[1]].detach()\n",
    "        #state_full = outp.last_hidden_state[:, :input_ids_full.shape[1]]\n",
    "        state_full = outp['states']\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        \n",
    "        weights_cur = weights_full2d.ravel()\n",
    "\n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        del outp\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "        weights_cur = weights_cur[~idx]\n",
    "    if state_full.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    #Для скорости считаем градиенты через трансформер не всегда\n",
    "    req_current = np.random.rand() < optimizer_params['transformer_update_rate']\n",
    "    for p in tadapter.parameters():\n",
    "        p.requires_grad = req_current\n",
    "\n",
    "    state_full = tadapter(state_full)[0]\n",
    "    state_cur = state_full.reshape([state_full.shape[0] * state_full.shape[1], state_full.shape[2]])\n",
    "    state_cur = state_cur[~idx]\n",
    "    if state_cur.shape[0] > max_loss_len:\n",
    "        state_cur = state_cur[:max_loss_len]\n",
    "\n",
    "    #ограничить размер\n",
    "    \n",
    "    #print('state_cur', state_cur.shape)\n",
    "    del state_full\n",
    "    perm = torch.randperm(state_cur.size(0), device=device)\n",
    "    state_cur = state_cur[perm]\n",
    "    weights_cur = weights_cur[perm]\n",
    "    labels_cur = labels_cur[perm]\n",
    "\n",
    "    loss = None\n",
    "    for x_start_pointer in range(0, state_cur.shape[0], max_batch_size4loss):\n",
    "        #logits = head(state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_batch_size4loss])\n",
    "        if use_cp_for_head:\n",
    "            logits = torch.utils.checkpoint.checkpoint(head, state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_batch_size4loss])\n",
    "        else:\n",
    "            logits = head(state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_batch_size4loss])\n",
    "        ce_loss = loss_fct(logits, labels_cur[x_start_pointer: x_start_pointer + max_batch_size4loss].view(-1))\n",
    "        ce_loss = ce_loss * weights_cur[x_start_pointer: x_start_pointer + max_batch_size4loss].to('cuda')\n",
    "        ce_loss = torch.clamp(ce_loss, -0.9, 10.0)#soft_clip(ce_loss, -0.6, 10.)\n",
    "        loss = torch.mean(ce_loss)\n",
    "\n",
    "        #l2 регуляризация\n",
    "        l2_reg = torch.tensor(0.0, device=device)\n",
    "        for param in tadapter.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg += torch.norm(param)\n",
    "        for param in head.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg += torch.norm(param)\n",
    "        #l2_reg = sum(p.norm(2) ** 2 for p in chain(tadapter.parameters(), head.parameters()) / 1e6)\n",
    "                     \n",
    "        total_loss = loss + lambda_l2 * l2_reg\n",
    "        \n",
    "        total_loss.backward()\n",
    "        # Клиппинг градиентов: обрезает норму градиентов до 2.0\n",
    "        #torch.nn.utils.clip_grad_norm_(tadapter.parameters(), max_norm=2.0 * optimizer_params['accum_batch'])\n",
    "        #torch.nn.utils.clip_grad_norm_(head.parameters(), max_norm=2.0 * optimizer_params['accum_batch'])\n",
    "        \n",
    "        # if state_cur.shape[0] - (x_start_pointer + max_batch_size4loss) > 0:\n",
    "        #     print('cannot backward, shape', state_cur.shape[0] - (x_start_pointer + max_batch_size4loss))\n",
    "        break\n",
    "       \n",
    "    del state_cur\n",
    "    del weights_cur\n",
    "    \n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits, axis=-1) == labels_cur[x_start_pointer: x_start_pointer + max_batch_size4loss]).to(torch.float16))\n",
    "    acc_copy = acc.cpu().numpy()\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= optimizer_params['accum_batch'] :\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "        if (np.random.rand() < 1 and optimizer_params['accum_batch'] > 500) or (np.random.rand() < 0.1 and optimizer_params['accum_batch']  <= 500):\n",
    "            try:\n",
    "                save_adapters(head, tadapter, path2lmhead, path2tadapter)\n",
    "                if np.random.rand()<0.1:\n",
    "                    save_adapters(head, tadapter, path2lmhead + '.back', path2tadapter + '.back')\n",
    "\n",
    "            except Exception:\n",
    "                #Я не хочу, чтобы скрипт падал только потому, что кто-то завалил веь диск\n",
    "                print(\"not saved\")\n",
    "            #снять метрики\n",
    "            assemble_model(model, \n",
    "                path2lmhead, \n",
    "                path2tadapter, \n",
    "                start_train=False,\n",
    "                to_generate=True,\n",
    "                lm_head_adapter_params=lm_head_adapter_params,\n",
    "                transformer_adapter_params=transformer_adapter_params,\n",
    "                optimizer_params=optimizer_params\n",
    "              )\n",
    "            current_benchmark = LLMBenchmark()\n",
    "            results = current_benchmark.run(model, tokenizer, device,4)\n",
    "            disassemble_model(model)\n",
    "            print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ad884-d47e-4d8a-82c3-60f4e2b120d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#33 min за 1300 при batch_size=1\n",
    "#45 min за 1300 при batch_size=2\n",
    "#21 min за 1300 при batch_size=1, 'transformer_update_rate' = 0.1\n",
    "#24 min за 1300 при batch_size=1, 'transformer_update_rate' = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d448-2a4d-4153-adbf-5bdfc27b6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print(batch_accum_counter)\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3705-a864-4c7f-bf5e-5cc592eae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ручной сейв\n",
    "save_adapters(head, tadapter, path2lmhead, path2tadapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9bab7-e955-407d-b6ce-1d25f44d0277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19e26e-4153-486c-912b-b8bab08f4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестим модель\n",
    "assemble_model(model, \n",
    "                path2lmhead, \n",
    "                path2tadapter, \n",
    "                start_train=False,\n",
    "                to_generate=True,\n",
    "                lm_head_adapter_params=lm_head_adapter_params,\n",
    "                transformer_adapter_params=transformer_adapter_params,\n",
    "                optimizer_params=optimizer_params\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ef640-f536-4479-92a1-1f285fdebe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = 'Твоя роль: Сталин. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "# prompt = ''''Шэдоухарт девушка сорока восьми лет. У неё черные волосы, заплетенные в длинную косу, и зеленые глаза, а справа на лице присутствует длинный шрам. Облачена в среднюю броню, вооружена булавой и круглым деревянным щитом. На голове девушка носит металлический обруч.\n",
    "\n",
    "# Психологический профиль\n",
    "# Шэдоухарт осторожна и не склонна раскрывать информацию о себе, своих мотивах и реликвии, которую она носит. С особым недоверием она'''\n",
    "#prompt = 'Твоя роль: Девушка-флористка. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Data scientist. \\n Серёга: Мне надо улучшить LLM. У меня ресурсы на запуск Llama 3.1 8B или Qwen 2.5 14B, квантованные до 4 бит. Мне нужно, чтобы мои модели былу умнее (на уровне 70 B), но при этом имели настолько же низкие требования по памяти и запускались так же быстро. Я готов обучать новую модель. Я готов пробовать другие архитектуры. Опиши план, что делать. Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Инженер-ракетчик. \\n Серёга: Мне надо сделать ракету, в домашних условиях, небольшую. Расскажи, как это провернуть? ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Геймер, студент-инженер. \\n Серёга: Перечисли всех монстров из Doom, каких вспомнишь! Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Гермиона Грейнджер. \\n Серёга: Как справиться с василиском? Your answer, НА РУССКОМ (end by \"|\", 200 tokens):'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "repetition_penalty = 1.2\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                        top_k=200,\n",
    "                                        top_p=0.9,\n",
    "                                        max_new_tokens=200,\n",
    "                                        temperature=0.01,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        #no_repeat_ngram_size=3,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        early_stopping=True,\n",
    "                                        use_cache=True,\n",
    "                                        num_beams=1,\n",
    "                                        tokenizer=tokenizer)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b56d4-fcaf-4bb3-b156-8c2627b4b773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b484f-c956-4e3e-b372-636eb3f64061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
