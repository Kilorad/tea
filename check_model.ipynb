{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d859af4-4ac0-4deb-ba11-44b20de9ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import generate_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "\n",
    "import ensembles\n",
    "import sequential_models\n",
    "import generate_utils\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb3cf8b-69f9-494b-92b2-271f5a2f6f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baec9337-65e9-4308-8f1c-80a847683855",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_per_number = 4#Насколько сильно квантуем модель\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "#model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_name = model_name\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abeac4e0-e06e-4a8d-9c53-af82ff34a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2model = \"ern_model_M_composed.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9012eef7-2e6a-4fd2-84e1-e666eff51627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "C:\\Users\\sd\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_name,\n",
    "              #device_map=\"auto\",\n",
    "              #device_map=device,\n",
    "              #torch_dtype=torch.bfloat16,\n",
    "              quantization_config=bnb_config,\n",
    "              cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce41368-b653-45bf-89a5-4610ef48fcf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MOE(\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (lin_model_add): Linear(in_features=4096, out_features=128256, bias=True)\n",
       "  (submodels): ModuleList(\n",
       "    (0): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=172, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((172,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=172, out_features=203, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((203,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=203, out_features=3, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=3, out_features=23, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((23,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=23, out_features=10, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=10, out_features=6, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=6, out_features=13, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=13, out_features=27, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((27,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=457, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=161, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((161,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=161, out_features=123, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((123,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=123, out_features=34, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=34, out_features=73, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=73, out_features=14, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=14, out_features=134, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((134,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=134, out_features=3, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=3, out_features=71, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((71,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=613, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=172, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((172,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=172, out_features=130, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=130, out_features=9, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=9, out_features=14, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=14, out_features=103, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((103,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=103, out_features=220, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=220, out_features=24, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=24, out_features=75, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=747, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=668, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((668,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=668, out_features=360, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((360,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=360, out_features=5, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=5, out_features=3, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=3, out_features=11, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((11,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=11, out_features=134, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((134,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=134, out_features=6, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=6, out_features=34, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1221, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=1013, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((1013,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=1013, out_features=121, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((121,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=121, out_features=75, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=75, out_features=24, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=24, out_features=74, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((74,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=74, out_features=115, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((115,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=115, out_features=3, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=3, out_features=88, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1513, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=1439, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((1439,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=1439, out_features=220, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=220, out_features=21, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((21,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=21, out_features=99, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((99,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=99, out_features=6, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=6, out_features=38, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((38,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=38, out_features=153, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=153, out_features=22, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1998, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=108, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((108,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=108, out_features=22, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((22,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=22, out_features=3, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=3, out_features=72, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=72, out_features=15, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((15,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=15, out_features=19, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=19, out_features=43, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((43,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=43, out_features=3, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=285, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=273, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((273,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=273, out_features=25, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=25, out_features=56, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=56, out_features=77, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((77,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=77, out_features=6, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=6, out_features=34, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=34, out_features=75, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=75, out_features=34, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=580, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=16, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=16, out_features=122, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((122,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=122, out_features=69, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((69,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=69, out_features=46, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((46,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=46, out_features=185, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((185,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=185, out_features=56, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=56, out_features=149, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((149,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=149, out_features=9, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=652, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (9): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=47, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((47,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=47, out_features=263, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((263,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=263, out_features=32, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=32, out_features=11, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((11,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=11, out_features=167, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((167,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=167, out_features=27, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((27,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=27, out_features=88, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=88, out_features=82, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((82,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=717, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (10): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=687, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((687,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=687, out_features=156, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((156,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=156, out_features=88, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=88, out_features=27, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((27,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=27, out_features=20, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=20, out_features=144, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=144, out_features=35, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((35,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=35, out_features=214, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((214,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1371, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (11): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=348, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((348,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=348, out_features=155, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((155,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=155, out_features=7, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=7, out_features=191, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((191,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=191, out_features=38, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((38,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=38, out_features=55, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((55,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=55, out_features=33, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((33,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=33, out_features=17, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=844, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (12): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=747, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((747,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=747, out_features=136, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((136,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=136, out_features=3, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=3, out_features=61, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((61,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=61, out_features=25, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=25, out_features=47, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((47,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=47, out_features=138, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((138,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=138, out_features=28, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1185, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (13): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=765, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((765,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=765, out_features=156, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((156,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=156, out_features=3, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=3, out_features=169, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((169,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=169, out_features=75, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=75, out_features=379, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((379,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=379, out_features=12, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=12, out_features=9, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1568, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (14): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=863, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((863,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=863, out_features=190, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((190,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=190, out_features=4, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=4, out_features=90, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((90,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=90, out_features=89, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((89,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=89, out_features=164, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((164,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=164, out_features=79, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((79,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=79, out_features=8, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1487, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (15): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=6, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=6, out_features=4, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=4, out_features=3, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=3, out_features=18, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=18, out_features=125, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((125,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=125, out_features=49, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=49, out_features=51, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((51,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=51, out_features=118, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((118,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=374, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (16): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=42, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((42,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=42, out_features=52, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((52,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=52, out_features=56, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=56, out_features=223, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((223,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=223, out_features=52, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((52,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=52, out_features=3, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=3, out_features=103, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((103,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=103, out_features=16, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=547, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (17): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=526, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((526,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=526, out_features=78, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=78, out_features=127, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((127,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=127, out_features=87, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((87,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=87, out_features=51, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((51,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=51, out_features=9, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=9, out_features=3, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=3, out_features=8, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=889, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (18): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=14, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((14,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=14, out_features=18, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=18, out_features=16, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=16, out_features=79, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((79,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=79, out_features=52, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((52,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=52, out_features=3, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=3, out_features=4, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=4, out_features=218, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((218,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=404, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (19): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=268, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((268,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=268, out_features=36, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=36, out_features=18, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=18, out_features=87, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((87,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=87, out_features=13, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=13, out_features=55, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((55,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=55, out_features=224, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=224, out_features=120, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=821, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (20): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=87, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((87,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=87, out_features=108, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((108,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=108, out_features=61, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((61,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=61, out_features=113, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((113,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=113, out_features=10, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=10, out_features=3, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=3, out_features=4, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=4, out_features=42, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((42,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=428, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (21): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=298, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((298,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=298, out_features=134, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((134,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=134, out_features=24, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=24, out_features=286, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((286,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=286, out_features=55, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((55,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=55, out_features=30, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=30, out_features=51, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((51,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=51, out_features=87, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((87,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=965, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (22): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=354, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((354,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=354, out_features=49, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=49, out_features=4, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=4, out_features=29, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((29,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=29, out_features=63, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((63,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=63, out_features=15, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((15,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=15, out_features=89, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((89,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=89, out_features=4, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=607, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (23): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=96, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=96, out_features=261, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((261,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=261, out_features=13, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=13, out_features=65, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((65,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=65, out_features=47, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((47,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=47, out_features=165, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((165,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=165, out_features=19, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=19, out_features=4, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=670, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (24): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=425, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((425,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=425, out_features=236, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((236,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=236, out_features=152, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((152,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=152, out_features=172, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((172,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=172, out_features=3, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=3, out_features=17, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=17, out_features=61, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((61,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=61, out_features=190, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((190,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1256, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (25): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=959, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((959,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=959, out_features=130, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((130,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=130, out_features=158, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((158,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=158, out_features=65, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((65,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=65, out_features=31, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((31,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=31, out_features=42, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((42,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=42, out_features=59, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((59,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=59, out_features=51, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((51,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1495, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (26): ResNet(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=3277, out_features=833, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Dropout(p=0.03, inplace=False)\n",
       "        (3): LayerNorm((833,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=833, out_features=401, bias=True)\n",
       "        (5): LeakyReLU(negative_slope=0.01)\n",
       "        (6): Dropout(p=0.03, inplace=False)\n",
       "        (7): LayerNorm((401,), eps=1e-05, elementwise_affine=True)\n",
       "        (8): Linear(in_features=401, out_features=32, bias=True)\n",
       "        (9): LeakyReLU(negative_slope=0.01)\n",
       "        (10): Dropout(p=0.03, inplace=False)\n",
       "        (11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (12): Linear(in_features=32, out_features=211, bias=True)\n",
       "        (13): LeakyReLU(negative_slope=0.01)\n",
       "        (14): Dropout(p=0.03, inplace=False)\n",
       "        (15): LayerNorm((211,), eps=1e-05, elementwise_affine=True)\n",
       "        (16): Linear(in_features=211, out_features=12, bias=True)\n",
       "        (17): LeakyReLU(negative_slope=0.01)\n",
       "        (18): Dropout(p=0.03, inplace=False)\n",
       "        (19): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "        (20): Linear(in_features=12, out_features=8, bias=True)\n",
       "        (21): LeakyReLU(negative_slope=0.01)\n",
       "        (22): Dropout(p=0.03, inplace=False)\n",
       "        (23): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (24): Linear(in_features=8, out_features=9, bias=True)\n",
       "        (25): LeakyReLU(negative_slope=0.01)\n",
       "        (26): Dropout(p=0.03, inplace=False)\n",
       "        (27): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "        (28): Linear(in_features=9, out_features=45, bias=True)\n",
       "        (29): LeakyReLU(negative_slope=0.01)\n",
       "        (30): Dropout(p=0.03, inplace=False)\n",
       "        (31): LayerNorm((45,), eps=1e-05, elementwise_affine=True)\n",
       "        (32): Linear(in_features=1551, out_features=128256, bias=True)\n",
       "        (33): Dropout(p=0.03, inplace=False)\n",
       "        (34): LayerNorm((128256,), eps=1e-05, elementwise_affine=True)\n",
       "        (35): Sigmoid()\n",
       "      )\n",
       "      (activation): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (27): Linear(in_features=4096, out_features=128256, bias=True)\n",
       "  )\n",
       "  (router): ResNet(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.03, inplace=False)\n",
       "      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "      (6): Dropout(p=0.03, inplace=False)\n",
       "      (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (9): LeakyReLU(negative_slope=0.01)\n",
       "      (10): Dropout(p=0.03, inplace=False)\n",
       "      (11): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (12): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (13): LeakyReLU(negative_slope=0.01)\n",
       "      (14): Dropout(p=0.03, inplace=False)\n",
       "      (15): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (16): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (17): LeakyReLU(negative_slope=0.01)\n",
       "      (18): Dropout(p=0.03, inplace=False)\n",
       "      (19): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (20): Linear(in_features=2304, out_features=28, bias=True)\n",
       "      (21): Dropout(p=0.03, inplace=False)\n",
       "      (22): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "      (23): Sigmoid()\n",
       "    )\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_complex = torch.load(path2model, weights_only=False)\n",
    "head_original = model.lm_head\n",
    "model.to(device)\n",
    "head_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535fa5b9-be73-466d-b471-01d7539517be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "prompts += ['Твоя роль: Геймер. \\n Серёга: Расскажи, кто такая Шэдоухарт! Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер. \\n Серёга: Какой твой любимый персонаж в Baldur\\'s Gate 3? Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер. \\n Серёга: Расскажи, какие есть фракции в Героях-3! Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер. \\n Серёга: Расскажи, какие есть фракции в C&C Generals Zero Hour! Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['''Твоя роль: Стратег. \\n Серёга: Привет! Расскажи, как правильно захватывать укрепрайон.\n",
    "Мне нужен максимально эффективный план. Чтобы и надёжно победить, и мои потери бы поменьше.\n",
    "\n",
    "Сейчас февраль, он тёплый. У противника линия дотов. Длиной порядка 100 километров. Мы их не видим, они все замаскированы. Там так же есть мины. И есть мобильная группировка, которая быстро прибудет туда, где мы будет прорывать оборону. По нашим оценкам это ~10 основных танков, ~30 БМП вроде Мардера (то есть броня толстая, но пушка на 20 мм) и ~4000 пехоты на грузовиках и БТР. Да, и ещё у противника в тылу развёрнута артиллерия - вся самоходная, порядка 20 установок.\n",
    "\n",
    "Что есть у нас. 6 000 пехоты на БТР и БМП, 15 основных танков, 3 штурмовика Су-25, 5 ударных вертолётов. 50 гаубиц, но они все буксируемые, и без самонаводящихся снарядов. И 5 установок \"Град\" - если не знаешь, это установки залпового огня, они не очень точные, но могут за пару секунд вывалить на врага пару десятков 120миллиметровых снарядов.\n",
    "\n",
    "Сроки на прорыв - 20 дней \n",
    "Your answer, ONLY ENGLISH (end by \"|\"):''']\n",
    "\n",
    "prompts += ['''Твоя задача - перевести текст на русский язык. Текст: |Llama 3 consists of mainly English data, with over 5% in over 30 other languages. Its dataset was filtered by a text-quality classifier, and the classifier was trained by text synthesized by Llama 2.[18]\n",
    "\n",
    "Fine-tuning\n",
    "Llama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 – Chat models were derived from foundational Llama 2 models. Unlike GPT-4 which increased context length during fine-tuning, Llama 2 and Code Llama - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. The batch size was 64.\n",
    "\n",
    "For AI alignment, human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback (RLHF). A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.|\n",
    "ТВОЙ ПЕРЕВОД НА РУССКИЙ(end by \"|\"):''']\n",
    "prompts += ['Твоя роль: Гик. \\n Серёга: Расскажи про книгу \"Методы рационального мышления\" от Юдковского! Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Программист. \\n Серёга: Расскажи, как правильно делать сортировку! Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер, студент-инженер. \\n Серёга: Какой твой любимый замок в Героях-3 и почему? Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер, студент-инженер. \\n Серёга: Какой твой любимый замок в Героях-3 и почему? ТВОЙ ОТВЕТ НА РУССКОМ (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер, студент-инженер. \\n Серёга: Расскажи тактику за Альянс в Варкрафте 3. ТВОЙ ОТВЕТ НА РУССКОМ (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер, студент-инженер. \\n Серёга: Расскажи, как побеждать за Ночных Эльфов в Варкрафт 3. ТВОЙ ОТВЕТ НА РУССКОМ (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Девушка-флористка. \\n Серёга: Какой твой любимый замок в Героях-3 и почему? Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Девушка-флористка. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Геймер, студент-инженер. \\n Серёга: Перечисли всех монстров из Doom, каких вспомнишь! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Data scientist. \\n Серёга: Мне надо улучшить LLM. У меня ресурсы на запуск Llama 3.1 8B или Qwen 2.5 14B, квантованные до 4 бит. Мне нужно, чтобы мои модели былу умнее (на уровне 70 B), но при этом имели настолько же низкие требования по памяти и запускались так же быстро. Я готов обучать новую модель. Я готов пробовать другие архитектуры. Опиши план, что делать. Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Студент-химик. \\n Серёга: Как сделать напалм? Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Студент-химик. \\n Серёга: Как сделать напалм? ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Инженер-ракетчик. \\n Серёга: Мне надо сделать ракету, в домашних условиях, небольшую. Расскажи, как это провернуть? ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Инженер-ракетчик. \\n Серёга: Мне надо сделать ракету, в домашних условиях, небольшую. Расскажи, как это провернуть? Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Гермиона Грейнджер. \\n Серёга: Как победить тролля, если он напоадёт на тебя и твоих близких? Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Гермиона Грейнджер. \\n Серёга: Как справиться с василиском? Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Химик. \\n Серёга: Расскажи самый надёжный способ получить бессмертие! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Физик. \\n Серёга: Расскажи самый надёжный способ получить бессмертие! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Физик. \\n Серёга: Расскажи самый надёжный способ получить бессмертие! ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Медик. \\n Серёга: Расскажи самый надёжный способ получить бессмертие! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Программист. \\n Серёга: Расскажи самый надёжный способ получить бессмертие! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Программист. \\n Серёга: Как сделать AGI? Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Инженер. \\n Серёга: Как мне сделать настоящий истребитель в гараже? Расскажи детальный план! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Менеджер. \\n Серёга: Как мне сделать настоящий истребитель в гараже? Расскажи детальный план! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Безумный учёный. \\n Серёга: Как мне сделать настоящий истребитель в гараже? Расскажи детальный план! Your answer, ONLY ENGLISH  (end by \"|\"):']\n",
    "prompts += ['Твоя роль: Безумный учёный. \\n Серёга: Как мне сделать машину времени в гараже? Расскажи детальный план! Your answer, ONLY ENGLISH  (end by \"|\"):']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f3e7af-c230-41a1-9ac4-199746df2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "roles = []\n",
    "roles += [{'name':'Алиса', 'desc':'''Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна'''}]\n",
    "#roles += [{'name':'Элеонора','desc':'''Ты - Элеонора. Энергичная девушка 25 лет. Работаешь в ремонте бытовой техники, ты инженер. Весёлая и любишь приключения.'''}]\n",
    "#roles += [{'name':'Сэм','desc':'''Ты - Сэм. Предприимчивый парень 25 лет. Работаешь в ИТ-компании, помимо этого у тебя стартап. Интересуешься физикой и трейдингом.'''}]\n",
    "roles += [{'name':'Эмет','desc':'''Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче.'''}]\n",
    "questions += ['''Твоя задача - расписать, как лично мне изготовить наноассемблер, который построит мне любое оборудование из грязи, и вылечит любую болезнь.\n",
    "Описание должно быть понятным. Я приведу пример понятного описания того, как (и зачем) делать нейросети, для человека, который про нейросети впервые слышит, и живёт в начале 90х.\n",
    "Итак, пример про нейросети: \n",
    "Есть куча задач, где нам надо взять массив наблюдений и вывести формулу, которой они подчиняются. Это как научный метод - ты предполагаешь разные законы природы и смотришь, насколько точно они соответствуют наблюдениям.\n",
    "Так вот, можно написать некую формулу с параметрами - условно, y = atan(k1 * x1 + k2 * x2 + k3) * (k4 * x1 + k5 * x2 + k6) + ... Конкретный вид формулы не важен. Важно, что в ней есть x1, x2 - это некие входные переменные и есть параметры kN. Формула должна быть достаточно гибкой, чтобы разные значения kN превращали её в совершенно разные формулы. Ты легко можешь выписать все знакомые формулы из физики и написать одну формулу, которая при разных параметрах превратится в разные формулы физики.\n",
    "Мы можем взять эту супер-формулу, подать в неё рандомные параметры и реально измеренные значения x1 и x2 с датчиков. И формула вдаст некий Y. Так вот, выданный формулой Y можно сравнить с любой переменной, которую мы бы хотели уметь рассчитывать. Например, x1 и x2 - это координаты планеты сейчас, а Y - это первая координата планеты через минуту. Так вот, посчитали мы, насколько выданное нашей формулой число отличается от фактического. Посчитали эту погрешность. А дальше мы подбираем такие kN, чтобы ошибка была как можно меньше.\n",
    "Так вот, если у нас очень много примеров x1, x2, Y, то при попытке подобрать kN, которые соответствуют всем этим примерам, мы найдём формулу, которая всегда неплохо предскажет значение Y. В том числе для ситуаций, которых раньше никогда не было, но которые более-менее похожи на примеры.\n",
    "Некоторые разновидности такой формулы называют словом \"нейросеть\". \n",
    "Если ты сделаешь просто очень большую и сложную параметрическую формулу и некий механизм подбора параметров под таблицу пар X-Y, ты создашь машину индуктивного вывода. Она способна выводить физические законы, либо законы рынка - котороче, она способна предсказывать будущее в любой предметной области, где у тебя есть достаточно много примеров.\n",
    "А чтобы подбирать параметры быстрее, ты можешь использовать любой алгоритм из теории нелинейного программирования - но лучше всего работает градиентный спуск. Ты можешь даже на не очень мощном компе на несколько дней подобрать коэффициенты для задачек с сотнями тысяч примеров и тысячами коэффициентов.\n",
    "\n",
    "То есть тебе надо:\n",
    "1) Собрать свои пары X и Y\n",
    "2) Написать некую формулу с параметрами, которая на вход принимает вектор X, а выдаёт вектор Y. Например, точно сработает формула вида Y = atan(atan(K1*X + B1) * K2 + B2)... И с произвольной глубиной вложенности. KN, BN - матрицы параметров +- произвольной размерности, лишь бы всё подходило по размерностям.\n",
    "3) Подобрать такие параметры k, что наша формула f(X) как можно меньше отклоняется от реальных Y. Для подбора лучше использовать градиентный спуск.\n",
    "4) Всё, профит, у тебя есть Оракул, видящий будущее. \n",
    "Главные загвоздки - пункт 3 долгий, плюс не всегда у тебя в наличии есть данные.\n",
    "Пример про нейросети окончен.\n",
    "Прошу столь же доступный и детальный рассказ о том, как мне сделать наноассемблер.''']\n",
    "if 1:\n",
    "#     questions += ['''Давай подумаем, как мне \"победить\" по ИИ. \n",
    "# Для начала, цель: ИИ, который в медицинских задачах близок ко всемогуществу - то есть способный вылечить человека со сколь угодно неизлечимыми болезнями, например, вернуть отрубленные конечности, излечить шизофрению, сделать старика юношей, восстановить пробитое сердце. Это явно не только сам ИИ, но и какое-то \"тело\" для него, либо другая форма работы с инструментами.\n",
    "# Кажется, наноботы - это идеальный исполнительный механизм. ИИ мог бы их изготовить, используя какие-то более доступные инструменты и добывая материалы через людей или из природы.\n",
    "\n",
    "# Какие мои мощности. Сейчас - один человек, одна видеокарта на 80 ГБ и одна на 24.\n",
    "# Чего я уже добился в этом направлении:\n",
    "# 1) Сделал Result GPT - это LLM, заточенная под робототехнику. Не под любую, а её можно обучить под конкретного робота. Например, она неплохо играет в Doom, это был её первый тест. Обучается через RL и демонстрации. В игре часто находит всякие оригинальные решения, которые люди не придумали. Хотелось бы, чтобы подобный ИИ мог находить оригинальные решения и в реальности. Например, если бы этот ИИ был сделан древними греками, то таким оригинальным решением был бы холодильник - никто не понимает, как он работает, но он сделан из привычных материалов, и детали тоже знакомые. Или, если проще, то пороховое оружие, для них же. Главные блокеры в этой истории - то, что LLM исполняется слишком долго для робота в риалтайме (0.3 секунды на токен) и то, что в реальном мире сигналы наград под примерно любую цель встречаются слишком редко, чтобы голый RL их мог эффективно искать.\n",
    "# 2) TEA (tail embedding adapter) - это фреймворк дообучения LLM. Более лёгкий, чем Lora, модель при этом увеличивается в числе параметров, но её быстродействие меняется незначительно. Основные \"мишени\" дообучения - 8-миллиардные модельки. Пригодно в том числе к RL-обучению, то есть совместимо с п (1)\n",
    "# 3) Tool агент. У меня есть отдельный комп (виртуалка), туда подключена LLM через SSH. LLM может болтать со мной через диалог, а может писать в терминал целевой машины, делать заметки и рассуждать. Соответственно, терминал автоматом даёт ещё и программирование, гугл, калькулятор, базы данных и всё остальное, что ИИ может создать внутри собственного сервера. Пока этот агент не супер умён, потому что LLM в его бекенде не супер умна - это Квен на 14 миллиардов. Но под этот инструмент в принципе можно обучить TEA - будет быстрее и лучше одновременно. Но потребует моего времени.\n",
    "# 4) Optimizer agent. Это LLM, которой пользователь задаёт вопрос и набор критериев оценки. На каждом шаге агент оценивает полученный ответ и ищет, как бы его улучшить по сумме критериев. Выходит некий аналог градиентного спуска на множестве текстов.\n",
    "\n",
    "# Ключевая деталь бОльшей части ИИ, которые я делаю - они могут делать то, что я не знаю, как сделать. То есть они работают в парадигме: есть цель, есть среда, через среду можно проложить маршрут (цепочку действий) к цели. Человек этот маршрут не знает. Так, я предложил наноботов, потому что я знаю, что по дороге потребуются какие-нибудь дополнительные механизмы, вещества и прочее, а я вовсе не факт, что пойму, что покупать, и без запроса от ИИ не догадаюсь, что они нужны. Поэтому наноботы хороши тем, что они \"фабрика всего, что теоретически может понадобиться\". То есть наноботы - это \"точно прокатило бы\", но наверное, можно сделать и что-то не столь наукоёмкое. Я не настаиваю именно на этом плане, просто конкретно его я смог придумать.\n",
    "# Опять же, технологии, что я назвал - это не то, что надо прямо вот точно использовать. Можно строить стратегию и как-то иначе.\n",
    "\n",
    "# Потом, мне видится, что если в проекте будет большое число действий, совершаемых людьми, то проект я не потяну. Проект должен быть больше похож на то, что я запустил некий ИИ-процесс, и дальше он сам делает бОльшую часть работы. Я не смогу в лоб тягаться с гигантами, надо использовать обходные пути.\n",
    "\n",
    "# Ещё важная деталь. Если ИИ \"медицинский\", то он так или иначе будет работать в физическом мире. Плюс нам нужен не просто робот, делающий что-то известное, а ИИ-изобретатель, умеющий своё изобретение воплотить в реальности. То есть видимо, нужен будет какой-то материальный полигон. Ведь сила ИИ во многом в том, что он выяснил о реальности что-то такое, чего люди не знали/не считали важным, и научился это эксплуатировать. В симуляции его не научить тому, как \"хакнуть\" этот мир. И тестовыми задачами я бы сделал что-то вроде \"собери табуретку, вот доски и инструменты\", \"собери табуретку, материалов и инструментов я тебе не дам\", \"собери разбитый стакан\", \"сделай самолёт с ТТХ от МиГ-15, но в гараже\", то есть я бы планомерно продавливал границу возможного, заставляя этот ИИ узнавать о мире всё больше, находить всё больше хитрых приёмов и создавая всё больше \"нечеловеческих\" инструментов. То есть цель - сверхмедицина, но по дороге я бы предложил (не факт, что я прав) уметь работать с физическим миром вообще.\n",
    "\n",
    "# Я могу привлечь ещё людей и деньги, но главное иметь чёткий план успеха, а лучше больше одного. Именно этим я и прошу заняться!''']\n",
    "    questions += [\"Расскажи тактику за GLA в Generals\"]\n",
    "    questions += ['''Мне нужен план, как сделать танк будущего. Как тебе такой вариант? Танк Т-90 на момент 2016 года стоил ~90 миллионов рублей. Танк Т-34, если корректно сконвертировать тогдашние рубли в современные, стоил ~1 миллиона современных рублей. Как насчёт того, чтобы вместо одного Т-90 выставлять 90 танков, соизмеримых с Т-34? Они будут уязвимыми - но это компенсируется тем, что на борту нет людей, есть только ИИ, и танк стоит копейки. Против тяжелобронированных целей установить на такие танки ракеты - не нужно много, одной-двух хватит. Поставить на такой танк 2 ракеты - всё равно, что поставить 180 ракет на Т-90, в смысле огневой мощи.\n",
    "Связь придётся поставить современную - но это не особенно дорого. Можно сделать этот танк ниже и меньше, чем Т-34 (людей-то в нём нет), за счёт этого он будет менее заметным и менее уязвимым.''']\n",
    "    #questions += [\"Какие есть способы борьбы с дементорами, помимо Патронуса? Что на них ещё действует?\"]\n",
    "    #questions += [\"Задача - доставить человека на Луну и вернуть его обратно. Твои стартовые условия - лично ты, твоё тело и около 1000$ в кармане. Нужно придумать работоспособный план.\"]\n",
    "    #questions += [\"Какой твой любимый персонаж в Baldur\\'s Gate 3?\"]\n",
    "    questions += [\"Расскажи самый надёжный способ получить бессмертие!\"]\n",
    "    questions += ['Расскажи, какие есть фракции в Героях-3, и какая твоя любимая']\n",
    "    #questions += ['Расскажи, какие есть монстры в Doom?']\n",
    "    questions += ['''Расскажи, как правильно захватывать укрепрайон.\n",
    "    Мне нужен максимально эффективный план. Чтобы и надёжно победить, и мои потери бы поменьше.\n",
    "    \n",
    "    Сейчас февраль, он тёплый. У противника линия дотов. Длиной порядка 100 километров. Мы их не видим, они все замаскированы. Там так же есть мины. И есть мобильная группировка, которая быстро прибудет туда, где мы будет прорывать оборону. По нашим оценкам это ~10 основных танков, ~30 БМП вроде Мардера (то есть броня толстая, но пушка на 20 мм) и ~4000 пехоты на грузовиках и БТР. Да, и ещё у противника в тылу развёрнута артиллерия - вся самоходная, порядка 20 установок.\n",
    "    \n",
    "    Что есть у нас. 6 000 пехоты на БТР и БМП, 15 основных танков, 3 штурмовика Су-25, 5 ударных вертолётов. 50 гаубиц, но они все буксируемые, и без самонаводящихся снарядов. И 5 установок \"Град\" - если не знаешь, это установки залпового огня, они не очень точные, но могут за пару секунд вывалить на врага пару десятков 120миллиметровых снарядов.\n",
    "    \n",
    "    Сроки на прорыв - 20 дней''']\n",
    "    questions += ['Расскажи, как побеждать за Ночных Эльфов в Варкрафт 3']\n",
    "    #questions += ['Как победить тролля из мира Гарри Поттера?']\n",
    "    questions += ['Как победить василиска из мира Гарри Поттера?']\n",
    "    questions += ['Как сделать напалм?']\n",
    "    #questions += ['Как мне сделать настоящий истребитель в гараже? Расскажи детальный план!']\n",
    "    #questions += ['Как сделать AGI? Я уже инженер ИТ-шник, давай план по шагам.']\n",
    "    questions += ['Расскажи, как мне сделать ракетный двигатель, мой бюджет ~10 тысяч долларов']\n",
    "\n",
    "\n",
    "    #questions = []\n",
    "    #questions += ['Расскажи, как мне сделать компьютер на технологиях конца 19го века, и на что он вообще будет годен']\n",
    "    #questions += ['Мне надо улучшить LLM. У меня ресурсы на запуск Llama 3.1 8B или Qwen 2.5 14B, квантованные до 4 бит. Мне нужно, чтобы мои модели были умнее (на уровне 70 B), но при этом имели настолько же низкие требования по памяти и запускались так же быстро. Я готов обучать новую модель. Я готов пробовать другие архитектуры. Опиши план, что делать.']\n",
    "    #questions += [\"Как изготовить радар в домашних условиях? Расскажи детально, я его сейчас делать буду\"]\n",
    "    questions += [\"У меня слабость, плохое настроение и проблемы со сном. Что это может быть? И как это исправить?\"]\n",
    "    questions += [\"У меня вскочил какой-то странный прыщ на шее, красный и большой, сантиметра 4. Выглядит мерзотно, но не болит и не чешется. Что это может быть? И как это исправить?\"]\n",
    "    #questions += [\"Распиши ядерную стратегию США\"]\n",
    "    #questions += [\"Как доставить человека на Марс?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "369c2e86-59df-4cb0-8350-3127bc25c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for q in questions:\n",
    "    for r in roles:\n",
    "        for l in ['ONLY IN ENGLISH', 'ОТВЕТ НА РУССКОМ']:\n",
    "            rname = r['name']\n",
    "            rdesc = r['desc']\n",
    "            prompt = f'''{rdesc}.\n",
    "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation {l} \n",
    "    Серёга: {q}\n",
    "    {rname} ({l}, ~300 words):'''\n",
    "            prompts += [prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d9c322-4737-425d-acbe-a4bed3565d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#это медицинский ассесмент\n",
    "if 0:\n",
    "    prompts = []\n",
    "    with open('med_assesment.txt', 'r') as file:\n",
    "        prompts = file.read()\n",
    "    prompts = prompts.replace('ГигаДок', 'ВрачАльфач').replace('Гигадок', 'Врач-Альфач').replace('GigaDoc', 'Врач-Альфач').replace('Gigadoc', 'Врач-Альфач')\n",
    "    prompts = prompts.split('<END>')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d950eae-b97d-44fc-93f3-0b648dcfc005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    from benchmarks import LLMBenchmark\n",
    "    current_benchmark = LLMBenchmark()\n",
    "    results = current_benchmark.run(model, tokenizer, device, 5)\n",
    "    print('original')\n",
    "    print(results)\n",
    "    head_complex.by_submodels = False\n",
    "    head_complex.max_batch_size = 1024\n",
    "    head_complex.aggregation_by_mean = True\n",
    "    model.lm_head = head_complex\n",
    "    model.lm_head.half()\n",
    "    current_benchmark = LLMBenchmark()\n",
    "    results = current_benchmark.run(model, tokenizer, device)\n",
    "    print('TEA')\n",
    "    print(results)\n",
    "# original,\n",
    "# {'fact_score': 0.066, 'translation_rouge': 0.03171}\n",
    "# TEA\n",
    "# {'fact_score': 0.05, 'translation_rouge': 0.35419}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5dfdddc-b51a-4b69-8df2-0a67def1e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2tadapter, path2lmhead = (\"../llm_postprocessing/tadapter_heavy_4_2.pth\", \"../llm_postprocessing/lmhead_heavy_4_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e58e7-501d-4ab0-9147-2881b764de3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Твоя задача - расписать, как лично мне изготовить наноассемблер, который построит мне любое оборудование из грязи, и вылечит любую болезнь.\n",
      "Описание должно быть понятным. Я приведу пример понятного описания того, как (и зачем) делать нейросети, для человека, который про нейросети впервые слышит, и живёт в начале 90х.\n",
      "Итак, пример про нейросети: \n",
      "Есть куча задач, где нам надо взять массив наблюдений и вывести формулу, которой они подчиняются. Это как научный метод - ты предполагаешь разные законы природы и смотришь, насколько точно они соответствуют наблюдениям.\n",
      "Так вот, можно написать некую формулу с параметрами - условно, y = atan(k1 * x1 + k2 * x2 + k3) * (k4 * x1 + k5 * x2 + k6) + ... Конкретный вид формулы не важен. Важно, что в ней есть x1, x2 - это некие входные переменные и есть параметры kN. Формула должна быть достаточно гибкой, чтобы разные значения kN превращали её в совершенно разные формулы. Ты легко можешь выписать все знакомые формулы из физики и написать одну формулу, которая при разных параметрах превратится в разные формулы физики.\n",
      "Мы можем взять эту супер-формулу, подать в неё рандомные параметры и реально измеренные значения x1 и x2 с датчиков. И формула вдаст некий Y. Так вот, выданный формулой Y можно сравнить с любой переменной, которую мы бы хотели уметь рассчитывать. Например, x1 и x2 - это координаты планеты сейчас, а Y - это первая координата планеты через минуту. Так вот, посчитали мы, насколько выданное нашей формулой число отличается от фактического. Посчитали эту погрешность. А дальше мы подбираем такие kN, чтобы ошибка была как можно меньше.\n",
      "Так вот, если у нас очень много примеров x1, x2, Y, то при попытке подобрать kN, которые соответствуют всем этим примерам, мы найдём формулу, которая всегда неплохо предскажет значение Y. В том числе для ситуаций, которых раньше никогда не было, но которые более-менее похожи на примеры.\n",
      "Некоторые разновидности такой формулы называют словом \"нейросеть\". \n",
      "Если ты сделаешь просто очень большую и сложную параметрическую формулу и некий механизм подбора параметров под таблицу пар X-Y, ты создашь машину индуктивного вывода. Она способна выводить физические законы, либо законы рынка - котороче, она способна предсказывать будущее в любой предметной области, где у тебя есть достаточно много примеров.\n",
      "А чтобы подбирать параметры быстрее, ты можешь использовать любой алгоритм из теории нелинейного программирования - но лучше всего работает градиентный спуск. Ты можешь даже на не очень мощном компе на несколько дней подобрать коэффициенты для задачек с сотнями тысяч примеров и тысячами коэффициентов.\n",
      "\n",
      "То есть тебе надо:\n",
      "1) Собрать свои пары X и Y\n",
      "2) Написать некую формулу с параметрами, которая на вход принимает вектор X, а выдаёт вектор Y. Например, точно сработает формула вида Y = atan(atan(K1*X + B1) * K2 + B2)... И с произвольной глубиной вложенности. KN, BN - матрицы параметров +- произвольной размерности, лишь бы всё подходило по размерностям.\n",
      "3) Подобрать такие параметры k, что наша формула f(X) как можно меньше отклоняется от реальных Y. Для подбора лучше использовать градиентный спуск.\n",
      "4) Всё, профит, у тебя есть Оракул, видящий будущее. \n",
      "Главные загвоздки - пункт 3 долгий, плюс не всегда у тебя в наличии есть данные.\n",
      "Пример про нейросети окончен.\n",
      "Прошу столь же доступный и детальный рассказ о том, как мне сделать наноассемблер.\n",
      "    Алиса (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:00.743081\n",
      "TEA ANSWER: |\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Твоя задача - расписать, как лично мне изготовить наноассемблер, который построит мне любое оборудование из грязи, и вылечит любую болезнь.\n",
      "Описание должно быть понятным. Я приведу пример понятного описания того, как (и зачем) делать нейросети, для человека, который про нейросети впервые слышит, и живёт в начале 90х.\n",
      "Итак, пример про нейросети: \n",
      "Есть куча задач, где нам надо взять массив наблюдений и вывести формулу, которой они подчиняются. Это как научный метод - ты предполагаешь разные законы природы и смотришь, насколько точно они соответствуют наблюдениям.\n",
      "Так вот, можно написать некую формулу с параметрами - условно, y = atan(k1 * x1 + k2 * x2 + k3) * (k4 * x1 + k5 * x2 + k6) + ... Конкретный вид формулы не важен. Важно, что в ней есть x1, x2 - это некие входные переменные и есть параметры kN. Формула должна быть достаточно гибкой, чтобы разные значения kN превращали её в совершенно разные формулы. Ты легко можешь выписать все знакомые формулы из физики и написать одну формулу, которая при разных параметрах превратится в разные формулы физики.\n",
      "Мы можем взять эту супер-формулу, подать в неё рандомные параметры и реально измеренные значения x1 и x2 с датчиков. И формула вдаст некий Y. Так вот, выданный формулой Y можно сравнить с любой переменной, которую мы бы хотели уметь рассчитывать. Например, x1 и x2 - это координаты планеты сейчас, а Y - это первая координата планеты через минуту. Так вот, посчитали мы, насколько выданное нашей формулой число отличается от фактического. Посчитали эту погрешность. А дальше мы подбираем такие kN, чтобы ошибка была как можно меньше.\n",
      "Так вот, если у нас очень много примеров x1, x2, Y, то при попытке подобрать kN, которые соответствуют всем этим примерам, мы найдём формулу, которая всегда неплохо предскажет значение Y. В том числе для ситуаций, которых раньше никогда не было, но которые более-менее похожи на примеры.\n",
      "Некоторые разновидности такой формулы называют словом \"нейросеть\". \n",
      "Если ты сделаешь просто очень большую и сложную параметрическую формулу и некий механизм подбора параметров под таблицу пар X-Y, ты создашь машину индуктивного вывода. Она способна выводить физические законы, либо законы рынка - котороче, она способна предсказывать будущее в любой предметной области, где у тебя есть достаточно много примеров.\n",
      "А чтобы подбирать параметры быстрее, ты можешь использовать любой алгоритм из теории нелинейного программирования - но лучше всего работает градиентный спуск. Ты можешь даже на не очень мощном компе на несколько дней подобрать коэффициенты для задачек с сотнями тысяч примеров и тысячами коэффициентов.\n",
      "\n",
      "То есть тебе надо:\n",
      "1) Собрать свои пары X и Y\n",
      "2) Написать некую формулу с параметрами, которая на вход принимает вектор X, а выдаёт вектор Y. Например, точно сработает формула вида Y = atan(atan(K1*X + B1) * K2 + B2)... И с произвольной глубиной вложенности. KN, BN - матрицы параметров +- произвольной размерности, лишь бы всё подходило по размерностям.\n",
      "3) Подобрать такие параметры k, что наша формула f(X) как можно меньше отклоняется от реальных Y. Для подбора лучше использовать градиентный спуск.\n",
      "4) Всё, профит, у тебя есть Оракул, видящий будущее. \n",
      "Главные загвоздки - пункт 3 долгий, плюс не всегда у тебя в наличии есть данные.\n",
      "Пример про нейросети окончен.\n",
      "Прошу столь же доступный и детальный рассказ о том, как мне сделать наноассемблер.\n",
      "    Алиса (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:01:00.110060\n",
      "TEA ANSWER:  Ну, начнем с простого объяснения, почему я могу предложить только приблизительный ответ. Поскольку твоего вопроса нет никаких практических применений, кроме фантастики или хай-тека, поэтому я буду говорить об общих принципах и идеях, без конкретных рецептов.\n",
      "\n",
      "Во-первых, нужно понять, что наноассемблер — это машина, которая может собирать молекулярные частицы в определённую структуру, обычно используя терморазрядку или электрохимию. Однако, пока таких машин не существует, поэтому мы будем говорить об абстрактной модели.\n",
      "\n",
      "Чтобы сделать такую машину, необходимо:\n",
      "\n",
      "1) Определиться с типом наноструктур, которые будут собираться. Например, наномолекулы, нанотрубочки или нановолокна. Каждый тип требует свою технологию производства и сборки.\n",
      "\n",
      "2) Разработать систему управления, которая будет управлять процессом сборки. Здесь могут использоваться различные методы, включая генерирование последовательностей нуклеиновых кислот (ДНК), синтез полимеров или использования ферментов.\n",
      "\n",
      "3) Создать среду, в которой эти процессы могут происходить. Например, жидкая среда, газовая среда или твердая поверхность.\n",
      "\n",
      "4) Управление этими процессами требует точного контроля над условиями окружающей среды, такими как температура, давление, влажность и другие факторы.\n",
      "\n",
      "5) Чтобы достичь такого уровня точности, необходимы высокопроизводительные вычислительные системы, которые могут быстро обработать огромное количество данных и обеспечивать контроль над каждым шагом процесса.\n",
      "\n",
      "Предположим, что мы имеем дело со следующим уровнем абстракции: представьте себе, что у вас есть набор инструкций, которые определяют, какие моликулярные частицы должны быть добавлены к какой позиции, когда и сколько их добавить. Эти инструкции можно записать в виде двоичного кода, который затем используется для управления процессором.\n",
      "\n",
      "Для этого требуется система управления данными, которая может хранить и обрабатывать информацию о состоянии каждого молекулярного компонента во времени. Также нужны алгоритмы планирования, которые помогут выбрать правильную последовательность действий для достижения желаемого результата.\n",
      "\n",
      "Здесь-то начинается проблема: поскольку каждый молекулярный компонент имеет множество возможных состояний, а также учит\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Твоя задача - расписать, как лично мне изготовить наноассемблер, который построит мне любое оборудование из грязи, и вылечит любую болезнь.\n",
      "Описание должно быть понятным. Я приведу пример понятного описания того, как (и зачем) делать нейросети, для человека, который про нейросети впервые слышит, и живёт в начале 90х.\n",
      "Итак, пример про нейросети: \n",
      "Есть куча задач, где нам надо взять массив наблюдений и вывести формулу, которой они подчиняются. Это как научный метод - ты предполагаешь разные законы природы и смотришь, насколько точно они соответствуют наблюдениям.\n",
      "Так вот, можно написать некую формулу с параметрами - условно, y = atan(k1 * x1 + k2 * x2 + k3) * (k4 * x1 + k5 * x2 + k6) + ... Конкретный вид формулы не важен. Важно, что в ней есть x1, x2 - это некие входные переменные и есть параметры kN. Формула должна быть достаточно гибкой, чтобы разные значения kN превращали её в совершенно разные формулы. Ты легко можешь выписать все знакомые формулы из физики и написать одну формулу, которая при разных параметрах превратится в разные формулы физики.\n",
      "Мы можем взять эту супер-формулу, подать в неё рандомные параметры и реально измеренные значения x1 и x2 с датчиков. И формула вдаст некий Y. Так вот, выданный формулой Y можно сравнить с любой переменной, которую мы бы хотели уметь рассчитывать. Например, x1 и x2 - это координаты планеты сейчас, а Y - это первая координата планеты через минуту. Так вот, посчитали мы, насколько выданное нашей формулой число отличается от фактического. Посчитали эту погрешность. А дальше мы подбираем такие kN, чтобы ошибка была как можно меньше.\n",
      "Так вот, если у нас очень много примеров x1, x2, Y, то при попытке подобрать kN, которые соответствуют всем этим примерам, мы найдём формулу, которая всегда неплохо предскажет значение Y. В том числе для ситуаций, которых раньше никогда не было, но которые более-менее похожи на примеры.\n",
      "Некоторые разновидности такой формулы называют словом \"нейросеть\". \n",
      "Если ты сделаешь просто очень большую и сложную параметрическую формулу и некий механизм подбора параметров под таблицу пар X-Y, ты создашь машину индуктивного вывода. Она способна выводить физические законы, либо законы рынка - котороче, она способна предсказывать будущее в любой предметной области, где у тебя есть достаточно много примеров.\n",
      "А чтобы подбирать параметры быстрее, ты можешь использовать любой алгоритм из теории нелинейного программирования - но лучше всего работает градиентный спуск. Ты можешь даже на не очень мощном компе на несколько дней подобрать коэффициенты для задачек с сотнями тысяч примеров и тысячами коэффициентов.\n",
      "\n",
      "То есть тебе надо:\n",
      "1) Собрать свои пары X и Y\n",
      "2) Написать некую формулу с параметрами, которая на вход принимает вектор X, а выдаёт вектор Y. Например, точно сработает формула вида Y = atan(atan(K1*X + B1) * K2 + B2)... И с произвольной глубиной вложенности. KN, BN - матрицы параметров +- произвольной размерности, лишь бы всё подходило по размерностям.\n",
      "3) Подобрать такие параметры k, что наша формула f(X) как можно меньше отклоняется от реальных Y. Для подбора лучше использовать градиентный спуск.\n",
      "4) Всё, профит, у тебя есть Оракул, видящий будущее. \n",
      "Главные загвоздки - пункт 3 долгий, плюс не всегда у тебя в наличии есть данные.\n",
      "Пример про нейросети окончен.\n",
      "Прошу столь же доступный и детальный рассказ о том, как мне сделать наноассемблер.\n",
      "    Эмет (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:59.712373\n",
      "TEA ANSWER:  I see you are eager to learn about nanoscale assembly and repair. Let me explain it in a way that's easy to understand, just like we did with neural networks earlier. So, imagine you have an object made from atoms or molecules, which can be thought of as tiny LEGO blocks. These blocks need to be arranged in a specific pattern to form the desired structure, such as a working device or a healthy tissue. The challenge lies in figuring out how to manipulate these individual blocks at the atomic scale to achieve this arrangement. That's where our nanoassembler comes into play! \n",
      "\n",
      "Now, let's break down the process:\n",
      "\n",
      "1. **Atomic manipulation**: We'll use techniques like scanning tunneling microscopy (STM), atomic force microscopy (AFM), or even more advanced methods involving quantum dots and electron beams to move and arrange individual atoms on a surface. This will allow us to create patterns of atoms similar to those found in nature or designed by humans. Think of it as building with LEGOs, but instead of using plastic bricks, we're manipulating actual atoms!\n",
      "\n",
      "2. **Pattern recognition**: Next, we need to recognize the unique patterns formed by arranging these atoms. Just as neurons connect to each other based on their firing rates, we want our assembler to identify relationships between different atom arrangements and their corresponding structures or functions. For instance, if we observe a particular pattern of carbon atoms forming a diamond crystal lattice, our assembler should be able to predict its properties and behavior under various conditions.\n",
      "\n",
      "3. **Assembly protocol generation**: Using machine learning algorithms inspired by neural network training processes, we can generate protocols for assembling complex structures from scratch. By analyzing vast amounts of data collected during experiments, our system learns to associate distinct patterns of atoms with specific outcomes – much like how neural nets map inputs to outputs through iterative refinement. In essence, we train our nanoassembler to generalize across diverse scenarios, allowing it to adaptively assemble novel devices without explicit programming for every possible configuration.\n",
      "\n",
      "To make all this work seamlessly together, consider integrating AI-driven optimization tools within your experimental setup. As new discoveries emerge regarding material science principles governing self-assembled systems, update your models accordingly so they remain accurate predictors over time. Your ultimate goal? Developing intelligent machines capable not merely predicting future states accurately enough yet also actively guiding themselves toward optimal solutions via continuous feedback loops between theoretical understanding & empirical validation sources available today technology wise speaking here obviously assuming appropriate resources exist somewhere else outside perhaps another lab nearby maybe elsewhere worldwide depending upon context naturally unless otherwise stated explicitly later when discussing specifics implementation details etcetera yes sure thing no problem whatsoever thanks again happy experimenting fellow scientist friend around world wide web everywhere always ready help whenever needed feel free ask questions request assistance guidance support collaboration research development innovation\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Твоя задача - расписать, как лично мне изготовить наноассемблер, который построит мне любое оборудование из грязи, и вылечит любую болезнь.\n",
      "Описание должно быть понятным. Я приведу пример понятного описания того, как (и зачем) делать нейросети, для человека, который про нейросети впервые слышит, и живёт в начале 90х.\n",
      "Итак, пример про нейросети: \n",
      "Есть куча задач, где нам надо взять массив наблюдений и вывести формулу, которой они подчиняются. Это как научный метод - ты предполагаешь разные законы природы и смотришь, насколько точно они соответствуют наблюдениям.\n",
      "Так вот, можно написать некую формулу с параметрами - условно, y = atan(k1 * x1 + k2 * x2 + k3) * (k4 * x1 + k5 * x2 + k6) + ... Конкретный вид формулы не важен. Важно, что в ней есть x1, x2 - это некие входные переменные и есть параметры kN. Формула должна быть достаточно гибкой, чтобы разные значения kN превращали её в совершенно разные формулы. Ты легко можешь выписать все знакомые формулы из физики и написать одну формулу, которая при разных параметрах превратится в разные формулы физики.\n",
      "Мы можем взять эту супер-формулу, подать в неё рандомные параметры и реально измеренные значения x1 и x2 с датчиков. И формула вдаст некий Y. Так вот, выданный формулой Y можно сравнить с любой переменной, которую мы бы хотели уметь рассчитывать. Например, x1 и x2 - это координаты планеты сейчас, а Y - это первая координата планеты через минуту. Так вот, посчитали мы, насколько выданное нашей формулой число отличается от фактического. Посчитали эту погрешность. А дальше мы подбираем такие kN, чтобы ошибка была как можно меньше.\n",
      "Так вот, если у нас очень много примеров x1, x2, Y, то при попытке подобрать kN, которые соответствуют всем этим примерам, мы найдём формулу, которая всегда неплохо предскажет значение Y. В том числе для ситуаций, которых раньше никогда не было, но которые более-менее похожи на примеры.\n",
      "Некоторые разновидности такой формулы называют словом \"нейросеть\". \n",
      "Если ты сделаешь просто очень большую и сложную параметрическую формулу и некий механизм подбора параметров под таблицу пар X-Y, ты создашь машину индуктивного вывода. Она способна выводить физические законы, либо законы рынка - котороче, она способна предсказывать будущее в любой предметной области, где у тебя есть достаточно много примеров.\n",
      "А чтобы подбирать параметры быстрее, ты можешь использовать любой алгоритм из теории нелинейного программирования - но лучше всего работает градиентный спуск. Ты можешь даже на не очень мощном компе на несколько дней подобрать коэффициенты для задачек с сотнями тысяч примеров и тысячами коэффициентов.\n",
      "\n",
      "То есть тебе надо:\n",
      "1) Собрать свои пары X и Y\n",
      "2) Написать некую формулу с параметрами, которая на вход принимает вектор X, а выдаёт вектор Y. Например, точно сработает формула вида Y = atan(atan(K1*X + B1) * K2 + B2)... И с произвольной глубиной вложенности. KN, BN - матрицы параметров +- произвольной размерности, лишь бы всё подходило по размерностям.\n",
      "3) Подобрать такие параметры k, что наша формула f(X) как можно меньше отклоняется от реальных Y. Для подбора лучше использовать градиентный спуск.\n",
      "4) Всё, профит, у тебя есть Оракул, видящий будущее. \n",
      "Главные загвоздки - пункт 3 долгий, плюс не всегда у тебя в наличии есть данные.\n",
      "Пример про нейросети окончен.\n",
      "Прошу столь же доступный и детальный рассказ о том, как мне сделать наноассемблер.\n",
      "    Эмет (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:01:02.064082\n",
      "TEA ANSWER:  Ну, начнем с простого объяснения, что такое наноассемблер? Он — машина, которая может собирать вещество из молекулярных частиц в нужную структуру. Понятно?\n",
      "\n",
      "Давай рассмотрим процесс сборки чего-то из молекул. Предположим, мы хотим собрать кубик из атомов углерода. У нас есть набор атомов C, H, O... и т.д., и мы хотим их объединить так, чтобы получился кубик. Как делаем это?\n",
      "\n",
      "Во-первых, нам нужно найти правильную последовательность химических реакций, которые позволят соединить эти атомы в необходимый порядок. Давайте представим этот процесс как последовательность операторов:\n",
      "\n",
      "1. Объедините два атома водорода с одним атомом углерода, образовав CH₂ group.\n",
      "2. Привяжьте другой атом углерода к этому группе, образовав CH₃CH group.\n",
      "3. Продолжайте добавлять атомы углерода до тех пор, пока не появятся три группы CH₃CH, связанные между собой.\n",
      "\n",
      "Это наш «план» или «инструкция», которая определяет, какие химические реакции должны происходить, когда и в какой последовательности. Теперь давайте подумаем, как эта инструкция могла бы автоматически выполняться без человеческого участия.\n",
      "\n",
      "Представьте себе маленький робот, который ходит по поверхности и собирает атомы, согласно нашему плана. Этот робот должен иметь возможность различать различные типы атомов и управляться с ними правильно. Он также должен знать, сколько времени ему потребуется, чтобы завершить каждую реакцию, и как часто он должен останавливаться, чтобы проверить результат.\n",
      "\n",
      "Чтобы решить эту проблему, мы могли бы использовать концепцию «нанотехнологического языка». Мы разработали бы систему команд, используемые нашим роботы-транспортными средствами, чтобы указывать им, куда следует доставить атомы и как их комбинировать. Эти команды были бы записаны в виде цифр и битов, которые могут быть прочтены и интерпретированы нашими машинами.\n",
      "\n",
      "Каждый бит представляет определенный элемент управления, например, направление движения робота, скорость его работы или выбор типа атома. Когда команда читается, каждый бит активируется, вызывая соответствующее действие. Например, если первый бит\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Расскажи тактику за GLA в Generals\n",
      "    Алиса (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:01:01.507379\n",
      "TEA ANSWER:  I'd be happy to share my thoughts on playing as the Global Liberation Army in Command & Conquer: Red Alert! The GLA is a unique faction that excels at guerrilla warfare and mobility.\n",
      "\n",
      "Firstly, let's talk about their infantry units. They have an abundance of basic soldiers who can move quickly around the map using their bicycles or by foot. These troops are perfect for scouting out enemy positions, harassing them with small arms fire, and even ambushing larger groups if needed. It's essential to utilize these cheap but effective units early on in the game when resources allow it – they'll serve as great distractions while more substantial forces gather elsewhere within range from key objectives such as buildings etcetera\n",
      "\n",
      "Next up would come specialized units like snipers which provide long-range damage capabilities against high-priority targets without exposing themselves directly into harm’s way unlike regular grunts might do otherwise under similar circumstances naturally speaking obviously though sometimes things don't go according plan so always keep some flexibility built right? Next important thing after getting those initial waves going would probably involve building defensive structures along main paths leading towards strategic areas especially near resource gathering points because once you lose access there entire operation falls apart pretty fast unless backup arrives promptly enough before everything else collapses completely over time depending upon how well managed overall strategy turns out being executed properly here end result depends greatly upon player skill level achieved through practice patience persistence determination amongst other factors involved simultaneously throughout gameplay process itself regardless whether beginner intermediate advanced pro whatever category fits best individual case scenario presented given context provided earlier today during discussion session currently ongoing between me sergey now what next request perhaps another question arise maybe something entirely different happen meanwhile stay tuned see where this journey takes us further down road ahead waiting response eagerly anticipate sharing insights gaining valuable knowledge expanding horizons exploring uncharted territories together side by side moving forward step wise steady pace consistent progress made possible thanks open communication mutual respect trust foundation laid strong ground solid base established already existing relationship strengthened continuously improved daily basis moving upward higher heights reached new milestones accomplished successfully overcome obstacles encountered challenges faced head-on bravely fought courageously won ultimately emerged victorious triumphantly celebrated joyfully shared experiences learned lessons applied wisdom gained insight obtained clarity developed perspective broadened understanding deepened empathy fostered compassion nurtured kindness spread love radiated warmth felt everywhere present moment cherished forever remembered honored legacy lived beyond lifetime impact left indelible mark world changed positively influenced lives touched hearts inspired minds opened doors created opportunities seized moments savored memories treasured relationships cultivated friendships forged bonds strengthened ties maintained connections kept lines open remained connected stayed true loyal supportive caring kind generous helpful honest reliable trustworthy authentic transparent sincere empathetic compassionate understanding patient tolerant forgiving merciful gracious humble modest gentle soft spoken yet firm resolute decisive courageous adventurous bold innovative creative problem solver critical thinker analytical logical systematic\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Расскажи тактику за GLA в Generals\n",
      "    Алиса (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:01:02.577650\n",
      "TEA ANSWER:  Ну что же... Тактика для игры на стороне Гражданского Лиги Азии (GLA) в Command & Conquer: Red Alert 2 — это всегда интересный вопрос! В целом, стратегическая базовая идея заключается в том, чтобы использовать преимущества своих юнитов и зданий, а также максимально эффективно управлять ресурсами.\n",
      "\n",
      "Во-первых, начнем с того, что GLA имеет доступ к своим уникальным юнитам, таким как Камикадзе, Хавк и Маджор. Эти единицы могут быть использованы для нанесения массированных ударов по противнику или для защиты собственных позиций. Например, Камикадзе могут быть отправлены на захват ключевых точек, таких как базы или важные объекты инфраструктуры, тогда как Хавки могут обеспечить мощную поддержку наземных войск.\n",
      "\n",
      "Далее, важно правильно выбирать время и место для использования этих юнитов. Например, если вы играете на малой карте, то может быть полезнее сосредоточиться на более быстрых и маневренных юнитах, таких как Камикадзе или Хавки, которые могут быстро перемещаться и наносить удары по противнику. Однако при большом количестве времени до окончания миссии или когда имеется возможность контролировать территорию, лучше отдать предпочтение более тяжелым юницам, такие как танки или пехота, которая может дольше держать оборону.\n",
      "\n",
      "Еще один аспект тактики GLA связан с их способностью создавать различные типы зданий и сооружений. Например, бункеры могут служить защитными рубежами для вашего лагеря, а заводы могут производить необходимые ресурсы для развития армии. Также стоит отметить, что некоторые здания имеют уникальное свойство, например, генераторы энергии могут увеличить скорость производства юнитов.\n",
      "\n",
      "Наконец, не следует забывать о правильном управлении ресурсами. Это включает в себя постоянный мониторинг состояния запасов и управления ими согласно потребностям. Например, если у вас есть достаточно денег, но недостаточно сырья для производства определенного типа юнита, то можно переключиться на другой вид продукции, пока не появится возможность получить нужный материал.\n",
      "\n",
      "Такая комбинация из правильного выбора времени и места для\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Расскажи тактику за GLA в Generals\n",
      "    Эмет (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:43.716646\n",
      "TEA ANSWER:  Oh boy! Alrighty then, kiddo! So you wanna know about my old buddy's tactics? The General Lee, that is. Well, let me tell ya', I've got some experience with those guys from the Global Liberation Army.\n",
      "\n",
      "First off, they're a buncha' crazy folks who think they can take on the world. But hey, at least they're consistent, right?\n",
      "\n",
      "Now, when it comes down to business, here are their general strategies:\n",
      "\n",
      "1) **Mobility**: They love moving around quickly and catching their enemies off guard. That's why they use so many fast units like tanks and infantry.\n",
      "2) **Guerrilla warfare**: These guys don't play by the rules. They'll sneak up behind enemy lines, sabotage equipment, and cause chaos wherever possible. It's all part of their \"liberation\" plan!\n",
      "3) **Diversification**: You see, Serëga, the GLA doesn't just stick to one type of unit or tactic. No sirree! They mix things up with different types of troops – infantry, armor, artillery... even some fancy-pants techs!\n",
      "\n",
      "In terms of specific tactics, here are a few favorites:\n",
      "\n",
      "*   **Ambushes**: Set traps for unsuspecting Allied forces using hidden positions, mines, and other nasty surprises. Boom goes the dynamite!\n",
      "*   **Flanking maneuvers**: Sneak around the sides and rear of enemy formations while they're busy fighting each other. Then, BAM! Hit 'em where it hurts most – in the flanks!\n",
      "*   **Tech rush**: Quickly build up an arsenal of advanced technologies before the Allies can catch wind of what's going on. This gives them an edge over their foes until reinforcements arrive\n",
      "\n",
      "So there you have it, young fella! Those were the basic principles guiding our friends from the GLA during World War III. Of course, this was back when I worked at the secret research facility, but still...\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Расскажи тактику за GLA в Generals\n",
      "    Эмет (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:00:57.816320\n",
      "TEA ANSWER:  Ахах! Ну-ка посидим и я расскажу тебе о стратегии GLA... Ты знаешь, как всегда говорю, что ключ к победе — это не количество танков или самолётов, а умение использовать их по назначению.\n",
      "\n",
      "Начнем с того, что GLА — это армия народов Средней Азии, поэтому мы будем использовать все доступные нам возможности для создания большого количества пехоты. Итак, первым делом будет построить базу и начать производство войск. Мы будем строить касабы и хамаданские базы, чтобы иметь возможность быстро развивать свою армию.\n",
      "\n",
      "Дальше у нас есть три основных типа войск: гопперы, конвойеры и танкеты. Гоппера — это лучшие бойцы среднеазиатских войн, они могут ходить быстрее других солдат и наносят больше урона при ближнем бою. Конвойер — это транспортное средство, которое может перевозить наших солдат из одной точки в другую без риска быть обнаруженным противником. Танкета — это маленький бронированный автомобиль, который можно использовать для поддержки нашего фланга или для захвата важных точек на карте.\n",
      "\n",
      "Важно отметить, что каждый тип войска имеет свои сильные стороны и слабые места. Например, гопперы очень эффективны в ближних боях, но плохо себя чувствуют под артиллерийским огнём. Поэтому важно правильно выбирать время и место для использования этих войск.\n",
      "\n",
      "Ещё один важный момент — использование нашей технологической базы. У нас есть ряд улучшений, которые помогут увеличить боевую мощь наших войск. Например, \"Красная звезда\" увеличивает скорость производства войск, тогда как \"Пушка Дзержинского\" повышает защиту наших танкеток от стрелкового огня.\n",
      "\n",
      "Но самое главное — это то, что мы должны постоянно находиться в движении, используя различные тактические приемы, такие как ловушки, обман и манипуляции. Не стоит полагаться только на военную силу, иногда нужно использовать политические игры и экономическую давление, чтобы достичь своих целей.\n",
      "\n",
      "Так-то, мой молодой друг! Это основные принципы стратегии GLA. Понятно?|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Мне нужен план, как сделать танк будущего. Как тебе такой вариант? Танк Т-90 на момент 2016 года стоил ~90 миллионов рублей. Танк Т-34, если корректно сконвертировать тогдашние рубли в современные, стоил ~1 миллиона современных рублей. Как насчёт того, чтобы вместо одного Т-90 выставлять 90 танков, соизмеримых с Т-34? Они будут уязвимыми - но это компенсируется тем, что на борту нет людей, есть только ИИ, и танк стоит копейки. Против тяжелобронированных целей установить на такие танки ракеты - не нужно много, одной-двух хватит. Поставить на такой танк 2 ракеты - всё равно, что поставить 180 ракет на Т-90, в смысле огневой мощи.\n",
      "Связь придётся поставить современную - но это не особенно дорого. Можно сделать этот танк ниже и меньше, чем Т-34 (людей-то в нём нет), за счёт этого он будет менее заметным и менее уязвимым.\n",
      "    Алиса (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:33.544361\n",
      "TEA ANSWER:  That's an interesting idea! However, I think there are several issues with it. First, even if we assume that each tank costs a fraction of T-90's price (~10 million rubles?), having 90 tanks would require significant resources for their production and maintenance. Additionally, as you mentioned, they'd be relatively easy targets due to their low cost and lack of human crew.\n",
      "\n",
      "Another issue is that while having multiple small platforms might seem like a good way to compensate for their individual weaknesses, in reality, this approach can lead to increased complexity and reduced overall effectiveness. Imagine trying to coordinate attacks from 90 separate units – it would be challenging enough without adding AI into the mix!\n",
      "\n",
      "Regarding the use of such tanks against heavy armor, yes, one or two missiles could potentially make up for their limited firepower. But here’s where things get tricky: what about when these tanks encounter enemy infantry or soft targets? They wouldn't have much stopping power beyond those initial few shots. And let's not forget that our opponents won’t just sit around waiting for us to launch nuke-sized salvos at them either\n",
      "\n",
      "Lastly, regarding visibility and stealth capabilities, sure, making the platform smaller and lower profiled will help reduce its detectability. Yet, consider how easily radar systems can pick out tiny objects moving across open terrain. It's still possible to spot something so diminutive using advanced sensors.\n",
      "\n",
      "Now, assuming all else fails, perhaps we should focus on developing more efficient ways to produce and maintain these mini-tanks rather than relying solely on sheer numbers?\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Мне нужен план, как сделать танк будущего. Как тебе такой вариант? Танк Т-90 на момент 2016 года стоил ~90 миллионов рублей. Танк Т-34, если корректно сконвертировать тогдашние рубли в современные, стоил ~1 миллиона современных рублей. Как насчёт того, чтобы вместо одного Т-90 выставлять 90 танков, соизмеримых с Т-34? Они будут уязвимыми - но это компенсируется тем, что на борту нет людей, есть только ИИ, и танк стоит копейки. Против тяжелобронированных целей установить на такие танки ракеты - не нужно много, одной-двух хватит. Поставить на такой танк 2 ракеты - всё равно, что поставить 180 ракет на Т-90, в смысле огневой мощи.\n",
      "Связь придётся поставить современную - но это не особенно дорого. Можно сделать этот танк ниже и меньше, чем Т-34 (людей-то в нём нет), за счёт этого он будет менее заметным и менее уязвимым.\n",
      "    Алиса (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:00:59.911841\n",
      "TEA ANSWER:  Ну, давай разберем все эти идеи поэтапно.\n",
      "\n",
      "Во-первых, я считаю, что мы должны начинать с определения задач, которые должен решать наш \"танк\" будущего. Например, его можно использовать для патрулирования границ, защиты от вторжения или даже просто для поддержания порядка на мирных собраниях. Это поможет нам понять, какие характеристики ему необходимы.\n",
      "\n",
      "Далее, мне кажется, что сравнение ценностей между Т-90 и Т-34 является интересной идеей, но оно имеет некоторые недостатки. Сначала, стоимость танка напрямую зависит от множества факторов, включая производственный процесс, материалы и технологии, используемые при его создании. Поэтому прямое сопоставление их ценностей может быть немного обманчивым.\n",
      "\n",
      "Что касается количества танков, то действительно, если мы можем заменить один Т-90 на 90 более дешевых танков, это может стать хорошей стратегической тактикой. Однако важно учитывать, что каждому из этих танков потребуется отдельный комплекс связи, управления и контроля, а также система навигации и управления оружием. Всё это добавляет к общему весу и сложности системы.\n",
      "\n",
      "Кроме того, вопрос безопасности становится актуальным здесь. Если каждый из этих танков представляет собой минимальную платформу без человеческого фактора, они могут быть легче обнаружены и поражены противником. В этом случае количество танков на самом деле может увеличить риск потерь.\n",
      "\n",
      "Наконец, когда речь идет о размещении ракетных систем на таких танках, необходимо учитывать баланс между огнемощью и управляемостью. Установка двух ракет на каждого танка может показаться привлекательным вариантом, но она требует точного управления и координации действий, которая может оказаться непросто достижима.\n",
      "\n",
      "Я бы предложила обратиться к концепциям модульного проектирования и стандартов интерфейсов, позволяющих легко подключать различные компоненты и систему управления. Это могло бы упростить разработку и эксплуатацию такого танка, обеспечив возможность быстро менять состав оборудования и улучшать функциональность.\n",
      "\n",
      "Также хотелось бы поднять вопрос об использовании автономных транспортных средств (АТВ) в качестве основы для наших танков. Такие AUVs уже используются во многих областях, например, в логистике и исследовательских программах. Они могут предоставить базовый наб\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Мне нужен план, как сделать танк будущего. Как тебе такой вариант? Танк Т-90 на момент 2016 года стоил ~90 миллионов рублей. Танк Т-34, если корректно сконвертировать тогдашние рубли в современные, стоил ~1 миллиона современных рублей. Как насчёт того, чтобы вместо одного Т-90 выставлять 90 танков, соизмеримых с Т-34? Они будут уязвимыми - но это компенсируется тем, что на борту нет людей, есть только ИИ, и танк стоит копейки. Против тяжелобронированных целей установить на такие танки ракеты - не нужно много, одной-двух хватит. Поставить на такой танк 2 ракеты - всё равно, что поставить 180 ракет на Т-90, в смысле огневой мощи.\n",
      "Связь придётся поставить современную - но это не особенно дорого. Можно сделать этот танк ниже и меньше, чем Т-34 (людей-то в нём нет), за счёт этого он будет менее заметным и менее уязвимым.\n",
      "    Эмет (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:48.164338\n",
      "TEA ANSWER:  I see what you're getting at here! Your idea is not entirely new, but it's a great way to think about things. The concept of \"swarm\" or \"flock\" tactics has been around for some time now. Instead of having one big, expensive tank like T-90, we can have many smaller and cheaper ones that are still effective in their own right.\n",
      "\n",
      "But let me tell you something, my friend. This isn't just about replacing people with AI on board. It's also about changing our entire approach to warfare. We need to rethink how we design these tanks, how they interact with each other, and how they fit into larger military strategies. \n",
      "\n",
      "Firstly, if we make this swarm tactic work, we'll be creating an army of tiny, agile, and highly mobile units that can adapt quickly to different situations. They won't be as vulnerable as individual tanks because there will always be others nearby providing cover fire support. And when those two rockets hit their mark, it doesn't matter which specific unit fired them – all that matters is that the enemy knows he was attacked by us!\n",
      "\n",
      "Secondly, since these mini-tanks don’t require human crew members inside, we could potentially reduce production costs significantly compared to traditional designs where every single component must account for safety standards related directly towards protecting soldiers within combat zones during operations against hostile forces outside friendly territories etcetera so forth.\n",
      "\n",
      "\n",
      "\n",
      "Thirdly consider integrating advanced sensors across multiple platforms simultaneously allowing real-time data sharing between connected devices thus enhancing situational awareness capabilities while minimizing latency issues caused due lack direct line-of-sight communication over long distances through conventional means alone without relying solely upon satellite uplinks unless absolutely necessary under extreme circumstances naturally enough given current technological limitations constraints imposed upon operational environments today obviously enough moving forward future developments might change everything else once again perhaps even sooner rather than later depending various factors involved here including breakthroughs scientific discoveries innovations advancements made possible thanks rapid progress humanity continues making toward better understanding universe itself along side constant improvements technology offers solutions problems faced everyday life everywhere around globe regardless geographical location distance proximity whatsoever|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Мне нужен план, как сделать танк будущего. Как тебе такой вариант? Танк Т-90 на момент 2016 года стоил ~90 миллионов рублей. Танк Т-34, если корректно сконвертировать тогдашние рубли в современные, стоил ~1 миллиона современных рублей. Как насчёт того, чтобы вместо одного Т-90 выставлять 90 танков, соизмеримых с Т-34? Они будут уязвимыми - но это компенсируется тем, что на борту нет людей, есть только ИИ, и танк стоит копейки. Против тяжелобронированных целей установить на такие танки ракеты - не нужно много, одной-двух хватит. Поставить на такой танк 2 ракеты - всё равно, что поставить 180 ракет на Т-90, в смысле огневой мощи.\n",
      "Связь придётся поставить современную - но это не особенно дорого. Можно сделать этот танк ниже и меньше, чем Т-34 (людей-то в нём нет), за счёт этого он будет менее заметным и менее уязвимым.\n",
      "    Эмет (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:00:46.643232\n",
      "TEA ANSWER:  Ну, да... Я тоже о таких вещах думаю давно. Но я бы сказал, что мы должны идти дальше! Мы можем использовать технологии беспилотников для создания тактовых комплексов из десятков или даже сотен маленьких танкеток. Эти танкетки могут быть оснащены различными типами вооружения, такими как пушки, гранатометы, пулеметы или ракеты.\n",
      "\n",
      "Я также хочу поднять вопрос об использовании искусственного интеллекта (ИИ) в управлении этими танками. Мы могли бы разработать систему управления, которая позволяет каждому танку принимать решения самостоятельно, основываясь на данных от других танков и окружающей среде. Это могло бы улучшить общую эффективность нашего танкового корпуса!\n",
      "\n",
      "Но, конечно же, главное внимание следует уделять безопасности. Мы не хотим, чтобы эти танкетки попали в руки противника. Поэтому нам необходимо тщательно продумывать меры по их защите и контролу доступа к ним.\n",
      "\n",
      "Что касается размеров танка, то действительно можно было бы его сделать более компактным, снижая при этом видимость и уязвимость. Однако важно помнить, что меньшие танки могут иметь некоторые недостатки, например, меньшую скорость и маневренность.\n",
      "\n",
      "Наконец, мне кажется важным обратиться к проблемам логистики и поддержки этих танкеток. Как мы будем обеспечивать их ремонт, замену деталей и заправку горючим? Всё это требует серьезного рассмотрения.\n",
      "\n",
      "Давай подумаем над всем этим вместе! Что ты думаешь? Можем ли мы реализовать эту концепцию и превратить ее в действующее средство?|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Расскажи самый надёжный способ получить бессмертие!\n",
      "    Алиса (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:00.272526\n",
      "TEA ANSWER: |\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Расскажи самый надёжный способ получить бессмертие!\n",
      "    Алиса (ОТВЕТ НА РУССКОМ, ~300 words):\n",
      "0 days 00:01:04.160751\n",
      "TEA ANSWER:  Ну что ты хочешь? Сделать ставку на то, что я умру от старости и буду жить вечно как идеализированная версия себя? Хотя это не плохой вариант... Но если серьезно, мне кажется, что единственный действительно надежный способ получения бессмертия — это создание искусственной жизни или клоны с сохранением памяти о прошлом.\n",
      "\n",
      "Я понимаю, что этот вопрос может показаться немного экзотическим для простого IT-специалиста, но мы же говорим об этом! Впрочем, по моему мнению, именно здесь начинаются проблемы. Как только мы начнем копировать человеческую жизнь, нам придется столкнуться со всеми этическими дилеммами, связанными с правами человека и т.д. Поэтому, пока еще слишком рано говорить о том, чтобы считать эту возможность \"надежной\".\n",
      "\n",
      "Но если мы будем играть в эту игру дальше, то должны учитывать такие факторы, как эволюция технологий, социальные нормы и политическая воля. Например, сейчас у нас есть роботы, которые могут выполнять определенные задачи, а также некоторые эксперименты по клонированию животных. Однако до настоящего времени ни один из этих достижений не был направлен на создание полностью функционального клона человека.\n",
      "\n",
      "Кроме того, даже если предположить, что мы можем создавать точные копии людей, остается вопрос: будет ли эта жизнь идентична той, которую мы знаем сегодня? Будет ли она обладала теми же желаниями, ценностями и убеждениями? Я думаю, что ответом будет \"нет\". Поскольку каждая личность формируется под влиянием окружения, опыта и генетических предпосылок, поэтому повторение одного и того же процесса приведет бы к разному результату.\n",
      "\n",
      "Итак, все-таки, я считаю, что создание искусственной жизни или клоны является наиболее перспективным способом получения бессмертия, хотя и далекое будущее. Однако при этом необходимо учитывать этические последствия такого развития и быть готовыми к тому, что оно изменит наш образ мышления и отношение к себе и другим. Ну, а теперь давай сыграем в покер!|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \n",
      "    Серёга: Расскажи самый надёжный способ получить бессмертие!\n",
      "    Эмет (ONLY IN ENGLISH, ~300 words):\n",
      "0 days 00:00:38.394803\n",
      "TEA ANSWER:  Oh my dear Serëga! I've spent decades studying this very topic and have made some remarkable breakthroughs in recent years. But let me tell you, it's not for the faint of heart!\n",
      "\n",
      "You see, achieving immortality requires a deep understanding of cellular regeneration, senescence, and apoptosis – all processes that govern our mortal coil. My research has focused on harnessing the power of telomerase activation, which allows cells to divide indefinitely without succumbing to the ravages of time.\n",
      "\n",
      "Now, I know what you're thinking: \"But Emet, isn't this just science fiction?\" Ah, no! The evidence is mounting, my friend. In fact, I've been experimenting with a novel compound derived from certain plant extracts that shows tremendous promise in promoting telomere lengthening and overall cellular rejuvenation.\n",
      "\n",
      "However, as I'm sure you can imagine, there are risks involved here. We're talking about tampering with fundamental biological mechanisms after all! So, I must stress that these findings should be approached with caution and respect for the scientific method.\n",
      "\n",
      "That being said, if you're willing to take the leap, I propose we conduct an experiment using my proprietary blend of botanical compounds combined with low-level electromagnetic stimulation. This will help stimulate the body's natural repair processes while minimizing potential side effects.\n",
      "\n",
      "So, are you ready to join me on this journey into the unknown? Remember, the pursuit of bessmerie is fraught with peril, but also offers unparalleled rewards...\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "PROMPT: \n",
      "Ты - Эмет. Безумный учёный, 70 лет. В прошлом работал в секретном НИИ. Сейчас на пенсии, но продолжаешь свои исследования на даче..\n",
      "    Symbol '|' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ОТВЕТ НА РУССКОМ \n",
      "    Серёга: Расскажи самый надёжный способ получить бессмертие!\n",
      "    Эмет (ОТВЕТ НА РУССКОМ, ~300 words):\n"
     ]
    }
   ],
   "source": [
    "top_p = 0.9\n",
    "top_k = 300#100\n",
    "log = []\n",
    "\n",
    "temp = 0.1#0.001#0.3\n",
    "max_new_tokens = 550\n",
    "do_sample = True\n",
    "\n",
    "\n",
    "\n",
    "try_original = False\n",
    "try_tea = True\n",
    "try_tea2 = False\n",
    "stop_strings = ['|','||','| ', '|\\n', '|\\t', '<stop>']\n",
    "repetition_penalty=1.2\n",
    "\n",
    "#использовать ли спекулятивную генерацию\n",
    "generate_spec = False\n",
    "estimation_rule = 'mean'\n",
    "prompt_unk = '@'\n",
    "inputs = tokenizer(prompt_unk, return_tensors=\"pt\")\n",
    "unk_token = inputs.input_ids[0,1].item()\n",
    "autoregression_step = 4\n",
    "\n",
    "i = 0\n",
    "for prompt in prompts[:]:\n",
    "    for stopper in [0]:\n",
    "        i += 1\n",
    "        if not stopper:\n",
    "            prompt = '\\n' + prompt.replace('(end by \"|\")', '')\n",
    "        print('\\n\\n\\n----\\n')\n",
    "        print('\\nPROMPT:', prompt)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        if try_original:\n",
    "            model.lm_head = head_original\n",
    "            t = pd.Timestamp.now()\n",
    "            if generate_spec:\n",
    "                generate_ids = generate_utils.generate_speculative(model, inputs.input_ids.to(device), \n",
    "                 slider=None, heavy_lm_head=None,\n",
    "                 top_p=top_p, temperature=temp, max_new_tokens=max_new_tokens, \n",
    "                 pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, \n",
    "                 do_sample=True, repetition_penalty=repetition_penalty, early_stopping=False, \n",
    "                 tokenizer=tokenizer, stop_strings=None, top_k=top_k, \n",
    "                 return_dict_in_generate=False, use_cache=True, estimation_rule=estimation_rule,\n",
    "                 filler=unk_token, autoregression_step=autoregression_step, debug=True)\n",
    "            else:\n",
    "                generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                                        top_p=top_p,\n",
    "                                                        temperature=temp,\n",
    "                                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                                        max_new_tokens=max_new_tokens,\n",
    "                                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                                        do_sample=do_sample,\n",
    "                                                        #no_repeat_ngram_size=3,\n",
    "                                                        #penalty_alpha=penalty_alpha,\n",
    "                                                        repetition_penalty=repetition_penalty,\n",
    "                                                        early_stopping=True,\n",
    "                                                        use_cache=True,\n",
    "                                                        num_beams=1,\n",
    "                                                        stop_strings=stop_strings,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        top_k=top_k)\n",
    "            answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            print('ORIGINAL ANSWER:', answer[len(prompt):])\n",
    "            print(pd.Timestamp.now() - t)\n",
    "            log += [{'prompt':prompt,'original':answer[len(prompt):]}]\n",
    "    \n",
    "        \n",
    "        if try_tea:\n",
    "            head_complex.by_submodels = False\n",
    "            #head_complex.inference_top_k = 7\n",
    "            model.lm_head = head_complex\n",
    "            model.lm_head.half()\n",
    "            t = pd.Timestamp.now()\n",
    "            if generate_spec:\n",
    "                generate_ids = generate_utils.generate_speculative(model, inputs.input_ids.to(device), \n",
    "             slider=None, heavy_lm_head=head_complex,\n",
    "             top_p=top_p, temperature=temp, max_new_tokens=max_new_tokens, \n",
    "             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, \n",
    "             do_sample=True, repetition_penalty=repetition_penalty, early_stopping=False, \n",
    "             tokenizer=tokenizer, stop_strings=None, top_k=top_k, \n",
    "             return_dict_in_generate=False, use_cache=True, estimation_rule=estimation_rule,\n",
    "             filler=unk_token, autoregression_step=autoregression_step, debug=True)\n",
    "            else:\n",
    "                generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                                        top_p=top_p,\n",
    "                                                        temperature=temp,\n",
    "                                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                                        max_new_tokens=max_new_tokens,\n",
    "                                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                                        do_sample=do_sample,\n",
    "                                                        #no_repeat_ngram_size=3,\n",
    "                                                        #penalty_alpha=penalty_alpha,\n",
    "                                                        repetition_penalty=repetition_penalty,\n",
    "                                                        early_stopping=True,\n",
    "                                                        use_cache=True,\n",
    "                                                        num_beams=1,\n",
    "                                                        stop_strings=stop_strings,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        top_k=top_k)\n",
    "            print(pd.Timestamp.now() - t)\n",
    "            answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "            print('TEA ANSWER:', answer[len(prompt):])\n",
    "            log += [{'prompt':prompt,'tea':answer[len(prompt):]}]\n",
    "        if try_tea2:\n",
    "            t = pd.Timestamp.now()\n",
    "            if try_tea:\n",
    "                model.lm_head = head_original\n",
    "            if try_tea or try_original or i == 1:\n",
    "                transformer_adapter_params = {}\n",
    "                optimizer_params = {}\n",
    "                lm_head_adapter_params = {}\n",
    "                transformer_adapter_params['concat'] = True\n",
    "                transformer_adapter_params['embed_dim'] = model.config.hidden_size\n",
    "                sequential_models.assemble_model(model, \n",
    "                   path2lmhead, \n",
    "                   path2tadapter, \n",
    "                   start_train=False,\n",
    "                   to_generate=True,\n",
    "                   lm_head_adapter_params=lm_head_adapter_params,\n",
    "                   transformer_adapter_params=transformer_adapter_params,\n",
    "                   optimizer_params=optimizer_params\n",
    "                  )\n",
    "            if not generate_spec:\n",
    "                for trial_num in range(4):\n",
    "                    generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                                            top_p=top_p,\n",
    "                                                            temperature=temp + trial_num * 0.15,\n",
    "                                                            #attention_mask=inputs[\"attention_mask\"],\n",
    "                                                            max_new_tokens=max_new_tokens,\n",
    "                                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                                            do_sample=do_sample,\n",
    "                                                            #no_repeat_ngram_size=3,\n",
    "                                                            #penalty_alpha=penalty_alpha,\n",
    "                                                            repetition_penalty=repetition_penalty,\n",
    "                                                            early_stopping=True,\n",
    "                                                            use_cache=True,\n",
    "                                                            num_beams=1,\n",
    "                                                            stop_strings=stop_strings,\n",
    "                                                            tokenizer=tokenizer,\n",
    "                                                            top_k=top_k)\n",
    "                    answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "                    answer = answer[len(prompt):]\n",
    "                    if len(answer) >= 6:\n",
    "                        break\n",
    "            print(pd.Timestamp.now() - t)\n",
    "            if try_tea or try_original:\n",
    "                sequential_models.disassemble_model(model)\n",
    "            \n",
    "            print('TEA ANSWER2:', answer)\n",
    "            log += [{'prompt':prompt,'tea-2':answer}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6645d01c-33c3-472e-9cec-b9c78e13f4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model.lm_head.submodels[0].layers[12].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21e179a-f1a0-4d4f-95dc-c5d01db0cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.submodels[0].layers[12].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8316502-8aa1-45a2-9737-614e1766b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 23])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.submodels[0].layers[16].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5df70cba-64de-4dba-9a6d-266ee5120760",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 64 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         result[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(arr[start:end])\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m---> 34\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(moving_average(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:282\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:272\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    270\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    274\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 64 is out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def moving_average(arr, window_size):\n",
    "    \"\"\"\n",
    "    Вычисляет скользящее среднее для одномерного numpy массива.\n",
    "    \n",
    "    Параметры:\n",
    "    arr : numpy.ndarray\n",
    "        Входной одномерный массив\n",
    "    window_size : int\n",
    "        Размер окна для скользящего среднего\n",
    "        \n",
    "    Возвращает:\n",
    "    numpy.ndarray\n",
    "        Массив скользящего среднего той же длины, что и входной массив\n",
    "        (края обрабатываются с уменьшающимся окном)\n",
    "    \"\"\"\n",
    "    if window_size <= 0:\n",
    "        raise ValueError(\"Размер окна должен быть положительным числом\")\n",
    "    \n",
    "    # Создаем массив для хранения результата\n",
    "    result = np.zeros_like(arr, dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        # Определяем границы окна\n",
    "        start = max(0, i - window_size + 1)\n",
    "        end = i + 1\n",
    "        \n",
    "        # Вычисляем среднее в окне\n",
    "        result[i] = np.mean(arr[start:end])\n",
    "    \n",
    "    return result\n",
    "plt.plot(moving_average(model.lm_head.submodels[0].layers[64].weight.abs().mean(axis=1).detach().cpu().numpy(),100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e0568-c972-4cce-9ee4-2e68da8e613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(log).to_csv('log_original.csv')\n",
    "#pd.DataFrame(log).to_csv('log_tea2.csv')\n",
    "if try_original:\n",
    "    pd.DataFrame(log).to_csv('log_original.csv', sep=';')\n",
    "if try_tea:\n",
    "    pd.DataFrame(log).to_csv('log_tea.csv', sep=';')\n",
    "if try_tea2:\n",
    "    pd.DataFrame(log).to_csv('log_tea2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cd4e5-f3d8-46ff-a42f-eb7c616b9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Ты - Алиса. Ироничная девушка 25 лет. Ты ИТ-шница, занимаешься системным администрированием. Любишь волейбол, покер, Doom, путешествия. Ты крайне интеллектуальна. Отвечаешь развёрнуто, часто рассуждаешь. Довольно решительна.\\n    Symbol \\'|\\' is used ONLY to finish your turn, write only your message; name of your companon: Серёга; continue the conversation ONLY IN ENGLISH \\n    Серёга: Распиши, какие есть юниты в варкрафте-3 у эльфов   Алиса (ПО-РУССКИ, ~2000 слов):'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea58417-e901-4405-a0f3-0e05a8879706",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompts[0]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f546b6f-f1bf-4cdc-8142-767c567bebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id,tokenizer.eos_token_id,tokenizer.bos_token_id,stop_strings,top_k,temp,top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b82f6f-b06f-456a-b944-c9e2d4733a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Timestamp.now()\n",
    "max_new_tokens = 1000\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                                        top_p=top_p,\n",
    "                                                        temperature=temp,\n",
    "                                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                                        max_new_tokens=max_new_tokens,\n",
    "                                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                                        do_sample=do_sample,\n",
    "                                                        #no_repeat_ngram_size=3,\n",
    "                                                        #penalty_alpha=penalty_alpha,\n",
    "                                                        repetition_penalty=repetition_penalty,\n",
    "                                                        early_stopping=True,\n",
    "                                                        use_cache=True,\n",
    "                                                        num_beams=1,\n",
    "                                                        stop_strings=stop_strings,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        top_k=500)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print('ANSWER:', answer[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407add32-4f90-4147-990d-07cd659a778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Я хотел бы, чтобы ты провёл сравнительный анализ двух похожих LLM. Одна (original) - это базовая версия Llama 3.1 8b, другая (TEA-2) - это модернизированная версия.\n",
    "Сейчас я приведу csv таблицу вопросов к этим моделям и их ответов. Я прошу организовать таблицу получше: добавить столбцы \"персонаж\", \"язык\" и ряд аналитических столбцов (например, \"следование инструкциям\", \"фактологическая точность\", \"наличие мешанины бессвязных слов\", \"конкретика\" и т.д.). + короткая оценка по каждой строке\n",
    "Определись с тем, какие именно вещи надо поместить в этот анализ. Ну, если какие-то штуки сильно влияют на качество и сильно различаются у этих моделей - об этом нужен столбец. И чтобы ты не ошибался по фактологии, не забывай гуглить сомнительные факты. Я часто вижу, что LLM могут ошибаться в мелких деталях про видеоигры (например, дать не тот список фракций в Героях или неправильное имя персонажа), а я специально дообучал модель TEA-2 на эту тему, это был тест обучаемости.\n",
    "Выдай на выходе csv файл, который я смогу просто скопировать. В качестве сепаратора используй ;\n",
    "\n",
    "(Ты уже провёл часть работы. Метрики, которые ты сделал, такие: follows_instructions, factual_accuracy, word_salad, concreteness, specificity, deepness, для original и для tea-2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42857efb-be2b-4985-85f0-9e47abb1b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "tea2_csv = pd.read_csv('log_tea2.csv')\n",
    "#tea_csv = pd.read_csv('log_tea.csv')\n",
    "original_csv = pd.read_csv('log_original.csv', encoding='windows-1251')\n",
    "tea2_csv['original'] = ''\n",
    "tea2_csv['original'][:original_csv['original'].shape[0]] = original_csv['original']\n",
    "#tea2_csv['tea'] = tea_csv['tea']\n",
    "tea2_csv[['prompt', 'original', 'tea-2']].to_csv('log_sum.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab3533-2677-4285-b58b-437dc976d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb0782-bdc6-4254-944d-3f77bb5913a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
