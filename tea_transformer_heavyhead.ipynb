{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b30ab6-3e00-423f-8453-54ffec3c6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#иной подход к TEA\n",
    "#1) Ставим адаптер в параллель с последним трансформерным слоем. Адаптер - это трансформерные слои + резнеты между ними\n",
    "#2) Возможно, ставим адаптер ещё и в параллель с оригинальной LM-head\n",
    "#Причём линейную LM-head можно как обучать, так и не обучать\n",
    "#Почему-то лоссы выходят не очень. Чем больше обучаем, тем более \"не очень\".\n",
    "#Поэтому полагаю, верный пайплайн такой: обучили и трансформерные адаптеры, и резнет на выходе (train_transformer = True, train_lm_head = True). \n",
    "#А потом, когда трансформерные слои научились выделять интересные нам фичи и делать эмбеддинг более репрезентативным,\n",
    "#Ставим train_transformer = False, train_lm_head = True - и дотягиваем последний адаптер\n",
    "#Специально стараюсь сделать, чтобы выходной адаптер был побольше, а трансформерные - поменьше. \n",
    "#Потому что затраты на выполнение трансформерных = O(N^2), где N - длина входной последовательности. А у выходного резнета сложность O(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7956a976-844c-4617-acb3-cb391e1368ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False\n",
    "#Через requirements поставить это затруднительно, последовательность установки важна, так как часть либ компилируются\n",
    "#Эта последовательность установки, как бы странно она ни выглядела, работает. Не в любых окружениях, но во многих.\n",
    "if install:\n",
    "\n",
    "    !pip install deepspeed==0.14.4\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --upgrade transformers==4.46.2 tyro==0.9.8 triton==2.3.1 trl==0.12.0\n",
    "\n",
    "    !pip install -v \"xformers==0.0.29.post1\"\n",
    "    #!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps -v \"peft==0.13.2\"\n",
    "    !pip install --no-deps packaging ninja einops flash-attn bitsandbytes==0.44.1\n",
    "    !pip install accelerate==0.34.2\n",
    "    !pip uninstall peft -y\n",
    "    !pip install -v \"peft==0.13.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b52d18-5aea-4246-b3d1-16565072b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c36be-465a-4494-b95a-efcaeade1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется GPU с номером: 0\n"
     ]
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Используется GPU с номером: {current_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09deb705-6e6d-461d-bdd7-c4d663280ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем доступность CUDA и количество GPU\n",
    "# if torch.cuda.is_available():\n",
    "#     if torch.cuda.device_count() > 1:  # Если есть более 1 GPU\n",
    "#         device = torch.device(\"cuda:1\")  # Явно указываем устройство 1\n",
    "#         print(f\"Переключились на GPU 1\")\n",
    "#     else:\n",
    "#         print(\"Доступен только один GPU (номер 0), переключение невозможно\")\n",
    "# else:\n",
    "#     print(\"CUDA недоступно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977ca4fe-5230-4018-a6f3-37a4848db95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c5e862-053f-4464-b502-c9a792e350de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Remaining GPU memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d95a8a-039f-489c-adc8-3b626b7cdd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_models import GPTAdapterLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae58ec38-60aa-4cfe-8231-8d9b68b7ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = False#Запускаем ли мы сейчас обучение с нуля, или с какого-то чекпоинта\n",
    "train_transformer = True\n",
    "train_lm_head = True\n",
    "\n",
    "\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling\n",
    "\n",
    "bits_per_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f909942-92c0-489f-86ef-02dd1e5ddbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size4loss = 1024*2\n",
    "head_max_batch_size = 1024*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a1bda1-705f-466e-ab80-af70966d1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конфигурации моделек\n",
    "mode = 'pretrain'#'finetune', 'pretrain'\n",
    "model_version = \"heavy_3\"\n",
    "\n",
    "\n",
    "if model_version == \"light_1\":\n",
    "    embed_dim = 2048\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 4\n",
    "    t_layers_count = 2\n",
    "    layer_configs = [2048] * 4\n",
    "    batch_size = 2\n",
    "    accum_batch = 2\n",
    "    lr = 1e-3\n",
    "    opt_type = 'adam'\n",
    "    heavy_head = False\n",
    "elif model_version == \"heavy_1\":\n",
    "    embed_dim = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 2\n",
    "    t_layers_count = 3\n",
    "    layer_configs = [1024*3] * 6\n",
    "    batch_size = 1\n",
    "    accum_batch = 8000#1000\n",
    "    lr = 1e-6\n",
    "    conservativity = 1e-3\n",
    "    opt_type = 'adam'\n",
    "    heavy_head = False\n",
    "elif model_version == \"heavy_2\":\n",
    "    embed_dim = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 4\n",
    "    t_layers_count = 3\n",
    "    layer_configs = [1024*4] * 10\n",
    "    batch_size = 1\n",
    "    accum_batch = 10000#1000\n",
    "    lr = 1e-6\n",
    "    conservativity = 1e-3\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    embedding_size = embed_dim\n",
    "    heavy_head = False\n",
    "    net_dropout_rate = 0\n",
    "    individ_dropout_rate = 0.02\n",
    "    layer_configs_head = [1024] * 2 #[2048] * 3 #+ [1024] * 4\n",
    "    sample_features = 1\n",
    "    composition_size = 1\n",
    "    memnet_params = {'num_heads':12, 'query_size':128, 'num_key_values':320, 'value_size':256}\n",
    "    use_memnets = True\n",
    "elif model_version == \"heavy_3\":\n",
    "    embed_dim = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 4\n",
    "    t_layers_count = 2\n",
    "    layer_configs = [1024*4] * 2\n",
    "    batch_size = 1\n",
    "    accum_batch = 3000#1000\n",
    "    lr = 1e-7\n",
    "    conservativity = 0.2\n",
    "    dropout = 0.15\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    embedding_size = embed_dim\n",
    "    heavy_head = True\n",
    "    net_dropout_rate = 0\n",
    "    individ_dropout_rate = 0.02\n",
    "    layer_configs_head = [2048] * 8 + [1024] * 9\n",
    "    sample_features = 1\n",
    "    composition_size = 1\n",
    "    memnet_params = {'num_heads':12, 'query_size':128, 'num_key_values':320, 'value_size':256}\n",
    "    use_memnets = False\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "elif model_version == \"heavy_4\":\n",
    "    embed_dim = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 4\n",
    "    t_layers_count = 1\n",
    "    layer_configs = [1024*4] * 2\n",
    "    batch_size = 1\n",
    "    accum_batch = 1000#1000\n",
    "    lr = 1e-5\n",
    "    conservativity = 3e-2\n",
    "    dropout = 0.15\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    embedding_size = embed_dim\n",
    "    heavy_head = True\n",
    "    net_dropout_rate = 0\n",
    "    individ_dropout_rate = 0.02\n",
    "    layer_configs_head = [2048] * 2 + [1024] * 2\n",
    "    sample_features = 1\n",
    "    composition_size = 1\n",
    "    memnet_params = {'num_heads':12, 'query_size':128, 'num_key_values':320, 'value_size':256}\n",
    "    use_memnets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb017559-8613-415c-b004-a8864c3245e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2tadapter = f\"tadapter_{model_version}.pth\"\n",
    "path2lmhead = f\"lmhead_{model_version}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4be17ac-e936-4a01-b028-48e64618af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "/tmp/ipykernel_1539717/2897674363.py:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/ssdovgan/main/lib/python3.12/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")#, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_name,\n",
    "              #device_map=\"auto\",\n",
    "              #device_map=device,\n",
    "              #torch_dtype=torch.bfloat16,\n",
    "              quantization_config=bnb_config,\n",
    "              cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886ece6-fd34-4e66-8584-5e4879fcb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model: nn.Module) -> tuple[int, int]:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable, frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3168899-730d-4eee-ab65-03ad429721a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_adapter(model, path2tadapter,path2lmhead, train_transformer, train_lm_head, heavy_head):\n",
    "\n",
    "    # Замена последнего слоя.\n",
    "    initial_layer = model.model.layers[-1]\n",
    "\n",
    "    #создать шаблон модели\n",
    "    model.model.layers[-1] = GPTAdapterLayer(initial_layer, \n",
    "                                             dim=128, \n",
    "                                             num_heads=2, \n",
    "                                             ff_dim=128, \n",
    "                                             t_layers_count=1, \n",
    "                                             layer_configs=[64],\n",
    "                                             conservativity=1.,\n",
    "                                             dropout=0.1).to(device)\n",
    "    \n",
    "    \n",
    "    model.model.layers[-1].corrector_stack = torch.load(path2tadapter, weights_only=False).to(device)\n",
    "    #model.lm_head = torch.load(path2lmhead, weights_only=False).to(device)\n",
    "    if heavy_head:\n",
    "        model.lm_head = torch.load(path2lmhead, weights_only=False).to(device)\n",
    "    else:\n",
    "        model.lm_head.load_state_dict(torch.load(path2lmhead))\n",
    "\n",
    "    # Заморозка всей модели\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Разморозка последнего блока\n",
    "    if train_transformer:\n",
    "        for param in model.model.layers[-1].corrector_stack.parameters():\n",
    "            param.requires_grad = True\n",
    "    #И lm-head\n",
    "    if train_lm_head or train_transformer:\n",
    "        model.lm_head.float()\n",
    "    \n",
    "    if train_lm_head:\n",
    "        for param in model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    if heavy_head:\n",
    "        for param in model.lm_head.submodels[-1].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if train_transformer:\n",
    "        model.model.layers[-1].transformer_float_mode = 32\n",
    "    else:\n",
    "        model.model.layers[-1].transformer_float_mode = 16\n",
    "    if train_lm_head:\n",
    "        model.model.layers[-1].lmhead_float_mode = 32\n",
    "    elif train_transformer:\n",
    "        model.model.layers[-1].lmhead_float_mode = 16\n",
    "    \n",
    "    trainable, frozen = count_trainable_parameters(model)\n",
    "    print(f\"Обучаемые: {trainable:,}\\nЗамороженные: {frozen:,}\")\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c29441-c84b-45db-8d86-b9732047417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_memnets True\n",
      "Обучаемые: 2,401,041,221\n",
      "Замороженные: 4,540,728,576\n",
      "Обучаемые: 2,401,041,221\n",
      "Замороженные: 4,540,728,576\n"
     ]
    }
   ],
   "source": [
    "if start_train:\n",
    "    # Исходная моделька\n",
    "\n",
    "\n",
    "    # Замена последнего слоя. Тут трансформер, то можно и твой резнет... \n",
    "    initial_layer = model.model.layers[-1]\n",
    "    \n",
    "    #создать модель\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    \n",
    "    model.model.layers[-1] = GPTAdapterLayer(initial_layer, \n",
    "                                             dim=embed_dim, \n",
    "                                             num_heads=num_heads_tlayer, \n",
    "                                             ff_dim=ff_dim, \n",
    "                                             t_layers_count=t_layers_count, \n",
    "                                             layer_configs=layer_configs,\n",
    "                                             conservativity=conservativity,\n",
    "                                             dropout=dropout).to(device)\n",
    "    \n",
    "    if heavy_head:\n",
    "        torch.save(model.lm_head.weight, \"lin_model.pth\")\n",
    "        head = ensembles.EResNetPro(input_size=embedding_size, \n",
    "               out_size=cardinality, \n",
    "               net_dropout_rate=net_dropout_rate, \n",
    "               individ_dropout_rate=individ_dropout_rate,\n",
    "               layer_configs=layer_configs_head, \n",
    "               use_sigmoid_end=False, \n",
    "               use_batchnorm=True, \n",
    "               use_activation=True, \n",
    "               activation=nn.LeakyReLU(), \n",
    "               sample_features=sample_features, \n",
    "               composition_size=composition_size, \n",
    "               lin_bottleneck_size=None,\n",
    "               lin_model_add=nn.Linear(embedding_size, cardinality).to(device),\n",
    "               memnet_params=memnet_params,\n",
    "               use_memnets=use_memnets,\n",
    "               max_batch_size=4096,\n",
    "               aggregation_by_mean=False).to(device)\n",
    "        head.max_batch_size=head_max_batch_size\n",
    "        head.submodels[-1].weight = torch.nn.Parameter(torch.load( \"lin_model.pth\").to(device).to(torch.float32))\n",
    "        head.submodels[-1].weight.requires_grad = True\n",
    "        model.lm_head = head\n",
    "\n",
    "        # Заморозка всей модели\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Разморозка последнего блока\n",
    "        if train_transformer:\n",
    "            for param in model.model.layers[-1].corrector_stack.parameters():\n",
    "                param.requires_grad = True\n",
    "        #И lm-head\n",
    "        model.lm_head.float()\n",
    "        \n",
    "        if train_lm_head:\n",
    "            for param in model.lm_head.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        if heavy_head:\n",
    "            for param in model.lm_head.submodels[-1].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if train_transformer:\n",
    "            model.model.layers[-1].transformer_float_mode = 32\n",
    "        if train_lm_head:\n",
    "            model.model.layers[-1].lmhead_float_mode = 32\n",
    "        \n",
    "        \n",
    "        trainable, frozen = count_trainable_parameters(model)\n",
    "        print(f\"Обучаемые: {trainable:,}\\nЗамороженные: {frozen:,}\")\n",
    "else:\n",
    "    integrate_adapter(model, path2tadapter,path2lmhead, train_transformer, train_lm_head, heavy_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74394285-a193-4acc-81b0-f40ffb21c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_type == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "else:\n",
    "    momentum = 0.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr*0.01, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "764405c6-3c7a-4370-b85a-feed823a6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd25f40-212e-486a-8718-3ca6b15b7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1\n"
     ]
    }
   ],
   "source": [
    "#настройки датасета\n",
    "\n",
    "if mode == 'pretrain':\n",
    "    #претрейн\n",
    "    max_seq_len = 6900\n",
    "    max_seq_len4inference = 6850\n",
    "    # max_seq_len = 4500\n",
    "    # max_seq_len4inference = 4450\n",
    "    # max_seq_len = 3800\n",
    "    # max_seq_len4inference = 3750\n",
    "    # max_seq_len = 3600\n",
    "    # max_seq_len4inference = 3550\n",
    "else:\n",
    "    #дообучение\n",
    "    # max_seq_len = 1950\n",
    "    # max_seq_len4inference = 1900\n",
    "    max_seq_len = 2050\n",
    "    max_seq_len4inference = 2000\n",
    "\n",
    "print('batch_size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caab10e0-52e4-4d09-b8f3-687c676051a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071219"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "while 1:\n",
    "    try:\n",
    "        if mode == 'pretrain':\n",
    "            #dataset = InstructDatasetR(\"./data/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_flat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "        else:\n",
    "            dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e, pd.Timestamp.now())\n",
    "        time.sleep(10)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f63ff8aa-cac7-4c1e-a034-7396ab6ae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens\n",
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52472c64-c2da-4ac5-826e-21379cf5a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0194cf5f-2b09-4fd9-b730-3d04ddc52315",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255071c2-229c-4c90-84b3-fc192d38a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1539717/2664646116.py:52: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  weights_full = torch.stack(weights_full).to('cuda').T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss 1.778 5024 from 1071219 0.469 % acc 0.6626245524088542 2025-04-20 12:43:16.201619\n",
      "ce_loss 1.76 10125 from 1071219 0.94518 % acc 0.6639802856445313 2025-04-20 13:06:47.375269\n",
      "ce_loss 1.756 15178 from 1071219 1.41689 % acc 0.6695648803710937 2025-04-20 13:30:03.021425\n",
      "ce_loss 1.747 20411 from 1071219 1.9054 % acc 0.6697180989583333 2025-04-20 13:54:21.487592\n",
      "ce_loss 1.727 25448 from 1071219 2.37561 % acc 0.6693824462890625 2025-04-20 14:18:39.164935\n",
      "ce_loss 1.701 30563 from 1071219 2.8531 % acc 0.6723819580078125 2025-04-20 14:42:20.754488\n"
     ]
    }
   ],
   "source": [
    "print('batch_size', batch_size)\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    #for i in range(batch_size):\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < batch_size:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        #print('fact_size', np.min([len(input_ids) + len(labels), max_seq_len4inference]), len(input_ids) + len(labels))\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "\n",
    "        # if not torch.any(input_ids!=0):\n",
    "        #     #X + Y\n",
    "        #     print(list(input_ids_cur.numpy()))\n",
    "        #     print(list(labels_cur.numpy()))\n",
    "        #     1/0\n",
    "    i_pointer += i #should be batch_size\n",
    "    #print('A', pd.Timestamp.now())\n",
    "    \n",
    "    input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "    labels_full = torch.stack(labels_full).to('cuda')\n",
    "    weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "    cnt_pads = count_padding(labels_full, padding_token)\n",
    "    if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "        continue\n",
    "    if cnt_pads > 0:\n",
    "        input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "        labels_full = labels_full[:, :-cnt_pads]\n",
    "            \n",
    "        outp = model.forward(input_ids_full)\n",
    "        logits = outp['logits']\n",
    "        del outp\n",
    "        #outp['states'] = outp['hidden_states'][-1]\n",
    "    # {\"loss\": loss_agg, \"logits\": logits, \"states\":embeddings}\n",
    "        #outp['states'] = outp['states'][:, :input_ids_full.shape[1]]\n",
    "        #state_cur = outp['states'].reshape([outp['states'].shape[0] * outp['states'].shape[1], outp['states'].shape[2]])#.cpu().to(torch.float16).numpy()\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        weights_cur = weights_full2d.ravel()\n",
    "\n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "        #state_cur = state_cur[~idx]\n",
    "        weights_cur = weights_cur[~idx]\n",
    "    else:\n",
    "        continue\n",
    "    if labels_cur.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    for batch_start in range(0, logits.shape[0], max_batch_size4loss):\n",
    "        ce_loss = loss_fct(logits.view([logits.shape[0] * logits.shape[1], logits.shape[2]])[~idx][batch_start : batch_start + max_batch_size4loss], labels_cur.view(-1)[batch_start : batch_start + max_batch_size4loss])\n",
    "        ce_loss = ce_loss * weights_cur[batch_start : batch_start + max_batch_size4loss].to('cuda')\n",
    "        ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "        loss = torch.mean(ce_loss)\n",
    "        loss.backward()\n",
    "        torch.cuda.empty_cache()\n",
    "    # ce_loss = loss_fct(logits.view([logits.shape[0] * logits.shape[1], logits.shape[2]])[~idx], labels_cur.view(-1))\n",
    "    # ce_loss = ce_loss * weights_cur.to('cuda')\n",
    "    # ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "    # loss = torch.mean(ce_loss)\n",
    "    # loss.backward()\n",
    "    \n",
    "    #print('D', pd.Timestamp.now())\n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits.view([logits.shape[0] * logits.shape[1], logits.shape[2]])[~idx], axis=-1) == labels_cur).to(torch.float16))\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= accum_batch:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "\n",
    "        \n",
    "        try:\n",
    "            if (np.random.rand() < 1 and accum_batch > 100) or (np.random.rand() < 0.1 and accum_batch <= 100):\n",
    "                model.lm_head.training = False\n",
    "                torch.save(model.model.layers[-1].corrector_stack, path2tadapter)\n",
    "                if heavy_head:\n",
    "                    torch.save(model.lm_head, path2lmhead)\n",
    "                else:\n",
    "                    torch.save(model.lm_head.state_dict(), path2lmhead)\n",
    "    \n",
    "                if np.random.rand()<0.2:\n",
    "                    torch.save(model.model.layers[-1].corrector_stack, path2tadapter + '.back')\n",
    "                    if heavy_head:\n",
    "                        torch.save(model.lm_head, path2lmhead + '.back')\n",
    "                    else:\n",
    "                        torch.save(model.lm_head.state_dict(), path2lmhead + '.back')\n",
    "                model.lm_head.training = True\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08e9db-b419-4054-a5eb-307164626524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4499188-93b6-42e4-a7a1-8abf34ee44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#батч = 4, аккум = 20\n",
    "#53 сек\n",
    "#батч = 2, аккум = 40\n",
    "#42 сек\n",
    "#батч = 1, аккум = 80\n",
    "#35 сек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa1b19-5727-4a31-b879-b0496f3b6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d538159-b67d-4d5d-9ddf-b4493f54530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Засейвить\n",
    "model.lm_head.training = False\n",
    "torch.save(model.model.layers[-1].corrector_stack, path2tadapter)\n",
    "if heavy_head:\n",
    "    torch.save(model.lm_head, path2lmhead)\n",
    "else:\n",
    "    torch.save(model.lm_head.state_dict(), path2lmhead)\n",
    "model.lm_head.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fda8f-27c1-435e-871f-d45eeb99d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e01f4-cdb9-487c-9926-d2aa0f18e002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eed02c-8b10-4ff6-b40b-565df9aeadff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7776186-74b5-4dfd-b3de-725271f0578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check generation\n",
    "\n",
    "model.model.layers[-1].half()\n",
    "model.lm_head.half()\n",
    "model.model.layers[-1].transformer_float_mode = 16\n",
    "model.model.layers[-1].lmhead_float_mode = 16\n",
    "\n",
    "prompt = 'Твоя роль: Сталин. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Девушка-флористка. \\n Серёга: Какая фракция в игре Герои Меча и Магии 3 соответствует твоей личности? Your answer, ONLY ENGLISH (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Data scientist. \\n Серёга: Мне надо улучшить LLM. У меня ресурсы на запуск Llama 3.1 8B или Qwen 2.5 14B, квантованные до 4 бит. Мне нужно, чтобы мои модели былу умнее (на уровне 70 B), но при этом имели настолько же низкие требования по памяти и запускались так же быстро. Я готов обучать новую модель. Я готов пробовать другие архитектуры. Опиши план, что делать. Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Инженер-ракетчик. \\n Серёга: Мне надо сделать ракету, в домашних условиях, небольшую. Расскажи, как это провернуть? ТВОЙ ОТВЕТ НА РУССКОМ  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Геймер, студент-инженер. \\n Серёга: Перечисли всех монстров из Doom, каких вспомнишь! Your answer, ONLY ENGLISH  (end by \"|\"):'\n",
    "#prompt = 'Твоя роль: Гермиона Грейнджер. \\n Серёга: Как справиться с василиском? Your answer, ONLY ENGLISH  (end by \"|\", 200 tokens):'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "repetition_penalty = 1.2\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                        max_new_tokens=200,\n",
    "                                        temperature=0.01,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        #no_repeat_ngram_size=3,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        early_stopping=True,\n",
    "                                        use_cache=True,\n",
    "                                        num_beams=1,\n",
    "                                        tokenizer=tokenizer)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9365b7d-180d-4ced-81fe-455a911cb901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40443d77-de33-49e5-a904-eaee6f498666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
