{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b30ab6-3e00-423f-8453-54ffec3c6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#иной подход к TEA: теперь адаптер будем ставить не в параллель с LM-head, а в параллель с трансформерным слоем. \n",
    "#И адаптер = трансформерные слои, чередующиеся с резнетами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b52d18-5aea-4246-b3d1-16565072b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977ca4fe-5230-4018-a6f3-37a4848db95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d95a8a-039f-489c-adc8-3b626b7cdd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_models import GPTAdapterLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae58ec38-60aa-4cfe-8231-8d9b68b7ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = True#Запускаем ли мы сейчас обучение с нуля, или с какого-то чекпоинта\n",
    "\n",
    "\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling\n",
    "\n",
    "bits_per_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a1bda1-705f-466e-ab80-af70966d1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конфигурации моделек\n",
    "mode = 'finetune'#'finetune', 'pretrain'\n",
    "model_version = \"light_1\"\n",
    "if model_version == \"light_1\":\n",
    "    embed_dim = 2048\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 4\n",
    "    t_layers_count = 2\n",
    "    layer_configs = [2048, 2048, 2048, 2048]\n",
    "    batch_size = 2\n",
    "    accum_batch = 2\n",
    "    lr = 1e-3\n",
    "    opt_type = 'adam'\n",
    "elif model_version == \"heavy_1\":\n",
    "    embed_dim = 4096\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    num_heads_tlayer = 16\n",
    "    ff_dim = embed_dim  * 2\n",
    "    t_layers_count = 2\n",
    "    layer_configs = [2048, 2048, 2048, 2048]\n",
    "    batch_size = 2\n",
    "    accum_batch = 10\n",
    "    lr = 1e-3\n",
    "    opt_type = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb017559-8613-415c-b004-a8864c3245e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2tadapter = f\"tadapter_{model_version}.pth\"\n",
    "path2lm_head = f\"lmhead_{model_version}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4be17ac-e936-4a01-b028-48e64618af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "/tmp/ipykernel_2828930/2897674363.py:21: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
      "/home/ssdovgan/main/lib/python3.12/site-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")#, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              model_name,\n",
    "              #device_map=\"auto\",\n",
    "              #device_map=device,\n",
    "              #torch_dtype=torch.bfloat16,\n",
    "              quantization_config=bnb_config,\n",
    "              cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c29441-c84b-45db-8d86-b9732047417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_memnets True\n",
      "use_memnets True\n",
      "Обучаемые: 180,251,602\n",
      "Замороженные: 749,275,136\n"
     ]
    }
   ],
   "source": [
    "# Исходная моделька\n",
    "\n",
    "\n",
    "# Замена последнего слоя. Тут трансформер, то можно и твой резнет... \n",
    "initial_layer = model.model.layers[-1]\n",
    "\n",
    "#создать модель\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "model.model.layers[-1] = GPTAdapterLayer(initial_layer, \n",
    "                                         dim=embed_dim, \n",
    "                                         num_heads=num_heads_tlayer, \n",
    "                                         ff_dim=ff_dim, \n",
    "                                         t_layers_count=t_layers_count, \n",
    "                                         layer_configs=layer_configs,\n",
    "                                         conservativity=conservativity ).to(device)\n",
    "\n",
    "if start_train:\n",
    "    pass\n",
    "else:\n",
    "    model.model.layers[-1].corrector_stack = torch.load(path2tadapter, weights_only=False).to(device)\n",
    "    #model.lm_head = torch.load(path2lmhead, weights_only=False).to(device)\n",
    "    model.lm_head.load_state_dict(torch.load(path2lmhead))\n",
    "\n",
    "# Заморозка всей модели\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Разморозка последнего блока\n",
    "for param in model.model.layers[-1].corrector_stack.parameters():\n",
    "    param.requires_grad = True\n",
    "#И lm-head\n",
    "model.lm_head.float()\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "model.model.layers[-1].transformer_float_mode = 32\n",
    "model.model.layers[-1].lmhead_float_mode = 32\n",
    "\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model: nn.Module) -> tuple[int, int]:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable, frozen\n",
    "trainable, frozen = count_trainable_parameters(model)\n",
    "print(f\"Обучаемые: {trainable:,}\\nЗамороженные: {frozen:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74394285-a193-4acc-81b0-f40ffb21c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_type == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "else:\n",
    "    momentum = 0.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr*0.01, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764405c6-3c7a-4370-b85a-feed823a6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd25f40-212e-486a-8718-3ca6b15b7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n"
     ]
    }
   ],
   "source": [
    "#настройки датасета\n",
    "\n",
    "if mode == 'pretrain':\n",
    "    #претрейн\n",
    "    max_seq_len = 6900\n",
    "    max_seq_len4inference = 6850\n",
    "    # max_seq_len = 4500\n",
    "    # max_seq_len4inference = 4450\n",
    "    # max_seq_len = 3800\n",
    "    # max_seq_len4inference = 3750\n",
    "    # max_seq_len = 3600\n",
    "    # max_seq_len4inference = 3550\n",
    "else:\n",
    "    #дообучение\n",
    "    # max_seq_len = 1950\n",
    "    # max_seq_len4inference = 1900\n",
    "    max_seq_len = 2050\n",
    "    max_seq_len4inference = 2000\n",
    "\n",
    "print('batch_size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caab10e0-52e4-4d09-b8f3-687c676051a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663531"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "if mode == 'pretrain':\n",
    "    #dataset = InstructDatasetR(\"./data/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "    dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "else:\n",
    "    dataset = InstructDatasetR(\"../data_tea/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f63ff8aa-cac7-4c1e-a034-7396ab6ae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens\n",
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52472c64-c2da-4ac5-826e-21379cf5a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0194cf5f-2b09-4fd9-b730-3d04ddc52315",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "255071c2-229c-4c90-84b3-fc192d38a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "ce_loss 3.164 6 from 663531 0.0009 % acc 0.465576171875 2025-04-11 00:31:08.054529\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "ce_loss 2.489 10 from 663531 0.00151 % acc 0.544677734375 2025-04-11 00:31:09.286413\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "ce_loss 4.113 16 from 663531 0.00241 % acc 0.333251953125 2025-04-11 00:31:10.484526\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     91\u001b[39m loss.backward()\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#print('D', pd.Timestamp.now())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m acc = torch.mean((\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m~\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m == labels_cur).to(torch.float16))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m logits\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m labels_cur\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#проинференсить модель на батчах, сделать датасет для постпроцессинга\n",
    "print('batch_size', batch_size)\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    #for i in range(batch_size):\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < batch_size:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        #print('fact_size', np.min([len(input_ids) + len(labels), max_seq_len4inference]), len(input_ids) + len(labels))\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "\n",
    "        # if not torch.any(input_ids!=0):\n",
    "        #     #X + Y\n",
    "        #     print(list(input_ids_cur.numpy()))\n",
    "        #     print(list(labels_cur.numpy()))\n",
    "        #     1/0\n",
    "    i_pointer += i #should be batch_size\n",
    "    #print('A', pd.Timestamp.now())\n",
    "    \n",
    "    input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "    labels_full = torch.stack(labels_full).to('cuda')\n",
    "    weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "    cnt_pads = count_padding(labels_full, padding_token)\n",
    "    if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "        continue\n",
    "    if cnt_pads > 0:\n",
    "        input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "        labels_full = labels_full[:, :-cnt_pads]\n",
    "            \n",
    "        outp = model.forward(input_ids_full)\n",
    "        logits = outp['logits']\n",
    "        del outp\n",
    "        #outp['states'] = outp['hidden_states'][-1]\n",
    "    # {\"loss\": loss_agg, \"logits\": logits, \"states\":embeddings}\n",
    "        #outp['states'] = outp['states'][:, :input_ids_full.shape[1]]\n",
    "        #state_cur = outp['states'].reshape([outp['states'].shape[0] * outp['states'].shape[1], outp['states'].shape[2]])#.cpu().to(torch.float16).numpy()\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        weights_cur = weights_full2d.ravel()\n",
    "\n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "        #state_cur = state_cur[~idx]\n",
    "        weights_cur = weights_cur[~idx]\n",
    "    else:\n",
    "        continue\n",
    "    print(1)\n",
    "    if labels_cur.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    ce_loss = loss_fct(logits.view([logits.shape[0] * logits.shape[1], logits.shape[2]])[~idx], labels_cur.view(-1))\n",
    "    print(2)\n",
    "    ce_loss = ce_loss * weights_cur.to('cuda')\n",
    "    ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "    loss = torch.mean(ce_loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    #print('D', pd.Timestamp.now())\n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits.view([logits.shape[0] * logits.shape[1], logits.shape[2]])[~idx], axis=-1) == labels_cur).to(torch.float16))\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= accum_batch:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        if (np.random.rand() < 1 and accum_batch > 100) or (np.random.rand() < 0.1 and accum_batch <= 100):\n",
    "            model.lm_head.training = False\n",
    "            torch.save(model.model.layers[-1].corrector_stack, path2tadapter)\n",
    "            torch.save(model.lm_head, path2lmhead)\n",
    "\n",
    "            if np.random.rand()<0.2:\n",
    "                torch.save(model.model.layers[-1].corrector_stack, path2tadapter + '.back')\n",
    "                torch.save(model.lm_head, path2lmhead + '.back')\n",
    "            model.lm_head.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa1b19-5727-4a31-b879-b0496f3b6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d538159-b67d-4d5d-9ddf-b4493f54530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Засейвить\n",
    "model.lm_head.training = False\n",
    "torch.save(model.model.layers[-1].corrector_stack, path2tadapter)\n",
    "torch.save(model.lm_head, path2lmhead)\n",
    "model.lm_head.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fda8f-27c1-435e-871f-d45eeb99d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e01f4-cdb9-487c-9926-d2aa0f18e002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eed02c-8b10-4ff6-b40b-565df9aeadff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7776186-74b5-4dfd-b3de-725271f0578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check generation\n",
    "\n",
    "model.model.layers[-1].half()\n",
    "model.lm_head.half()\n",
    "model.model.layers[-1].transformer_float_mode = 16\n",
    "model.model.layers[-1].lmhead_float_mode = 16\n",
    "\n",
    "\n",
    "prompt = \"Превед медвед\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = model.generate(inputs.input_ids.to(device),\n",
    "                                        #attention_mask=inputs[\"attention_mask\"],\n",
    "                                        max_new_tokens=200,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        #no_repeat_ngram_size=3,\n",
    "                                        #penalty_alpha=penalty_alpha,\n",
    "                                        early_stopping=True,\n",
    "                                        use_cache=True,\n",
    "                                        num_beams=1,\n",
    "                                        tokenizer=tokenizer)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9365b7d-180d-4ced-81fe-455a911cb901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40443d77-de33-49e5-a904-eaee6f498666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
