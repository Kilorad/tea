{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f211d5f3-5f53-4380-9241-7e471ba073ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#учит LLM в сборе. То есть приделываем в конец feature head и так и учим.\n",
    "#особенность в том, что мы накладываем адаптер на последовательность, а не один эмбеддинг\n",
    "#в даннм случае адаптер - это конволюционка, перемалывающая несколько последних эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5889c1ea-bfb9-4a28-adfd-200fc1251ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False\n",
    "\n",
    "if install:\n",
    "    !conda install -c anaconda git -y\n",
    "\n",
    "    !pip install deepspeed==0.14.4\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --upgrade transformers==4.43.3 tyro==0.9.8 triton==2.3.1 trl==0.8.6\n",
    "\n",
    "    !pip install -v \"xformers==0.0.29.post1\"\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps -v \"peft==0.13.2\"\n",
    "    !pip install --no-deps packaging ninja einops flash-attn bitsandbytes\n",
    "    !pip install accelerate==0.34.2\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    !pip uninstall peft -y\n",
    "    !pip install -v \"peft==0.13.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2570a2e2-a09e-44ad-a77b-c4b8357581ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import slider_utils\n",
    "import generate_utils\n",
    "import ensembles\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e842ab-0145-4c24-91cf-af7473779856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638765cd-2449-4dee-905c-ac93167d697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b4b46f-dfb2-4b45-b3a2-e7ef9744a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = True#Запускаем ли мы сейчас обучение с нуля, или с какого-то чекпоинта\n",
    "\n",
    "\n",
    "bits_per_number = 4#Насколько сильно квантуем модель\n",
    "use_gguf = False#Можно использовать gguf\n",
    "gguf_folder_name = \"gguf_31\"#из этой папки\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "learnable_linear_model = False#учим ли мы линейную субмодель\n",
    "learnable_all = False#учим ли мы вообще весь трансформер (это очень тяжело по GPU-памяти, я не осилил)\n",
    "\n",
    "cfg_switch = 1\n",
    "if cfg_switch == 1:\n",
    "    #ЛЕГКОВЕСНЫЙ ТРАНСФОРМЕР\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    architecture = 'A'\n",
    "    opt_type = 'adam'\n",
    "    mode = 'pretrain'#'finetune', 'pretrain'\n",
    "    #1.403\n",
    "else:\n",
    "    #ТЯЖЁЛЫЙ ТРАНСФОРМЕР\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    architecture = 'I'\n",
    "    opt_type = 'adam'\n",
    "    mode = 'pretrain'\n",
    "\n",
    "\n",
    "tokenizer_name = model_name\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f621d01-f274-4423-8574-e8b5ee739a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'pretrain':\n",
    "    #претрейн\n",
    "    # max_seq_len = 6900\n",
    "    # max_seq_len4inference = 6850\n",
    "    # max_seq_len = 4500\n",
    "    # max_seq_len4inference = 4450\n",
    "    max_seq_len = 3800\n",
    "    max_seq_len4inference = 3750\n",
    "    # max_seq_len = 3600\n",
    "    # max_seq_len4inference = 3550\n",
    "else:\n",
    "    #дообучение\n",
    "    # max_seq_len = 1950\n",
    "    # max_seq_len4inference = 1900\n",
    "    max_seq_len = 2050\n",
    "    max_seq_len4inference = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6dc713-3f0b-42e2-b2ef-79a46038ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\":\n",
    "    embedding_size = 2048#это просто свойство исходной сетки\n",
    "    #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "    net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "    #дропаут внутри субмоделей\n",
    "    individ_dropout_rate = 0.05\n",
    "    conservativity = 0.13#насколько сильно подавляем отклонение от старой стратегии\n",
    "    accum_batch = 100#60#для дачала сделайте 2\n",
    "    lr = 2e-5\n",
    "    \n",
    "    #layer_configs = [4096, 4096, 2048, 2048, 2048, 2048, 2048, 2048, 1024, 1024, 1024, 1024, 1024]#версия A\n",
    "    layer_configs = [2048] * 6 + [1024] * 6#версия A\n",
    "    use_memnets = True\n",
    "    memnet_params={'num_heads':8, 'query_size':64, 'num_key_values':350, 'value_size':256}\n",
    "    composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "    sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "    path2model = \"ern_model_A_composed.pth\"\n",
    "    max_tokens_in_loss = 150#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    batch_size = 1#2\n",
    "    \n",
    "    \n",
    "    window_sizes = [12] * 5\n",
    "    squeeze_dim = embedding_size//2#выходной размер слайдера\n",
    "elif model_name == \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\":\n",
    "    embedding_size = 4096#это просто свойство исходной сетки\n",
    "\n",
    "    if architecture == 'D':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.2\n",
    "        conservativity = 0.12#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 4200#60#для дачала сделайте 2\n",
    "        lr = 3e-6\n",
    "        \n",
    "        layer_configs = [1024, 512, 512, 512, 256, 256, 256, 256, 128, 128, 128, 128]#версия C\n",
    "        layer_configs = [2048, 1024, 1024, 512, 512, 512, 256, 256, 256, 256, 256, 256, 256, 256, 256]#версия D\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_D_composed_slider.pth\"\n",
    "        max_tokens_in_loss = 5500#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    elif architecture == 'E':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.05\n",
    "        conservativity = 0.15#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 2300#60#для дачала сделайте 2\n",
    "        lr = 1e-3\n",
    "        \n",
    "        layer_configs = [2048, 2048, 1024, 1024, 1024, 512, 512, 512, 512, 512, 512, 512, 256, 256, 256, 256]#версия E\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_E_composed_slider.pth\"\n",
    "        max_tokens_in_loss = 4000#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    elif architecture == 'I':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.05\n",
    "        conservativity = 0.15#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 30#60#для дачала сделайте 2\n",
    "        lr = 1e-5\n",
    "        \n",
    "        layer_configs = [2048] * 6 + [1024] * 6#версия I\n",
    "        use_memnets = True\n",
    "        memnet_params={'num_heads':8, 'query_size':64, 'num_key_values':300, 'value_size':256}\n",
    "\n",
    "        window_sizes = [8] * 8\n",
    "        squeeze_dim = embedding_size//2#выходной размер слайдера\n",
    "        \n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_E_composed_slider.pth\"\n",
    "        max_tokens_in_loss = 90#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    else:\n",
    "        1/0\n",
    "\n",
    "    #я подобрал перебором это значение batch_size и accum_batch, именно при нём быстрее всего проходим батч размера 2000. Я не знаю, почему, но это так.\n",
    "    batch_size = 1#2\n",
    "\n",
    "    \n",
    "    if learnable_all:\n",
    "        #опасный и тормозной режим\n",
    "        batch_size = 2\n",
    "        layer_configs = [512, 256]\n",
    "\n",
    "path2slidermodel = path2model + '.slider'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe92a99-e891-4af6-adc4-71b8faf0eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27095857-2839-492d-a399-23877657f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "C:\\Users\\sd\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if learnable_all and not start_train:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                  './last_modell',\n",
    "                  #device_map=\"auto\",\n",
    "                  #device_map=device,\n",
    "                  #torch_dtype=torch.bfloat16,\n",
    "                  quantization_config=bnb_config,\n",
    "                  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                  model_name,\n",
    "                  #device_map=\"auto\",\n",
    "                  #device_map=device,\n",
    "                  #torch_dtype=torch.bfloat16,\n",
    "                  quantization_config=bnb_config,\n",
    "                  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "if use_gguf:\n",
    "    model = PeftModelForCausalLM.from_pretrained(model, gguf_folder_name, load_in_4bit =True)\n",
    "    if isinstance(model, PeftModelForCausalLM):\n",
    "        model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d86cb70-956d-47c8-95b4-58d01131af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.lm_head\n",
    "torch.save(model.lm_head.weight, \"lin_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3876e567-6e0a-4abc-a9bc-1e54f61998e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c173dd71-d677-407b-b6e5-ec3916b968ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99ede99-9df1-4630-a478-7875ceac40e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420956"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "if mode == 'pretrain':\n",
    "    dataset = InstructDatasetR(\"../llms_local/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "else:\n",
    "    dataset = InstructDatasetR(\"../llms_local/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df0c1f2-4e8e-40b0-8428-a1cbc03564d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingWindowLinear(\n",
      "  (linear): Linear(in_features=12288, out_features=1024, bias=True)\n",
      ")\n",
      "SlidingWindowLinear(\n",
      "  (linear): Linear(in_features=12288, out_features=1024, bias=True)\n",
      ")\n",
      "SlidingWindowLinear(\n",
      "  (linear): Linear(in_features=12288, out_features=1024, bias=True)\n",
      ")\n",
      "SlidingWindowLinear(\n",
      "  (linear): Linear(in_features=12288, out_features=1024, bias=True)\n",
      ")\n",
      "SlidingWindowLinear(\n",
      "  (linear): Linear(in_features=12288, out_features=1024, bias=True)\n",
      ")\n",
      "use_memnets True\n"
     ]
    }
   ],
   "source": [
    "if start_train:\n",
    "    #создать модель\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    slider = slider_utils.StackedSlidingWindow(input_dim=embedding_size, \n",
    "                                  output_dim=embedding_size, \n",
    "                                  window_sizes=window_sizes, \n",
    "                                  dropout_prob=individ_dropout_rate,\n",
    "                                  squeeze_dim=squeeze_dim)\n",
    "    \n",
    "    head = ensembles.EResNetPro(input_size=embedding_size, \n",
    "               out_size=cardinality, \n",
    "               net_dropout_rate=net_dropout_rate, \n",
    "               individ_dropout_rate=individ_dropout_rate,\n",
    "               layer_configs=layer_configs, \n",
    "               use_sigmoid_end=False, \n",
    "               use_batchnorm=True, \n",
    "               use_activation=True, \n",
    "               activation=nn.LeakyReLU(), \n",
    "               sample_features=sample_features, \n",
    "               composition_size=composition_size, \n",
    "               lin_bottleneck_size=None,\n",
    "               lin_model_add=nn.Linear(embedding_size, cardinality).to(device),\n",
    "               use_memnets=use_memnets,\n",
    "               memnet_params=memnet_params)\n",
    "    head.submodels[-1].weight = torch.nn.Parameter(torch.load( \"lin_model.pth\").to(device).to(torch.float32))\n",
    "    head.submodels[-1].weight.requires_grad = learnable_linear_model\n",
    "else:\n",
    "    head = torch.load(path2model, weights_only=False)\n",
    "    slider = torch.load(path2slidermodel, weights_only=False)\n",
    "\n",
    "#собрать\n",
    "if learnable_all:\n",
    "    model.train()\n",
    "    for param in model.parameters():\n",
    "        try:\n",
    "            param.requires_grad = True\n",
    "            print('scss')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "else:\n",
    "    model.eval()\n",
    "head.train()\n",
    "slider.train()\n",
    "head.to(device)\n",
    "slider.to(device)\n",
    "head.by_submodels = True\n",
    "optimizer = torch.optim.Adam(list(head.parameters()) + list(slider.parameters()), lr=lr)\n",
    "#momentum = 0.9\n",
    "#optimizer = torch.optim.SGD(list(head.parameters()) + list(slider.parameters()), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4456d-546a-44af-819d-33988e72d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00d1d3-eae1-49da-a838-9accd1e636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c4e27-16c3-4121-bdc7-12401f445a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864e119-4808-41ec-a776-8e2f57048106",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.02\n",
    "proba_pad_idx = 0.1#некоторые токены мы замещаем паддингами. Модель должна всё равно работать. Кроме того, это надо для спекулятивного кодирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a710644-e9e2-4b6a-b48f-04c07eaf147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#проинференсить модель на батчах, сделать датасет для постпроцессинга\n",
    "print('batch_size', batch_size)\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    #for i in range(batch_size):\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < batch_size:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        #print('fact_size', np.min([len(input_ids) + len(labels), max_seq_len4inference]), len(input_ids) + len(labels))\n",
    "\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "\n",
    "        # if not torch.any(input_ids!=0):\n",
    "        #     #X + Y\n",
    "        #     print(list(input_ids_cur.numpy()))\n",
    "        #     print(list(labels_cur.numpy()))\n",
    "        #     1/0\n",
    "    i_pointer += i #should be batch_size\n",
    "    #print('A', pd.Timestamp.now())\n",
    "    with torch.no_grad():\n",
    "        input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "        labels_full = torch.stack(labels_full).to('cuda')\n",
    "        weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "        cnt_pads = count_padding(labels_full, padding_token)\n",
    "        if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "            continue\n",
    "        if cnt_pads > 0:\n",
    "            input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "            labels_full = labels_full[:, :-cnt_pads]\n",
    "\n",
    "        #рандомные паддинги, для спекулятивки\n",
    "        to_pad_idx = torch.rand_like(input_ids_full.to(torch.float)) < proba_pad_idx\n",
    "        input_ids_full[to_pad_idx] = padding_token\n",
    "        #\n",
    "        #outp = model.forward(input_ids_full, attention_mask=None, labels=labels_full, token_type_ids=None, write_caches=False, read_caches=False, return_states=True)\n",
    "        # head.by_submodels = False\n",
    "        # head.training = False\n",
    "        # model.lm_head = head\n",
    "        # model.lm_head.half()\n",
    "        # outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=False)\n",
    "\n",
    "        outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=True)\n",
    "        outp['states'] = outp['hidden_states'][-1]\n",
    "    # {\"loss\": loss_agg, \"logits\": logits, \"states\":embeddings}\n",
    "        outp['states'] = outp['states'][:, :input_ids_full.shape[1]]\n",
    "        \n",
    "        \n",
    "        #state_cur = outp['states'].reshape([outp['states'].shape[0] * outp['states'].shape[1], outp['states'].shape[2]])#.cpu().to(torch.float16).numpy()\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        weights_cur = weights_full2d.ravel()\n",
    "        state_cur_raw = outp['states']\n",
    "        \n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        del outp\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    state_cur_slided = slider.forward(state_cur_raw.to(torch.float32))\n",
    "    shape_cur_slided = state_cur_slided.shape\n",
    "    state_cur = state_cur_slided.reshape((shape_cur_slided[0] * shape_cur_slided[1], shape_cur_slided[2]))\n",
    "    del state_cur_slided\n",
    "    del state_cur_raw\n",
    "    state_cur = state_cur[~idx]\n",
    "    weights_cur = weights_cur[~idx]\n",
    "        # if len(labels_cur) > max_tokens_in_loss:\n",
    "        #     indices = np.random.choice(len(labels_cur), size=max_tokens_in_loss, replace=False)\n",
    "        #     labels_cur = labels_cur[indices]\n",
    "        #     state_cur = state_cur[indices]\n",
    "    #print('B', pd.Timestamp.now())\n",
    "    if state_cur.shape[0] == 0:\n",
    "        continue\n",
    "    for x_start_pointer in range(0, state_cur.shape[0], max_tokens_in_loss): \n",
    "        logits, lst_logits = head(state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_tokens_in_loss])\n",
    "        \n",
    "        ce_loss = loss_fct(logits, labels_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss].view(-1))\n",
    "        ce_loss = ce_loss * weights_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss].to('cuda')\n",
    "        ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "        loss_ampl_head = torch.mean(torch.abs(lst_logits[-1] - logits)) / (torch.std(lst_logits[-1]) + 1)\n",
    "        \n",
    "        loss = torch.mean(ce_loss) * (1./(conservativity + 1.)) + loss_ampl_head * (conservativity/(conservativity + 1.))\n",
    "        loss.backward(retain_graph=True)\n",
    "        l = len(lst_logits)\n",
    "        for j in range(l - 1, -1, -1):\n",
    "            del lst_logits[j]\n",
    "        #print('-C', pd.Timestamp.now())\n",
    "        #del lst_logits\n",
    "    del state_cur\n",
    "    #print('D', pd.Timestamp.now())\n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits, axis=-1) == labels_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss]).to(torch.float16))\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= accum_batch:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), 'loss_ampl_head', np.round(loss_ampl_head.item(), 4), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        if (np.random.rand() < 1 and accum_batch > 100) or (np.random.rand() < 0.1 and accum_batch <= 100):\n",
    "            head.training = False\n",
    "            torch.save(head, path2model)\n",
    "            torch.save(slider, path2slidermodel)\n",
    "            if np.random.rand()<0.2:\n",
    "                torch.save(head, path2model + '.back')\n",
    "                torch.save(slider, path2slidermodel + '.back')\n",
    "            head.training = True\n",
    "            if learnable_all:\n",
    "                model.eval()\n",
    "                model.save_pretrained(\"./last_modell\")\n",
    "                model.train()\n",
    "                for param in model.parameters():\n",
    "                    try:\n",
    "                        param.requires_grad = True\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444721ef-2140-4fc8-ae2b-00067f659b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cur.shape, idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2634e-e8bc-4c32-8d53-60685f0e98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), 'loss_ampl_head', np.round(loss_ampl_head.item(), 4), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cefa2d-1085-4616-a6a6-4a25ea2c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение tail adapter-а вручную\n",
    "head.training = False\n",
    "head.by_submodels = False\n",
    "torch.save(head, path2model)\n",
    "torch.save(slider, path2slidermodel)\n",
    "\n",
    "if learnable_all:\n",
    "    model.eval()\n",
    "    model.save_pretrained(\"./last_modell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e8065-14b3-4c53-8389-de01df85aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eae7c9-70c7-4630-91b3-5faefbbd9d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36465c4-880e-487d-aa1d-ede2ec41f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5cfe5-08ba-4606-b5be-809f79769f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb29bc6-7b5a-44a6-86c0-5609d3222cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "slider.half()\n",
    "head.half()\n",
    "head.by_submodels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2847cf05-600e-4d45-b2d6-313646117960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:13.539772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?art�\\'moursер�Iingи at\\n# d�xОode\\n\\n\\u200b�String..(\"\"тyie notNg��t;\\r\\nv //be\\r\\nsl n�7iпange�\\r\\n\\r\\n\\r\\nt�im ву�с();\\nе&#Vq\\\\вA();\\nо g1� н$ o7нes���$�млcccanpunctionvalto �is� P, iipsn а\\x15�##itceuress seemsim�riics uos//ap s;kikanthaceox]ool# e�+ t\\rressac\\x0eߠ ,�ur3mSrascth hhe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Привет, как дела?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# Generate\n",
    "temp = 0.9\n",
    "top_p=0.01\n",
    "max_new_tokens = 150\n",
    "repetition_penalty = 1.2\n",
    "top_k = 10\n",
    "\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = generate_utils.generate_speculative(model, inputs.input_ids.to(device), slider=slider, heavy_lm_head=head,\n",
    "             top_p=top_p, temperature=temp, max_new_tokens=max_new_tokens, \n",
    "             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, \n",
    "             do_sample=True, repetition_penalty=repetition_penalty, early_stopping=False, \n",
    "             tokenizer=tokenizer, stop_strings=None, top_k=top_k, \n",
    "             return_dict_in_generate=False, use_cache=True, estimation_rule='0.2')\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7012f3fd-2573-4b54-a643-a8031eca91f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:00:08.590997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?I, kidding responding bit you response am'm just Just been busy things do life to. I for had time good day today also's in much better as yesterday was is evenWhat than the thing have like that what it not so nice but thanks very kind me your again andSorry you previous sorry no long one know about this thank already.\\n\\n late new be of still after here ago days more since when some are always we hope make now if done with't at happyven out all best i everything can help because well will try every single any get ask soon would need a maybe talk else other or my want share has us before never another end could did how please back yes tell then hi you hi hi friend dear hi hi hi hi hello\\n\\n hi\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Привет, как дела?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# Generate\n",
    "temp = 0.9\n",
    "top_p=0.01\n",
    "max_new_tokens = 150\n",
    "repetition_penalty = 1.2\n",
    "top_k = 10\n",
    "\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = generate_utils.generate_speculative(model, inputs.input_ids.to(device), \n",
    "             slider=None, heavy_lm_head=None,\n",
    "             top_p=top_p, temperature=temp, max_new_tokens=max_new_tokens, \n",
    "             pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, \n",
    "             do_sample=True, repetition_penalty=repetition_penalty, early_stopping=False, \n",
    "             tokenizer=tokenizer, stop_strings=None, top_k=top_k, \n",
    "             return_dict_in_generate=False, use_cache=True, estimation_rule='0.2')\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344a1b7-8552-4100-857b-b1d218be3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Привет, как дела?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# Generate\n",
    "\n",
    "t = pd.Timestamp.now()\n",
    "generate_ids = model.generate(inputs.input_ids.to(device), top_p=top_p, temperature=temp, max_new_tokens=max_new_tokens, \n",
    "             pad_token_id=None, eos_token_id=None, bos_token_id=None, \n",
    "             do_sample=True, repetition_penalty=repetition_penalty, early_stopping=False, \n",
    "             tokenizer=tokenizer, stop_strings=None, top_k=top_k, \n",
    "             return_dict_in_generate=False, output_scores=False, use_cache=True)\n",
    "print(pd.Timestamp.now() - t)\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda2b5a-5624-4255-9d28-1204b542b641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f794dd-9336-49b1-9a0f-d73345e1825a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
