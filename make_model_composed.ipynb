{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f211d5f3-5f53-4380-9241-7e471ba073ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#учит LLM в сборе. То есть приделываем в конец feature head и так и учим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5889c1ea-bfb9-4a28-adfd-200fc1251ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "install = False\n",
    "if install:\n",
    "    !conda install -c anaconda git -y\n",
    "\n",
    "    !pip install deepspeed==0.14.4\n",
    "\n",
    "    !pip install --upgrade pip\n",
    "    !pip install --upgrade transformers==4.46.2 tyro==0.9.8 triton==2.3.1 trl==0.12.0\n",
    "\n",
    "    !pip install -v \"xformers==0.0.29.post1\"\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps -v \"peft==0.13.2\"\n",
    "    !pip install --no-deps packaging ninja einops flash-attn bitsandbytes==0.44.1\n",
    "    !pip install accelerate==0.34.2\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    !pip uninstall peft -y\n",
    "    !pip install -v \"peft==0.13.2\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2570a2e2-a09e-44ad-a77b-c4b8357581ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from torch import cuda, LongTensor, FloatTensor\n",
    "from peft import PeftModel, PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "import ensembles\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e842ab-0145-4c24-91cf-af7473779856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638765cd-2449-4dee-905c-ac93167d697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(7*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e836de15-9520-40c7-9dd3-45390627e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = False#Запускаем ли мы сейчас обучение с нуля, или с какого-то чекпоинта\n",
    "\n",
    "\n",
    "bits_per_number = 4#Насколько сильно квантуем модель\n",
    "use_gguf = False#Можно использовать gguf\n",
    "gguf_folder_name = \"gguf_31\"#из этой папки\n",
    "\n",
    "\n",
    "cardinality = 128256#размер словаря токенов\n",
    "learnable_linear_model = False#учим ли мы линейную субмодель\n",
    "learnable_all = False#учим ли мы вообще весь трансформер (это очень тяжело по GPU-памяти, я не осилил)\n",
    "\n",
    "cfg_switch = 2\n",
    "if cfg_switch == 1:\n",
    "    #ЛЕГКОВЕСНЫЙ ТРАНСФОРМЕР\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "    architecture = 'A'\n",
    "    opt_type = 'adam'\n",
    "    mode = 'pretrain'#'finetune', 'pretrain'\n",
    "    #1.403\n",
    "else:\n",
    "    #ТЯЖЁЛЫЙ ТРАНСФОРМЕР\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "    architecture = 'H'\n",
    "    opt_type = 'adam'\n",
    "    mode = 'pretrain'\n",
    "\n",
    "\n",
    "tokenizer_name = model_name\n",
    "padding_token = 128009\n",
    "forbidden_tokens_list = [padding_token]\n",
    "seed = int(np.random.rand() * 1000000)#random seed for data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6dc713-3f0b-42e2-b2ef-79a46038ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_memnets = True\n",
    "memnet_params = {}\n",
    "if model_name == \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\":\n",
    "    embedding_size = 2048#это просто свойство исходной сетки\n",
    "    #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "    net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "    #дропаут внутри субмоделей\n",
    "    individ_dropout_rate = 0.05\n",
    "    conservativity = 0.05#насколько сильно подавляем отклонение от старой стратегии\n",
    "    accum_batch = 1000#130#110#60#для дачала сделайте 2\n",
    "    lr = 1e-6#1e-3\n",
    "    \n",
    "    layer_configs = [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 1024, 1024, 1024, 1024, 1024, 512, 512, 512, 512, 512, 512, 512]#версия A\n",
    "    composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "    sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "    path2model = \"ern_model_A_composed.pth\"\n",
    "    max_tokens_in_loss = 1300#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    batch_size = 3#2\n",
    "elif model_name == \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\":\n",
    "    embedding_size = 4096#это просто свойство исходной сетки\n",
    "\n",
    "    if architecture == 'D':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.2\n",
    "        conservativity = 0.12#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 4200#60#для дачала сделайте 2\n",
    "        lr = 3e-6\n",
    "        \n",
    "        layer_configs = [1024, 512, 512, 512, 256, 256, 256, 256, 128, 128, 128, 128]#версия C\n",
    "        layer_configs = [2048, 1024, 1024, 512, 512, 512, 256, 256, 256, 256, 256, 256, 256, 256, 256]#версия D\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_D_composed.pth\"\n",
    "        max_tokens_in_loss = 5500#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    elif architecture == 'E':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.05\n",
    "        conservativity = 0.15#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 2300#60#для дачала сделайте 2\n",
    "        lr = 1e-3\n",
    "        \n",
    "        layer_configs = [2048, 2048, 1024, 1024, 1024, 512, 512, 512, 512, 512, 512, 512, 256, 256, 256, 256]#версия E\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_E_composed.pth\"\n",
    "        max_tokens_in_loss = 4000#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "    elif architecture == 'G':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.025\n",
    "        conservativity = 0.07#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 5000#1400#12000#60#для дачала сделайте 2\n",
    "        #lr = 3e-7\n",
    "        lr = 1e-4\n",
    "        \n",
    "        layer_configs = [2048, 2048, 2048, 2048, 2048, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 512, 512, 512, 512, 512, 512]\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_G_composed.pth\"\n",
    "        max_tokens_in_loss = 4000#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "\n",
    "    elif architecture == 'H':\n",
    "        #сколько субмоделей выбрасывается из forward на обучении. Чем больше, тем лучше защита от оверфита. Если одна-две субмодели, net_dropout лучше занулять\n",
    "        net_dropout_rate = 0.0#если десятки субмоделей и хотим огромную защиту от оверфита, то это число надо проставлять в 0.9-0.95\n",
    "        #дропаут внутри субмоделей\n",
    "        individ_dropout_rate = 0.025\n",
    "        conservativity = 0.11#насколько сильно подавляем отклонение от старой стратегии\n",
    "        accum_batch = 4000#1400#12000#60#для дачала сделайте 2\n",
    "        #lr = 3e-7\n",
    "        lr = 1e-6\n",
    "        \n",
    "        layer_configs = [2048] * 5 + [1024] * 7\n",
    "        composition_size = 1#число субмоделей класса ResNet. Чем больше их, тем больше или защита от оверфита (если большой net_dropout), или ёмкость\n",
    "        sample_features = 1.#какая доля фичей приходит в субмодель. По дефолту ставится 0.4-0.6, но это если субмоделей как минимум 15. Сейчас их 1.\n",
    "        path2model = \"ern_model_H_composed.pth\"\n",
    "        max_tokens_in_loss = 2300#сколько пар эмбеддинг-токен проходит через tail adapter за один forward\n",
    "        use_memnets = True\n",
    "        memnet_params={'num_heads':12, 'query_size':64, 'num_key_values':320, 'value_size':256}\n",
    "        \n",
    "\n",
    "    #я подобрал перебором это значение batch_size и accum_batch, именно при нём быстрее всего проходим батч размера 2000. Я не знаю, почему, но это так.\n",
    "    batch_size = 2#2\n",
    "\n",
    "    \n",
    "    if learnable_all:\n",
    "        #опасный и тормозной режим\n",
    "        batch_size = 2\n",
    "        layer_configs = [512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95f8f96-4254-4011-b5d8-6b9143009114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n"
     ]
    }
   ],
   "source": [
    "#настройки датасета\n",
    "\n",
    "if mode == 'pretrain':\n",
    "    #претрейн\n",
    "    max_seq_len = 6900\n",
    "    max_seq_len4inference = 6850\n",
    "    # max_seq_len = 4500\n",
    "    # max_seq_len4inference = 4450\n",
    "    # max_seq_len = 3800\n",
    "    # max_seq_len4inference = 3750\n",
    "    # max_seq_len = 3600\n",
    "    # max_seq_len4inference = 3550\n",
    "else:\n",
    "    #дообучение\n",
    "    # max_seq_len = 1950\n",
    "    # max_seq_len4inference = 1900\n",
    "    max_seq_len = 2050\n",
    "    max_seq_len4inference = 2000\n",
    "\n",
    "print('batch_size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe92a99-e891-4af6-adc4-71b8faf0eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27095857-2839-492d-a399-23877657f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")#, cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "\n",
    "if bits_per_number == 4:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\"#, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "elif bits_per_number == 8:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True, bnb_8bit_use_double_quant=True, bnb_8bit_quant_type=\"nf8\"#, bnb_8bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if learnable_all and not start_train:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                  './last_modell',\n",
    "                  #device_map=\"auto\",\n",
    "                  #device_map=device,\n",
    "                  #torch_dtype=torch.bfloat16,\n",
    "                  quantization_config=bnb_config,\n",
    "                  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                  model_name,\n",
    "                  #device_map=\"auto\",\n",
    "                  #device_map=device,\n",
    "                  #torch_dtype=torch.bfloat16,\n",
    "                  quantization_config=bnb_config,\n",
    "                  cache_dir=\"D:\\cache\\huggingface\\\\\"+ model_name)\n",
    "if use_gguf:\n",
    "    model = PeftModelForCausalLM.from_pretrained(model, gguf_folder_name, load_in_4bit =True)\n",
    "    if isinstance(model, PeftModelForCausalLM):\n",
    "        model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d86cb70-956d-47c8-95b4-58d01131af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.lm_head\n",
    "torch.save(model.lm_head.weight, \"lin_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3876e567-6e0a-4abc-a9bc-1e54f61998e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c173dd71-d677-407b-b6e5-ec3916b968ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDatasetR(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_seq_length, cut=None):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cut = cut\n",
    "        self.data = self.load_data()\n",
    "        self.log_samples = deque(maxlen=45)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        random.seed(seed)\n",
    "        data = random.sample(data, len(data))\n",
    "        # if self.cut is None:\n",
    "        #     data = [data_cur for data_cur in data if len(data_cur[1])>5]\n",
    "        # else:\n",
    "        #     data = [data_cur for data_cur in data[:self.cut] if self.cut>5]\n",
    "        # for i in range(100):\n",
    "        #     print('***', data[i])\n",
    "        #     try:\n",
    "        #         if (not 'Алис' in data[i][0]) and (not 'Элеон\"' in data[i][0]) and (not 'Софи\"' in data[i][0]) and (not 'Сэм\"' in data[i][0]) (not 'Алис' in data[i][1]) and (not 'Элеон\"' in data[i][1]) and (not 'Софи\"' in data[i][1]) and (not 'Сэм\"' in data[i][1]):\n",
    "        #             print('***', data[i])\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.log_samples.append(self.data[idx])\n",
    "        parts = self.data[idx]\n",
    "        text = parts[0]\n",
    "        label = parts[1]\n",
    "        parts = [parts[0], parts[1], 1]\n",
    "        for r_variant in [-2,-1,-0.5,0.5,1,2]:\n",
    "            s = f\"<r{r_variant}>\"\n",
    "            if s in label:\n",
    "                parts[-1] = r_variant\n",
    "                label = label.replace(s, '')\n",
    "                break\n",
    "        r = parts[-1]\n",
    "        if text is None:\n",
    "            text = label\n",
    "        \n",
    "        # Кодируем текст и метку с помощью tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Кодируем метку\n",
    "        label_encoding = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label_ids = label_encoding['input_ids']\n",
    "        return {\n",
    "            'input_ids': input_ids[0],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_ids[0],\n",
    "            'mult': torch.tensor(r)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25638998-e6c1-40d7-8f90-48b4e86d661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7.6G\n",
      "-rw-r--r-- 1 jovyan jovyan 1.2G Mar 13 17:38 dataset_llm_full_flat.pkl\n",
      "-rw-r--r-- 1 jovyan jovyan 367M Mar 17 10:38 dataset_llm_full_instruct.pkl\n",
      "drwxr-xr-x 2 jovyan jovyan 4.0K Mar 20 16:22 .\n",
      "-rw-r--r-- 1 jovyan jovyan 6.1G Mar 20 16:42 dataset_llm_full_mflat.pkl\n",
      "drwxr-xr-x 8 jovyan jovyan 4.0K Mar 21 06:45 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -alrth ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99ede99-9df1-4630-a478-7875ceac40e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание Dataset\n",
    "if mode == 'pretrain':\n",
    "    dataset = InstructDatasetR(\"./data/dataset_llm_full_mflat.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "else:\n",
    "    dataset = InstructDatasetR(\"./data/dataset_llm_full_instruct.pkl\", tokenizer, max_seq_len, cut=None)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df0c1f2-4e8e-40b0-8428-a1cbc03564d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if start_train:\n",
    "    #создать модель\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    head = ensembles.EResNetPro(input_size=embedding_size, \n",
    "               out_size=cardinality, \n",
    "               net_dropout_rate=net_dropout_rate, \n",
    "               individ_dropout_rate=individ_dropout_rate,\n",
    "               layer_configs=layer_configs, \n",
    "               use_sigmoid_end=False, \n",
    "               use_batchnorm=True, \n",
    "               use_activation=True, \n",
    "               activation=nn.LeakyReLU(), \n",
    "               sample_features=sample_features, \n",
    "               composition_size=composition_size, \n",
    "               lin_bottleneck_size=None,\n",
    "               lin_model_add=nn.Linear(embedding_size, cardinality).to(device),\n",
    "               memnet_params=memnet_params,\n",
    "               use_memnets=use_memnets)\n",
    "    head.submodels[-1].weight = torch.nn.Parameter(torch.load( \"lin_model.pth\").to(device).to(torch.float32))\n",
    "    head.submodels[-1].weight.requires_grad = learnable_linear_model\n",
    "else:\n",
    "    head = torch.load(path2model, weights_only=False)\n",
    "\n",
    "#собрать\n",
    "if learnable_all:\n",
    "    model.train()\n",
    "    for param in model.parameters():\n",
    "        try:\n",
    "            param.requires_grad = True\n",
    "            print('scss')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "else:\n",
    "    model.eval()\n",
    "head.train()\n",
    "head.to(device)\n",
    "head.by_submodels = True\n",
    "if opt_type == 'adam':\n",
    "    optimizer = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "else:\n",
    "    momentum = 0.\n",
    "    optimizer = torch.optim.SGD(head.parameters(), lr=lr*0.01, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa4456d-546a-44af-819d-33988e72d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_clip(tens, mn, mx):\n",
    "    mn = torch.tensor(mn).to(tens.device)\n",
    "    mx = torch.tensor(mx).to(tens.device)\n",
    "    tens[tens<mn] = torch.nn.Sigmoid()(tens[tens<mn]) + mn\n",
    "    tens[tens>mx] = torch.nn.Sigmoid()(tens[tens>mx]) + mx\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb00d1d3-eae1-49da-a838-9accd1e636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss(reduction = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "370c4e27-16c3-4121-bdc7-12401f445a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчёта количества последних токенов, равных padding_token\n",
    "def count_padding(tensor, padding_token):\n",
    "    counts = []\n",
    "    for row in tensor:\n",
    "        count = 0\n",
    "        for token in reversed(row):\n",
    "            if token.item() == padding_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        counts.append(count)\n",
    "    return min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e864e119-4808-41ec-a776-8e2f57048106",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_drop_idx = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a710644-e9e2-4b6a-b48f-04c07eaf147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247970/3388736101.py:53: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  weights_full = torch.stack(weights_full).to('cuda').T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss 0.85 loss_ampl_head 0.1139 8000 from 420956 1.90044 % acc 0.8417843780517578 2025-03-21 08:20:30.291758\n",
      "ce_loss 0.857 loss_ampl_head 0.1096 16000 from 420956 3.80087 % acc 0.841799560546875 2025-03-21 09:54:36.833333\n",
      "ce_loss 0.868 loss_ampl_head 0.1057 24000 from 420956 5.70131 % acc 0.8392976684570312 2025-03-21 11:28:19.015863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m labels_cur\n\u001b[1;32m    119\u001b[0m batch_accum_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 120\u001b[0m loss_array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mce_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    121\u001b[0m metr_array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [acc\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_accum_counter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m accum_batch:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#проинференсить модель на батчах, сделать датасет для постпроцессинга\n",
    "print('batch_size', batch_size)\n",
    "i_pointer = 0\n",
    "\n",
    "batch_accum_counter = 0\n",
    "loss_array = []\n",
    "metr_array = []\n",
    "while 1: \n",
    "    input_ids_full = []\n",
    "    labels_full = []\n",
    "    weights_full = []\n",
    "    #for i in range(batch_size):\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    while i < batch_size:\n",
    "        offset_counter += 1\n",
    "        sample_cur  = dataset[i_pointer + i]\n",
    "        input_ids = sample_cur['input_ids']\n",
    "        labels = sample_cur['labels']\n",
    "        weights = sample_cur['mult']\n",
    "        \n",
    "        input_ids_len = len(input_ids)\n",
    "        labels_len = len(labels)\n",
    "        input_ids = input_ids[input_ids!=padding_token]\n",
    "        \n",
    "        drop_idx = torch.rand_like(input_ids.to(torch.float)) < proba_drop_idx\n",
    "        input_ids = input_ids[~drop_idx]\n",
    "        \n",
    "        labels = labels[labels!=padding_token]\n",
    "        padding_size = input_ids_len + labels_len - len(input_ids) - len(labels)\n",
    "        input_ids_cur = torch.cat([input_ids, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        labels_cur = torch.cat([input_ids * 0 + padding_token, labels, torch.zeros(padding_size, dtype=torch.int32) + padding_token])[:max_seq_len4inference]\n",
    "        #print('fact_size', np.min([len(input_ids) + len(labels), max_seq_len4inference]), len(input_ids) + len(labels))\n",
    "        if torch.all(labels_cur) == padding_token:\n",
    "            print('empty label')\n",
    "            continue\n",
    "        i += 1\n",
    "        \n",
    "        input_ids_full.append(input_ids_cur[:-1])\n",
    "        labels_full.append(labels_cur[1:])\n",
    "        weights_full.append(weights)\n",
    "\n",
    "        # if not torch.any(input_ids!=0):\n",
    "        #     #X + Y\n",
    "        #     print(list(input_ids_cur.numpy()))\n",
    "        #     print(list(labels_cur.numpy()))\n",
    "        #     1/0\n",
    "    i_pointer += i #should be batch_size\n",
    "    #print('A', pd.Timestamp.now())\n",
    "    with torch.no_grad():\n",
    "        input_ids_full = torch.stack(input_ids_full).to('cuda')\n",
    "        labels_full = torch.stack(labels_full).to('cuda')\n",
    "        weights_full = torch.stack(weights_full).to('cuda').T\n",
    "\n",
    "        if 1:\n",
    "            cnt_pads = count_padding(labels_full, padding_token)\n",
    "            if cnt_pads >= input_ids_full.shape[1] - 5:\n",
    "                continue\n",
    "            if cnt_pads > 0:\n",
    "                input_ids_full = input_ids_full[:, :-cnt_pads]\n",
    "                labels_full = labels_full[:, :-cnt_pads]\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #\n",
    "        #outp = model.forward(input_ids_full, attention_mask=None, labels=labels_full, token_type_ids=None, write_caches=False, read_caches=False, return_states=True)\n",
    "        # head.by_submodels = False\n",
    "        # head.training = False\n",
    "        # model.lm_head = head\n",
    "        # model.lm_head.half()\n",
    "        # outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=False)\n",
    "\n",
    "        outp = model.forward(input_ids_full, output_hidden_states=True, return_dict=True)\n",
    "        outp['states'] = outp['hidden_states'][-1]\n",
    "    # {\"loss\": loss_agg, \"logits\": logits, \"states\":embeddings}\n",
    "        outp['states'] = outp['states'][:, :input_ids_full.shape[1]]\n",
    "        state_cur = outp['states'].reshape([outp['states'].shape[0] * outp['states'].shape[1], outp['states'].shape[2]])#.cpu().to(torch.float16).numpy()\n",
    "        weights_full2d = torch.vstack([weights_full] * labels_full.shape[-1]).to('cuda').T\n",
    "        labels_cur = labels_full.ravel()#.cpu().numpy()\n",
    "        weights_cur = weights_full2d.ravel()\n",
    "\n",
    "        del labels_full\n",
    "        del weights_full2d\n",
    "        del weights_full\n",
    "        del outp\n",
    "        idx = torch.isin(labels_cur, torch.tensor(forbidden_tokens_list).to(device))\n",
    "        labels_cur = labels_cur[~idx]\n",
    "        state_cur = state_cur[~idx]\n",
    "        weights_cur = weights_cur[~idx]\n",
    "        # if len(labels_cur) > max_tokens_in_loss:\n",
    "        #     indices = np.random.choice(len(labels_cur), size=max_tokens_in_loss, replace=False)\n",
    "        #     labels_cur = labels_cur[indices]\n",
    "        #     state_cur = state_cur[indices]\n",
    "    #print('B', pd.Timestamp.now())\n",
    "    if state_cur.shape[0] == 0:\n",
    "        continue\n",
    "    for x_start_pointer in range(0, state_cur.shape[0], max_tokens_in_loss): \n",
    "        logits, lst_logits = head(state_cur.to(torch.float32)[x_start_pointer: x_start_pointer + max_tokens_in_loss])\n",
    "        \n",
    "        ce_loss = loss_fct(logits, labels_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss].view(-1))\n",
    "        ce_loss = ce_loss * weights_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss].to('cuda')\n",
    "        ce_loss = soft_clip(ce_loss, -0.6, 10.)\n",
    "        loss_ampl_head = torch.mean(torch.abs(lst_logits[-1] - logits)) / (torch.std(lst_logits[-1]) + 1)\n",
    "        \n",
    "        loss = torch.mean(ce_loss) * (1./(conservativity + 1.)) + loss_ampl_head * (conservativity/(conservativity + 1.))\n",
    "        loss.backward()\n",
    "        l = len(lst_logits)\n",
    "        for j in range(l - 1, -1, -1):\n",
    "            del lst_logits[j]\n",
    "        #print('-C', pd.Timestamp.now())\n",
    "        #del lst_logits\n",
    "    del state_cur\n",
    "    #print('D', pd.Timestamp.now())\n",
    "\n",
    "    acc = torch.mean((torch.argmax(logits, axis=-1) == labels_cur[x_start_pointer: x_start_pointer + max_tokens_in_loss]).to(torch.float16))\n",
    "    del logits\n",
    "    del labels_cur\n",
    "    batch_accum_counter += 1\n",
    "    loss_array += [torch.mean(ce_loss).item()]\n",
    "    metr_array += [acc.item()]\n",
    "    if batch_accum_counter >= accum_batch:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        print('ce_loss', np.round(np.mean(loss_array), 3), 'loss_ampl_head', np.round(loss_ampl_head.item(), 4), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())\n",
    "        loss_array = []\n",
    "        metr_array = []\n",
    "        batch_accum_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        if (np.random.rand() < 1 and accum_batch > 100) or (np.random.rand() < 0.1 and accum_batch <= 100):\n",
    "            head.training = False\n",
    "            torch.save(head, path2model)\n",
    "            if np.random.rand()<0.2:\n",
    "                torch.save(head, path2model + '.back')\n",
    "            head.training = True\n",
    "            if learnable_all:\n",
    "                model.eval()\n",
    "                model.save_pretrained(\"./last_modell\")\n",
    "                model.train()\n",
    "                for param in model.parameters():\n",
    "                    try:\n",
    "                        param.requires_grad = True\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bfa71b-f539-4680-ba9f-8a401eca735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 секунд на строку в среднем, длина 4200, adam\n",
    "#2 сек на строку длины 3800 с adam\n",
    "#0.8 сек на строку длины 3800, sgd без momentum (?!)\n",
    "# сек на строку длины 4200, sgd без momentum\n",
    "#2 сек на строку длины 3600 с adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71c2634e-e8bc-4c32-8d53-60685f0e98d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss 0.879 loss_ampl_head 0.1073 28498 from 420956 6.76983 % acc 0.8372876584741993 2025-03-21 12:21:42.631645\n"
     ]
    }
   ],
   "source": [
    "#если принудительно стопнули, можно добить батч\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "print('ce_loss', np.round(np.mean(loss_array), 3), 'loss_ampl_head', np.round(loss_ampl_head.item(), 4), i_pointer, 'from', len(dataset), np.round(100 * i_pointer/len(dataset), 5), '%', 'acc', np.mean(metr_array), pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78cefa2d-1085-4616-a6a6-4a25ea2c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение tail adapter-а вручную\n",
    "head.training = False\n",
    "head.by_submodels = False\n",
    "torch.save(head, path2model)\n",
    "\n",
    "if learnable_all:\n",
    "    model.eval()\n",
    "    model.save_pretrained(\"./last_modell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e8065-14b3-4c53-8389-de01df85aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30137390-1c71-4e2a-b220-f2f7590c306d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
